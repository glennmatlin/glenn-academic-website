[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a technology professional with expertise in software development, data science, and machine learning. My work focuses on building innovative solutions that bridge the gap between cutting-edge research and practical applications.\n\n\nThroughout my career, I’ve had the privilege of working with leading organizations in healthcare and technology:\n\nChange Healthcare - Healthcare technology and insights\nKomodo Health - Healthcare intelligence platform\nLendUp - Financial technology and services\nRichRelevance - Personalization and recommendation systems\n\n\n\n\nMy technical interests and expertise span several domains:\n\nMachine Learning & AI: Building and evaluating ML models, with a focus on Large Language Models (LLMs) and their applications\nSoftware Engineering: Full-stack development with modern frameworks and best practices\nData Science: Statistical analysis, data visualization, and predictive modeling\nHealthcare Technology: Leveraging technology to improve healthcare outcomes\nFinancial Technology: Building secure and scalable financial services\n\n\n\n\nI’m currently exploring the intersection of AI and domain-specific applications, particularly in:\n\nEvaluating and improving Large Language Models for specialized tasks\nBuilding frameworks for responsible AI deployment\nCreating tools that make advanced ML techniques accessible to domain experts\n\n\n\n\nI believe in continuous learning and staying current with rapidly evolving technologies. I regularly contribute to open-source projects and share knowledge through technical writing and documentation.\nFeel free to explore my blog posts, projects, and publications to learn more about my work. You can also find various technical resources in my knowledge base.\n\nLet’s connect! Feel free to reach out via email or connect on LinkedIn."
  },
  {
    "objectID": "about.html#glenn-matlin",
    "href": "about.html#glenn-matlin",
    "title": "About Me",
    "section": "",
    "text": "I’m a technology professional with expertise in software development, data science, and machine learning. My work focuses on building innovative solutions that bridge the gap between cutting-edge research and practical applications.\n\n\nThroughout my career, I’ve had the privilege of working with leading organizations in healthcare and technology:\n\nChange Healthcare - Healthcare technology and insights\nKomodo Health - Healthcare intelligence platform\nLendUp - Financial technology and services\nRichRelevance - Personalization and recommendation systems\n\n\n\n\nMy technical interests and expertise span several domains:\n\nMachine Learning & AI: Building and evaluating ML models, with a focus on Large Language Models (LLMs) and their applications\nSoftware Engineering: Full-stack development with modern frameworks and best practices\nData Science: Statistical analysis, data visualization, and predictive modeling\nHealthcare Technology: Leveraging technology to improve healthcare outcomes\nFinancial Technology: Building secure and scalable financial services\n\n\n\n\nI’m currently exploring the intersection of AI and domain-specific applications, particularly in:\n\nEvaluating and improving Large Language Models for specialized tasks\nBuilding frameworks for responsible AI deployment\nCreating tools that make advanced ML techniques accessible to domain experts\n\n\n\n\nI believe in continuous learning and staying current with rapidly evolving technologies. I regularly contribute to open-source projects and share knowledge through technical writing and documentation.\nFeel free to explore my blog posts, projects, and publications to learn more about my work. You can also find various technical resources in my knowledge base.\n\nLet’s connect! Feel free to reach out via email or connect on LinkedIn."
  },
  {
    "objectID": "docs/IMPROVEMENT_PLAN.html",
    "href": "docs/IMPROVEMENT_PLAN.html",
    "title": "Website Improvement Plan",
    "section": "",
    "text": "Website Improvement Plan\nThis document outlines a plan for improving the Quarto-based academic website. The improvements are broken down into three categories.\n\n\nPart 1: Configuration, SEO, and Robustness\nThese are foundational changes to make the site work better and be more discoverable.\n\nAdd a Custom 404 Page: Create a custom 404.qmd page that matches the site’s branding and helps users who follow a broken link.\nEnhance the Footer: Add a copyright notice (e.g., “© 2025 Glenn Matlin”) and a “Published with Quarto” notice to _quarto.yml for a more professional touch.\nFix Analytics Placeholder: Remove the placeholder Google Analytics ID (G-XXXXXXXXXX) from _quarto.yml to prevent errors.\nImprove Page Metadata: Add a description field to the YAML frontmatter of key pages (index.qmd, about.qmd, research.qmd, etc.) to improve search engine optimization (SEO).\n\n\n\nPart 2: Content Enrichment and Consistency\nThis focuses on making existing content more detailed and uniform.\n\nEnrich Publications Data: Add valuable fields like abstract, doi (link to the paper), and pdf (link to a PDF file) to each entry in publications/papers.yml.\nStandardize Project Pages: Analyze project pages in /projects/ to ensure a consistent structure, including a featured image (image:), tags (tags:), and links to the code repository (repo_url:) and a live demo (demo_url:).\nRefine the CV Page: Examine cv.qmd and suggest either embedding the PDF directly on the page or converting the CV content into HTML for better accessibility and searchability.\n\n\n\nPart 3: User Experience and Polish\nThese are smaller details that create a more polished experience.\n\nEnsure Image Accessibility: Check main pages (index.qmd, about.qmd) to ensure that all images have descriptive alt text for accessibility and SEO.\nAdd a Full Set of Favicons: Add a comprehensive set of favicons for different devices (Apple touch icons, Android icons, etc.) and a site.webmanifest file."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Glenn Matlin",
    "section": "",
    "text": "MIT License\nCopyright (c) 2016-present George Cushen\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Here are some of the key projects I’ve worked on. These range from research initiatives to practical tools and frameworks designed to solve real-world problems."
  },
  {
    "objectID": "projects.html#featured-projects",
    "href": "projects.html#featured-projects",
    "title": "Projects",
    "section": "Featured Projects",
    "text": "Featured Projects"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to my blog where I share insights on technology, machine learning, and software development. Here you’ll find articles covering everything from technical deep-dives to industry observations and practical guides."
  },
  {
    "objectID": "blog.html#recent-posts",
    "href": "blog.html#recent-posts",
    "title": "Blog",
    "section": "Recent Posts",
    "text": "Recent Posts"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html",
    "href": "knowledgebase/python/python3_logging_howto.html",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "This page contains tutorial information. For links to reference information and a logging cookbook, please see Other resources.\n\n\nLogging is a means of tracking events that happen when some software runs. The software’s developer adds logging calls to their code to indicate that certain events have occurred. An event is described by a descriptive message which can optionally contain variable data (i.e. data that is potentially different for each occurrence of the event). Events also have an importance which the developer ascribes to the event; the importance can also be called the level or severity.\n\n\nYou can access logging functionality by creating a logger via logger = getLogger(__name__), and then calling the logger’s debug(), info(), warning(), error() and critical() methods. To determine when to use logging, and to see which logger methods to use when, see the table below. It states, for each of a set of common tasks, the best tool to use for that task.\n\nTask you want to perform\n|\nThe best tool for the task\n\n\n\n\n\n\n\n\nDisplay console output for ordinary usage of a command line script or program\n|\nprint()\n| |\nReport events that occur during normal operation of a program (e.g. for status monitoring or fault investigation)\n|\nA logger’s info() (or debug() method for very detailed output for diagnostic purposes)\n| |\nIssue a warning regarding a particular runtime event\n|\nwarnings.warn() in library code if the issue is avoidable and the client application should be modified to eliminate the warning\nA logger’s warning() method if there is nothing the client application can do about the situation, but the event should still be noted\n| |\nReport an error regarding a particular runtime event\n|\nRaise an exception\n| |\nReport suppression of an error without raising an exception (e.g. error handler in a long-running server process)\n|\nA logger’s error(), exception() or critical() method as appropriate for the specific error and application domain\n|\nThe logger methods are named after the level or severity of the events they are used to track. The standard levels and their applicability are described below (in increasing order of severity):\n\nLevel\n|\nWhen it’s used\n\n\n\n\n\n\n\n\nDEBUG\n|\nDetailed information, typically of interest only when diagnosing problems.\n| |\nINFO\n|\nConfirmation that things are working as expected.\n| |\nWARNING\n|\nAn indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected.\n| |\nERROR\n|\nDue to a more serious problem, the software has not been able to perform some function.\n| |\nCRITICAL\n|\nA serious error, indicating that the program itself may be unable to continue running.\n|\nThe default level is WARNING, which means that only events of this severity and higher will be tracked, unless the logging package is configured to do otherwise.\nEvents that are tracked can be handled in different ways. The simplest way of handling tracked events is to print them to the console. Another common way is to write them to a disk file.\n\n\n\nA very simple example is:\nimport logging\nlogging.warning('Watch out!')  # will print a message to the console\nlogging.info('I told you so')  # will not print anything\n\nIf you type these lines into a script and run it, you’ll see:\nprinted out on the console. The INFO message doesn’t appear because the default level is WARNING. The printed message includes the indication of the level and the description of the event provided in the logging call, i.e. ‘Watch out!’. The actual output can be formatted quite flexibly if you need that; formatting options will also be explained later.\nNotice that in this example, we use functions directly on the logging module, like logging.debug, rather than creating a logger and calling functions on it. These functions operation on the root logger, but can be useful as they will call basicConfig() for you if it has not been called yet, like in this example. In larger programs you’ll usually want to control the logging configuration explicitly however - so for that reason as well as others, it’s better to create loggers and call their methods.\n\n\n\nA very common situation is that of recording logging events in a file, so let’s look at that next. Be sure to try the following in a newly started Python interpreter, and don’t just continue from the session described above:\nimport logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\nlogger.debug('This message should go to the log file')\nlogger.info('So should this')\nlogger.warning('And this, too')\nlogger.error('And non-ASCII stuff, too, like Øresund and Malmö')\n\nChanged in version 3.9: The encoding argument was added. In earlier Python versions, or if not specified, the encoding used is the default value used by open(). While not shown in the above example, an errors argument can also now be passed, which determines how encoding errors are handled. For available values and the default, see the documentation for open().\nAnd now if we open the file and look at what we have, we should find the log messages:\nDEBUG:__main__:This message should go to the log file\nINFO:__main__:So should this\nWARNING:__main__:And this, too\nERROR:__main__:And non-ASCII stuff, too, like Øresund and Malmö\n\nThis example also shows how you can set the logging level which acts as the threshold for tracking. In this case, because we set the threshold to DEBUG, all of the messages were printed.\nIf you want to set the logging level from a command-line option such as:\nand you have the value of the parameter passed for --log in some variable loglevel, you can use:\ngetattr(logging, loglevel.upper())\n\nto get the value which you’ll pass to basicConfig() via the level argument. You may want to error check any user input value, perhaps as in the following example:\n# assuming loglevel is bound to the string value obtained from the\n# command line argument. Convert to upper case to allow the user to\n# specify --log=DEBUG or --log=debug\nnumeric_level = getattr(logging, loglevel.upper(), None)\nif not isinstance(numeric_level, int):\n    raise ValueError('Invalid log level: %s' % loglevel)\nlogging.basicConfig(level=numeric_level, ...)\n\nThe call to basicConfig() should come before any calls to a logger’s methods such as debug(), info(), etc. Otherwise, that logging event may not be handled in the desired manner.\nIf you run the above script several times, the messages from successive runs are appended to the file example.log. If you want each run to start afresh, not remembering the messages from earlier runs, you can specify the filemode argument, by changing the call in the above example to:\nlogging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)\n\nThe output will be the same as before, but the log file is no longer appended to, so the messages from earlier runs are lost.\n\n\n\nTo log variable data, use a format string for the event description message and append the variable data as arguments. For example:\nimport logging\nlogging.warning('%s before you %s', 'Look', 'leap!')\n\nwill display:\nWARNING:root:Look before you leap!\n\nAs you can see, merging of variable data into the event description message uses the old, %-style of string formatting. This is for backwards compatibility: the logging package pre-dates newer formatting options such as str.format() and string.Template. These newer formatting options are supported, but exploring them is outside the scope of this tutorial: see Using particular formatting styles throughout your application for more information.\n\n\n\nTo change the format which is used to display messages, you need to specify the format you want to use:\nimport logging\nlogging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\nlogging.debug('This message should appear on the console')\nlogging.info('So should this')\nlogging.warning('And this, too')\n\nwhich would print:\nDEBUG:This message should appear on the console\nINFO:So should this\nWARNING:And this, too\n\nNotice that the ‘root’ which appeared in earlier examples has disappeared. For a full set of things that can appear in format strings, you can refer to the documentation for LogRecord attributes, but for simple usage, you just need the levelname (severity), message (event description, including variable data) and perhaps to display when the event occurred. This is described in the next section.\n\n\n\nTo display the date and time of an event, you would place ‘%(asctime)s’ in your format string:\nimport logging\nlogging.basicConfig(format='%(asctime)s %(message)s')\nlogging.warning('is when this event was logged.')\n\nwhich should print something like this:\n2010-12-12 11:41:42,612 is when this event was logged.\n\nThe default format for date/time display (shown above) is like ISO8601 or RFC 3339. If you need more control over the formatting of the date/time, provide a datefmt argument to basicConfig, as in this example:\nimport logging\nlogging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\nlogging.warning('is when this event was logged.')\n\nwhich would display something like this:\n12/12/2010 11:46:36 AM is when this event was logged.\n\nThe format of the datefmt argument is the same as supported by time.strftime().\n\n\n\nThat concludes the basic tutorial. It should be enough to get you up and running with logging. There’s a lot more that the logging package offers, but to get the best out of it, you’ll need to invest a little more of your time in reading the following sections. If you’re ready for that, grab some of your favourite beverage and carry on.\nIf your logging needs are simple, then use the above examples to incorporate logging into your own scripts, and if you run into problems or don’t understand something, please post a question on the comp.lang.python Usenet group (available at https://groups.google.com/g/comp.lang.python) and you should receive help before too long.\nStill here? You can carry on reading the next few sections, which provide a slightly more advanced/in-depth tutorial than the basic one above. After that, you can take a look at the Logging Cookbook.\n\n\n\n\nThe logging library takes a modular approach and offers several categories of components: loggers, handlers, filters, and formatters.\n\nLoggers expose the interface that application code directly uses.\nHandlers send the log records (created by loggers) to the appropriate destination.\nFilters provide a finer grained facility for determining which log records to output.\nFormatters specify the layout of log records in the final output.\n\nLog event information is passed between loggers, handlers, filters and formatters in a LogRecord instance.\nLogging is performed by calling methods on instances of the Logger class (hereafter called loggers). Each instance has a name, and they are conceptually arranged in a namespace hierarchy using dots (periods) as separators. For example, a logger named ‘scan’ is the parent of loggers ‘scan.text’, ‘scan.html’ and ‘scan.pdf’. Logger names can be anything you want, and indicate the area of an application in which a logged message originates.\nA good convention to use when naming loggers is to use a module-level logger, in each module which uses logging, named as follows:\nlogger = logging.getLogger(__name__)\n\nThis means that logger names track the package/module hierarchy, and it’s intuitively obvious where events are logged just from the logger name.\nThe root of the hierarchy of loggers is called the root logger. That’s the logger used by the functions debug(), info(), warning(), error() and critical(), which just call the same-named method of the root logger. The functions and the methods have the same signatures. The root logger’s name is printed as ‘root’ in the logged output.\nIt is, of course, possible to log messages to different destinations. Support is included in the package for writing log messages to files, HTTP GET/POST locations, email via SMTP, generic sockets, queues, or OS-specific logging mechanisms such as syslog or the Windows NT event log. Destinations are served by handler classes. You can create your own log destination class if you have special requirements not met by any of the built-in handler classes.\nBy default, no destination is set for any logging messages. You can specify a destination (such as console or file) by using basicConfig() as in the tutorial examples. If you call the functions debug(), info(), warning(), error() and critical(), they will check to see if no destination is set; and if one is not set, they will set a destination of the console (sys.stderr) and a default format for the displayed message before delegating to the root logger to do the actual message output.\nThe default format set by basicConfig() for messages is:\nseverity:logger name:message\n\nYou can change this by passing a format string to basicConfig() with the format keyword argument. For all options regarding how a format string is constructed, see Formatter Objects.\n\n\nThe flow of log event information in loggers and handlers is illustrated in the following diagram.\nLogger flow Create LogRecord Logging call in user code, e.g. logger.info(…) Stop Does a filter attached to logger reject the record? Pass record to handlers of current logger Is propagate true for current logger? Is there a parent logger? Set current logger to parent At least one handler in hierarchy? Use lastResort handler Handler enabled for level of record? Does a filter attached to handler reject the record? Stop Emit (includes formatting) Handler flow Logger enabled for level of call? No Yes Yes No No Yes Yes No No Yes No Yes No Yes Record passed to handler\n\n\n\nLogger objects have a threefold job. First, they expose several methods to application code so that applications can log messages at runtime. Second, logger objects determine which log messages to act upon based upon severity (the default filtering facility) or filter objects. Third, logger objects pass along relevant log messages to all interested log handlers.\nThe most widely used methods on logger objects fall into two categories: configuration and message sending.\nThese are the most common configuration methods:\n\nLogger.setLevel() specifies the lowest-severity log message a logger will handle, where debug is the lowest built-in severity level and critical is the highest built-in severity. For example, if the severity level is INFO, the logger will handle only INFO, WARNING, ERROR, and CRITICAL messages and will ignore DEBUG messages.\nLogger.addHandler() and Logger.removeHandler() add and remove handler objects from the logger object. Handlers are covered in more detail in Handlers.\nLogger.addFilter() and Logger.removeFilter() add and remove filter objects from the logger object. Filters are covered in more detail in Filter Objects.\n\nYou don’t need to always call these methods on every logger you create. See the last two paragraphs in this section.\nWith the logger object configured, the following methods create log messages:\n\nLogger.debug(), Logger.info(), Logger.warning(), Logger.error(), and Logger.critical() all create log records with a message and a level that corresponds to their respective method names. The message is actually a format string, which may contain the standard string substitution syntax of %s, %d, %f, and so on. The rest of their arguments is a list of objects that correspond with the substitution fields in the message. With regard to **kwargs, the logging methods care only about a keyword of exc_info and use it to determine whether to log exception information.\nLogger.exception() creates a log message similar to Logger.error(). The difference is that Logger.exception() dumps a stack trace along with it. Call this method only from an exception handler.\nLogger.log() takes a log level as an explicit argument. This is a little more verbose for logging messages than using the log level convenience methods listed above, but this is how to log at custom log levels.\n\ngetLogger() returns a reference to a logger instance with the specified name if it is provided, or root if not. The names are period-separated hierarchical structures. Multiple calls to getLogger() with the same name will return a reference to the same logger object. Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo.\nLoggers have a concept of effective level. If a level is not explicitly set on a logger, the level of its parent is used instead as its effective level. If the parent has no explicit level set, its parent is examined, and so on - all ancestors are searched until an explicitly set level is found. The root logger always has an explicit level set (WARNING by default). When deciding whether to process an event, the effective level of the logger is used to determine whether the event is passed to the logger’s handlers.\nChild loggers propagate messages up to the handlers associated with their ancestor loggers. Because of this, it is unnecessary to define and configure handlers for all the loggers an application uses. It is sufficient to configure handlers for a top-level logger and create child loggers as needed. (You can, however, turn off propagation by setting the propagate attribute of a logger to False.)\n\n\n\nHandler objects are responsible for dispatching the appropriate log messages (based on the log messages’ severity) to the handler’s specified destination. Logger objects can add zero or more handler objects to themselves with an addHandler() method. As an example scenario, an application may want to send all log messages to a log file, all log messages of error or higher to stdout, and all messages of critical to an email address. This scenario requires three individual handlers where each handler is responsible for sending messages of a specific severity to a specific location.\nThe standard library includes quite a few handler types (see Useful Handlers); the tutorials use mainly StreamHandler and FileHandler in its examples.\nThere are very few methods in a handler for application developers to concern themselves with. The only handler methods that seem relevant for application developers who are using the built-in handler objects (that is, not creating custom handlers) are the following configuration methods:\n\nThe setLevel() method, just as in logger objects, specifies the lowest severity that will be dispatched to the appropriate destination. Why are there two setLevel() methods? The level set in the logger determines which severity of messages it will pass to its handlers. The level set in each handler determines which messages that handler will send on.\nsetFormatter() selects a Formatter object for this handler to use.\naddFilter() and removeFilter() respectively configure and deconfigure filter objects on handlers.\n\nApplication code should not directly instantiate and use instances of Handler. Instead, the Handler class is a base class that defines the interface that all handlers should have and establishes some default behavior that child classes can use (or override).\n\n\n\nFormatter objects configure the final order, structure, and contents of the log message. Unlike the base logging.Handler class, application code may instantiate formatter classes, although you could likely subclass the formatter if your application needs special behavior. The constructor takes three optional arguments – a message format string, a date format string and a style indicator.\nlogging.Formatter.__init__(fmt=None, datefmt=None, style=‘%’)¶\nIf there is no message format string, the default is to use the raw message. If there is no date format string, the default date format is:\nwith the milliseconds tacked on at the end. The style is one of '%', '{', or '$'. If one of these is not specified, then '%' will be used.\nIf the style is '%', the message format string uses %(&lt;dictionary key&gt;)s styled string substitution; the possible keys are documented in LogRecord attributes. If the style is '{', the message format string is assumed to be compatible with str.format() (using keyword arguments), while if the style is '$' then the message format string should conform to what is expected by string.Template.substitute().\nChanged in version 3.2: Added the style parameter.\nThe following message format string will log the time in a human-readable format, the severity of the message, and the contents of the message, in that order:\n'%(asctime)s - %(levelname)s - %(message)s'\n\nFormatters use a user-configurable function to convert the creation time of a record to a tuple. By default, time.localtime() is used; to change this for a particular formatter instance, set the converter attribute of the instance to a function with the same signature as time.localtime() or time.gmtime(). To change it for all formatters, for example if you want all logging times to be shown in GMT, set the converter attribute in the Formatter class (to time.gmtime for GMT display).\n\n\n\nProgrammers can configure logging in three ways:\n\nCreating loggers, handlers, and formatters explicitly using Python code that calls the configuration methods listed above.\nCreating a logging config file and reading it using the fileConfig() function.\nCreating a dictionary of configuration information and passing it to the dictConfig() function.\n\nFor the reference documentation on the last two options, see Configuration functions. The following example configures a very simple logger, a console handler, and a simple formatter using Python code:\nimport logging\n\n# create logger\nlogger = logging.getLogger('simple_example')\nlogger.setLevel(logging.DEBUG)\n\n# create console handler and set level to debug\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\n\n# create formatter\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n# add formatter to ch\nch.setFormatter(formatter)\n\n# add ch to logger\nlogger.addHandler(ch)\n\n# 'application' code\nlogger.debug('debug message')\nlogger.info('info message')\nlogger.warning('warn message')\nlogger.error('error message')\nlogger.critical('critical message')\n\nRunning this module from the command line produces the following output:\n$ python simple_logging_module.py\n2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message\n2005-03-19 15:10:26,620 - simple_example - INFO - info message\n2005-03-19 15:10:26,695 - simple_example - WARNING - warn message\n2005-03-19 15:10:26,697 - simple_example - ERROR - error message\n2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message\n\nThe following Python module creates a logger, handler, and formatter nearly identical to those in the example listed above, with the only difference being the names of the objects:\nimport logging\nimport logging.config\n\nlogging.config.fileConfig('logging.conf')\n\n# create logger\nlogger = logging.getLogger('simpleExample')\n\n# 'application' code\nlogger.debug('debug message')\nlogger.info('info message')\nlogger.warning('warn message')\nlogger.error('error message')\nlogger.critical('critical message')\n\nHere is the logging.conf file:\n[loggers]\nkeys=root,simpleExample\n\n[handlers]\nkeys=consoleHandler\n\n[formatters]\nkeys=simpleFormatter\n\n[logger_root]\nlevel=DEBUG\nhandlers=consoleHandler\n\n[logger_simpleExample]\nlevel=DEBUG\nhandlers=consoleHandler\nqualname=simpleExample\npropagate=0\n\n[handler_consoleHandler]\nclass=StreamHandler\nlevel=DEBUG\nformatter=simpleFormatter\nargs=(sys.stdout,)\n\n[formatter_simpleFormatter]\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\n\nThe output is nearly identical to that of the non-config-file-based example:\n$ python simple_logging_config.py\n2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message\n2005-03-19 15:38:55,979 - simpleExample - INFO - info message\n2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message\n2005-03-19 15:38:56,055 - simpleExample - ERROR - error message\n2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message\n\nYou can see that the config file approach has a few advantages over the Python code approach, mainly separation of configuration and code and the ability of noncoders to easily modify the logging properties.\nWarning\nThe fileConfig() function takes a default parameter, disable_existing_loggers, which defaults to True for reasons of backward compatibility. This may or may not be what you want, since it will cause any non-root loggers existing before the fileConfig() call to be disabled unless they (or an ancestor) are explicitly named in the configuration. Please refer to the reference documentation for more information, and specify False for this parameter if you wish.\nThe dictionary passed to dictConfig() can also specify a Boolean value with key disable_existing_loggers, which if not specified explicitly in the dictionary also defaults to being interpreted as True. This leads to the logger-disabling behaviour described above, which may not be what you want - in which case, provide the key explicitly with a value of False.\nNote that the class names referenced in config files need to be either relative to the logging module, or absolute values which can be resolved using normal import mechanisms. Thus, you could use either WatchedFileHandler (relative to the logging module) or mypackage.mymodule.MyHandler (for a class defined in package mypackage and module mymodule, where mypackage is available on the Python import path).\nIn Python 3.2, a new means of configuring logging has been introduced, using dictionaries to hold configuration information. This provides a superset of the functionality of the config-file-based approach outlined above, and is the recommended configuration method for new applications and deployments. Because a Python dictionary is used to hold configuration information, and since you can populate that dictionary using different means, you have more options for configuration. For example, you can use a configuration file in JSON format, or, if you have access to YAML processing functionality, a file in YAML format, to populate the configuration dictionary. Or, of course, you can construct the dictionary in Python code, receive it in pickled form over a socket, or use whatever approach makes sense for your application.\nHere’s an example of the same configuration as above, in YAML format for the new dictionary-based approach:\nversion: 1\nformatters:\n  simple:\n    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nhandlers:\n  console:\n    class: logging.StreamHandler\n    level: DEBUG\n    formatter: simple\n    stream: ext://sys.stdout\nloggers:\n  simpleExample:\n    level: DEBUG\n    handlers: [console]\n    propagate: no\nroot:\n  level: DEBUG\n  handlers: [console]\n\nFor more information about logging using a dictionary, see Configuration functions.\n\n\n\nIf no logging configuration is provided, it is possible to have a situation where a logging event needs to be output, but no handlers can be found to output the event.\nThe event is output using a ‘handler of last resort’, stored in lastResort. This internal handler is not associated with any logger, and acts like a StreamHandler which writes the event description message to the current value of sys.stderr (therefore respecting any redirections which may be in effect). No formatting is done on the message - just the bare event description message is printed. The handler’s level is set to WARNING, so all events at this and greater severities will be output.\nChanged in version 3.2: For versions of Python prior to 3.2, the behaviour is as follows:\n\nIf raiseExceptions is False (production mode), the event is silently dropped.\nIf raiseExceptions is True (development mode), a message ‘No handlers could be found for logger X.Y.Z’ is printed once.\n\nTo obtain the pre-3.2 behaviour, lastResort can be set to None.\n\n\n\nWhen developing a library which uses logging, you should take care to document how the library uses logging - for example, the names of loggers used. Some consideration also needs to be given to its logging configuration. If the using application does not use logging, and library code makes logging calls, then (as described in the previous section) events of severity WARNING and greater will be printed to sys.stderr. This is regarded as the best default behaviour.\nIf for some reason you don’t want these messages printed in the absence of any logging configuration, you can attach a do-nothing handler to the top-level logger for your library. This avoids the message being printed, since a handler will always be found for the library’s events: it just doesn’t produce any output. If the library user configures logging for application use, presumably that configuration will add some handlers, and if levels are suitably configured then logging calls made in library code will send output to those handlers, as normal.\nA do-nothing handler is included in the logging package: NullHandler (since Python 3.1). An instance of this handler could be added to the top-level logger of the logging namespace used by the library (if you want to prevent your library’s logged events being output to sys.stderr in the absence of logging configuration). If all logging by a library foo is done using loggers with names matching ‘foo.x’, ‘foo.x.y’, etc. then the code:\nimport logging\nlogging.getLogger('foo').addHandler(logging.NullHandler())\n\nshould have the desired effect. If an organisation produces a number of libraries, then the logger name specified can be ‘orgname.foo’ rather than just ‘foo’.\nNote\nIt is strongly advised that you do not log to the root logger in your library. Instead, use a logger with a unique and easily identifiable name, such as the __name__ for your library’s top-level package or module. Logging to the root logger will make it difficult or impossible for the application developer to configure the logging verbosity or handlers of your library as they wish.\nNote\nIt is strongly advised that you do not add any handlers other than NullHandler to your library’s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers ‘under the hood’, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements.\n\n\n\n\nThe numeric values of logging levels are given in the following table. These are primarily of interest if you want to define your own levels, and need them to have specific values relative to the predefined levels. If you define a level with the same numeric value, it overwrites the predefined value; the predefined name is lost.\n\nLevel\n|\nNumeric value\n\n\n\n\n\n\n\n\nCRITICAL\n|\n50\n| |\nERROR\n|\n40\n| |\nWARNING\n|\n30\n| |\nINFO\n|\n20\n| |\nDEBUG\n|\n10\n| |\nNOTSET\n|\n0\n|\nLevels can also be associated with loggers, being set either by the developer or through loading a saved logging configuration. When a logging method is called on a logger, the logger compares its own level with the level associated with the method call. If the logger’s level is higher than the method call’s, no logging message is actually generated. This is the basic mechanism controlling the verbosity of logging output.\nLogging messages are encoded as instances of the LogRecord class. When a logger decides to actually log an event, a LogRecord instance is created from the logging message.\nLogging messages are subjected to a dispatch mechanism through the use of handlers, which are instances of subclasses of the Handler class. Handlers are responsible for ensuring that a logged message (in the form of a LogRecord) ends up in a particular location (or set of locations) which is useful for the target audience for that message (such as end users, support desk staff, system administrators, developers). Handlers are passed LogRecord instances intended for particular destinations. Each logger can have zero, one or more handlers associated with it (via the addHandler() method of Logger). In addition to any handlers directly associated with a logger, all handlers associated with all ancestors of the logger are called to dispatch the message (unless the propagate flag for a logger is set to a false value, at which point the passing to ancestor handlers stops).\nJust as for loggers, handlers can have levels associated with them. A handler’s level acts as a filter in the same way as a logger’s level does. If a handler decides to actually dispatch an event, the emit() method is used to send the message to its destination. Most user-defined subclasses of Handler will need to override this emit().\n\n\nDefining your own levels is possible, but should not be necessary, as the existing levels have been chosen on the basis of practical experience. However, if you are convinced that you need custom levels, great care should be exercised when doing this, and it is possibly a very bad idea to define custom levels if you are developing a library. That’s because if multiple library authors all define their own custom levels, there is a chance that the logging output from such multiple libraries used together will be difficult for the using developer to control and/or interpret, because a given numeric value might mean different things for different libraries.\n\n\n\n\nIn addition to the base Handler class, many useful subclasses are provided:\n\nStreamHandler instances send messages to streams (file-like objects).\nFileHandler instances send messages to disk files.\nBaseRotatingHandler is the base class for handlers that rotate log files at a certain point. It is not meant to be instantiated directly. Instead, use RotatingFileHandler or TimedRotatingFileHandler.\nRotatingFileHandler instances send messages to disk files, with support for maximum log file sizes and log file rotation.\nTimedRotatingFileHandler instances send messages to disk files, rotating the log file at certain timed intervals.\nSocketHandler instances send messages to TCP/IP sockets. Since 3.4, Unix domain sockets are also supported.\nDatagramHandler instances send messages to UDP sockets. Since 3.4, Unix domain sockets are also supported.\nSMTPHandler instances send messages to a designated email address.\nSysLogHandler instances send messages to a Unix syslog daemon, possibly on a remote machine.\nNTEventLogHandler instances send messages to a Windows NT/2000/XP event log.\nMemoryHandler instances send messages to a buffer in memory, which is flushed whenever specific criteria are met.\nHTTPHandler instances send messages to an HTTP server using either GET or POST semantics.\nWatchedFileHandler instances watch the file they are logging to. If the file changes, it is closed and reopened using the file name. This handler is only useful on Unix-like systems; Windows does not support the underlying mechanism used.\nQueueHandler instances send messages to a queue, such as those implemented in the queue or multiprocessing modules.\nNullHandler instances do nothing with error messages. They are used by library developers who want to use logging, but want to avoid the ‘No handlers could be found for logger XXX’ message which can be displayed if the library user has not configured logging. See Configuring Logging for a Library for more information.\n\nThe NullHandler, StreamHandler and FileHandler classes are defined in the core logging package. The other handlers are defined in a sub-module, logging.handlers. (There is also another sub-module, logging.config, for configuration functionality.)\nLogged messages are formatted for presentation through instances of the Formatter class. They are initialized with a format string suitable for use with the % operator and a dictionary.\nFor formatting multiple messages in a batch, instances of BufferingFormatter can be used. In addition to the format string (which is applied to each message in the batch), there is provision for header and trailer format strings.\nWhen filtering based on logger level and/or handler level is not enough, instances of Filter can be added to both Logger and Handler instances (through their addFilter() method). Before deciding to process a message further, both loggers and handlers consult all their filters for permission. If any filter returns a false value, the message is not processed further.\nThe basic Filter functionality allows filtering by specific logger name. If this feature is used, messages sent to the named logger and its children are allowed through the filter, and all others dropped.\n\n\n\nThe logging package is designed to swallow exceptions which occur while logging in production. This is so that errors which occur while handling logging events - such as logging misconfiguration, network or other similar errors - do not cause the application using logging to terminate prematurely.\nSystemExit and KeyboardInterrupt exceptions are never swallowed. Other exceptions which occur during the emit() method of a Handler subclass are passed to its handleError() method.\nThe default implementation of handleError() in Handler checks to see if a module-level variable, raiseExceptions, is set. If set, a traceback is printed to sys.stderr. If not set, the exception is swallowed.\nNote\nThe default value of raiseExceptions is True. This is because during development, you typically want to be notified of any exceptions that occur. It’s advised that you set raiseExceptions to False for production usage.\n\n\n\nIn the preceding sections and examples, it has been assumed that the message passed when logging the event is a string. However, this is not the only possibility. You can pass an arbitrary object as a message, and its __str__() method will be called when the logging system needs to convert it to a string representation. In fact, if you want to, you can avoid computing a string representation altogether - for example, the SocketHandler emits an event by pickling it and sending it over the wire.\n\n\n\nFormatting of message arguments is deferred until it cannot be avoided. However, computing the arguments passed to the logging method can also be expensive, and you may want to avoid doing it if the logger will just throw away your event. To decide what to do, you can call the isEnabledFor() method which takes a level argument and returns true if the event would be created by the Logger for that level of call. You can write code like this:\nif logger.isEnabledFor(logging.DEBUG):\n    logger.debug('Message with %s, %s', expensive_func1(),\n                                        expensive_func2())\n\nso that if the logger’s threshold is set above DEBUG, the calls to expensive_func1 and expensive_func2 are never made.\nNote\nIn some cases, isEnabledFor() can itself be more expensive than you’d like (e.g. for deeply nested loggers where an explicit level is only set high up in the logger hierarchy). In such cases (or if you want to avoid calling a method in tight loops), you can cache the result of a call to isEnabledFor() in a local or instance variable, and use that instead of calling the method each time. Such a cached value would only need to be recomputed when the logging configuration changes dynamically while the application is running (which is not all that common).\nThere are other optimizations which can be made for specific applications which need more precise control over what logging information is collected. Here’s a list of things you can do to avoid processing during logging which you don’t need:\n\nWhat you don’t want to collect\n|\nHow to avoid collecting it\n\n\n\n\n\n\n\n\nInformation about where calls were made from.\n|\nSet logging._srcfile to None. This avoids calling sys._getframe(), which may help to speed up your code in environments like PyPy (which can’t speed up code that uses sys._getframe()).\n| |\nThreading information.\n|\nSet logging.logThreads to False.\n| |\nCurrent process ID (os.getpid())\n|\nSet logging.logProcesses to False.\n| |\nCurrent process name when using multiprocessing to manage multiple processes.\n|\nSet logging.logMultiprocessing to False.\n| |\nCurrent asyncio.Task name when using asyncio.\n|\nSet logging.logAsyncioTasks to False.\n|\nAlso note that the core logging module only includes the basic handlers. If you don’t import logging.handlers and logging.config, they won’t take up any memory."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html#basic-logging-tutorial",
    "href": "knowledgebase/python/python3_logging_howto.html#basic-logging-tutorial",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "Logging is a means of tracking events that happen when some software runs. The software’s developer adds logging calls to their code to indicate that certain events have occurred. An event is described by a descriptive message which can optionally contain variable data (i.e. data that is potentially different for each occurrence of the event). Events also have an importance which the developer ascribes to the event; the importance can also be called the level or severity.\n\n\nYou can access logging functionality by creating a logger via logger = getLogger(__name__), and then calling the logger’s debug(), info(), warning(), error() and critical() methods. To determine when to use logging, and to see which logger methods to use when, see the table below. It states, for each of a set of common tasks, the best tool to use for that task.\n\nTask you want to perform\n|\nThe best tool for the task\n\n\n\n\n\n\n\n\nDisplay console output for ordinary usage of a command line script or program\n|\nprint()\n| |\nReport events that occur during normal operation of a program (e.g. for status monitoring or fault investigation)\n|\nA logger’s info() (or debug() method for very detailed output for diagnostic purposes)\n| |\nIssue a warning regarding a particular runtime event\n|\nwarnings.warn() in library code if the issue is avoidable and the client application should be modified to eliminate the warning\nA logger’s warning() method if there is nothing the client application can do about the situation, but the event should still be noted\n| |\nReport an error regarding a particular runtime event\n|\nRaise an exception\n| |\nReport suppression of an error without raising an exception (e.g. error handler in a long-running server process)\n|\nA logger’s error(), exception() or critical() method as appropriate for the specific error and application domain\n|\nThe logger methods are named after the level or severity of the events they are used to track. The standard levels and their applicability are described below (in increasing order of severity):\n\nLevel\n|\nWhen it’s used\n\n\n\n\n\n\n\n\nDEBUG\n|\nDetailed information, typically of interest only when diagnosing problems.\n| |\nINFO\n|\nConfirmation that things are working as expected.\n| |\nWARNING\n|\nAn indication that something unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected.\n| |\nERROR\n|\nDue to a more serious problem, the software has not been able to perform some function.\n| |\nCRITICAL\n|\nA serious error, indicating that the program itself may be unable to continue running.\n|\nThe default level is WARNING, which means that only events of this severity and higher will be tracked, unless the logging package is configured to do otherwise.\nEvents that are tracked can be handled in different ways. The simplest way of handling tracked events is to print them to the console. Another common way is to write them to a disk file.\n\n\n\nA very simple example is:\nimport logging\nlogging.warning('Watch out!')  # will print a message to the console\nlogging.info('I told you so')  # will not print anything\n\nIf you type these lines into a script and run it, you’ll see:\nprinted out on the console. The INFO message doesn’t appear because the default level is WARNING. The printed message includes the indication of the level and the description of the event provided in the logging call, i.e. ‘Watch out!’. The actual output can be formatted quite flexibly if you need that; formatting options will also be explained later.\nNotice that in this example, we use functions directly on the logging module, like logging.debug, rather than creating a logger and calling functions on it. These functions operation on the root logger, but can be useful as they will call basicConfig() for you if it has not been called yet, like in this example. In larger programs you’ll usually want to control the logging configuration explicitly however - so for that reason as well as others, it’s better to create loggers and call their methods.\n\n\n\nA very common situation is that of recording logging events in a file, so let’s look at that next. Be sure to try the following in a newly started Python interpreter, and don’t just continue from the session described above:\nimport logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\nlogger.debug('This message should go to the log file')\nlogger.info('So should this')\nlogger.warning('And this, too')\nlogger.error('And non-ASCII stuff, too, like Øresund and Malmö')\n\nChanged in version 3.9: The encoding argument was added. In earlier Python versions, or if not specified, the encoding used is the default value used by open(). While not shown in the above example, an errors argument can also now be passed, which determines how encoding errors are handled. For available values and the default, see the documentation for open().\nAnd now if we open the file and look at what we have, we should find the log messages:\nDEBUG:__main__:This message should go to the log file\nINFO:__main__:So should this\nWARNING:__main__:And this, too\nERROR:__main__:And non-ASCII stuff, too, like Øresund and Malmö\n\nThis example also shows how you can set the logging level which acts as the threshold for tracking. In this case, because we set the threshold to DEBUG, all of the messages were printed.\nIf you want to set the logging level from a command-line option such as:\nand you have the value of the parameter passed for --log in some variable loglevel, you can use:\ngetattr(logging, loglevel.upper())\n\nto get the value which you’ll pass to basicConfig() via the level argument. You may want to error check any user input value, perhaps as in the following example:\n# assuming loglevel is bound to the string value obtained from the\n# command line argument. Convert to upper case to allow the user to\n# specify --log=DEBUG or --log=debug\nnumeric_level = getattr(logging, loglevel.upper(), None)\nif not isinstance(numeric_level, int):\n    raise ValueError('Invalid log level: %s' % loglevel)\nlogging.basicConfig(level=numeric_level, ...)\n\nThe call to basicConfig() should come before any calls to a logger’s methods such as debug(), info(), etc. Otherwise, that logging event may not be handled in the desired manner.\nIf you run the above script several times, the messages from successive runs are appended to the file example.log. If you want each run to start afresh, not remembering the messages from earlier runs, you can specify the filemode argument, by changing the call in the above example to:\nlogging.basicConfig(filename='example.log', filemode='w', level=logging.DEBUG)\n\nThe output will be the same as before, but the log file is no longer appended to, so the messages from earlier runs are lost.\n\n\n\nTo log variable data, use a format string for the event description message and append the variable data as arguments. For example:\nimport logging\nlogging.warning('%s before you %s', 'Look', 'leap!')\n\nwill display:\nWARNING:root:Look before you leap!\n\nAs you can see, merging of variable data into the event description message uses the old, %-style of string formatting. This is for backwards compatibility: the logging package pre-dates newer formatting options such as str.format() and string.Template. These newer formatting options are supported, but exploring them is outside the scope of this tutorial: see Using particular formatting styles throughout your application for more information.\n\n\n\nTo change the format which is used to display messages, you need to specify the format you want to use:\nimport logging\nlogging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\nlogging.debug('This message should appear on the console')\nlogging.info('So should this')\nlogging.warning('And this, too')\n\nwhich would print:\nDEBUG:This message should appear on the console\nINFO:So should this\nWARNING:And this, too\n\nNotice that the ‘root’ which appeared in earlier examples has disappeared. For a full set of things that can appear in format strings, you can refer to the documentation for LogRecord attributes, but for simple usage, you just need the levelname (severity), message (event description, including variable data) and perhaps to display when the event occurred. This is described in the next section.\n\n\n\nTo display the date and time of an event, you would place ‘%(asctime)s’ in your format string:\nimport logging\nlogging.basicConfig(format='%(asctime)s %(message)s')\nlogging.warning('is when this event was logged.')\n\nwhich should print something like this:\n2010-12-12 11:41:42,612 is when this event was logged.\n\nThe default format for date/time display (shown above) is like ISO8601 or RFC 3339. If you need more control over the formatting of the date/time, provide a datefmt argument to basicConfig, as in this example:\nimport logging\nlogging.basicConfig(format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')\nlogging.warning('is when this event was logged.')\n\nwhich would display something like this:\n12/12/2010 11:46:36 AM is when this event was logged.\n\nThe format of the datefmt argument is the same as supported by time.strftime().\n\n\n\nThat concludes the basic tutorial. It should be enough to get you up and running with logging. There’s a lot more that the logging package offers, but to get the best out of it, you’ll need to invest a little more of your time in reading the following sections. If you’re ready for that, grab some of your favourite beverage and carry on.\nIf your logging needs are simple, then use the above examples to incorporate logging into your own scripts, and if you run into problems or don’t understand something, please post a question on the comp.lang.python Usenet group (available at https://groups.google.com/g/comp.lang.python) and you should receive help before too long.\nStill here? You can carry on reading the next few sections, which provide a slightly more advanced/in-depth tutorial than the basic one above. After that, you can take a look at the Logging Cookbook."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html#advanced-logging-tutorial",
    "href": "knowledgebase/python/python3_logging_howto.html#advanced-logging-tutorial",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "The logging library takes a modular approach and offers several categories of components: loggers, handlers, filters, and formatters.\n\nLoggers expose the interface that application code directly uses.\nHandlers send the log records (created by loggers) to the appropriate destination.\nFilters provide a finer grained facility for determining which log records to output.\nFormatters specify the layout of log records in the final output.\n\nLog event information is passed between loggers, handlers, filters and formatters in a LogRecord instance.\nLogging is performed by calling methods on instances of the Logger class (hereafter called loggers). Each instance has a name, and they are conceptually arranged in a namespace hierarchy using dots (periods) as separators. For example, a logger named ‘scan’ is the parent of loggers ‘scan.text’, ‘scan.html’ and ‘scan.pdf’. Logger names can be anything you want, and indicate the area of an application in which a logged message originates.\nA good convention to use when naming loggers is to use a module-level logger, in each module which uses logging, named as follows:\nlogger = logging.getLogger(__name__)\n\nThis means that logger names track the package/module hierarchy, and it’s intuitively obvious where events are logged just from the logger name.\nThe root of the hierarchy of loggers is called the root logger. That’s the logger used by the functions debug(), info(), warning(), error() and critical(), which just call the same-named method of the root logger. The functions and the methods have the same signatures. The root logger’s name is printed as ‘root’ in the logged output.\nIt is, of course, possible to log messages to different destinations. Support is included in the package for writing log messages to files, HTTP GET/POST locations, email via SMTP, generic sockets, queues, or OS-specific logging mechanisms such as syslog or the Windows NT event log. Destinations are served by handler classes. You can create your own log destination class if you have special requirements not met by any of the built-in handler classes.\nBy default, no destination is set for any logging messages. You can specify a destination (such as console or file) by using basicConfig() as in the tutorial examples. If you call the functions debug(), info(), warning(), error() and critical(), they will check to see if no destination is set; and if one is not set, they will set a destination of the console (sys.stderr) and a default format for the displayed message before delegating to the root logger to do the actual message output.\nThe default format set by basicConfig() for messages is:\nseverity:logger name:message\n\nYou can change this by passing a format string to basicConfig() with the format keyword argument. For all options regarding how a format string is constructed, see Formatter Objects.\n\n\nThe flow of log event information in loggers and handlers is illustrated in the following diagram.\nLogger flow Create LogRecord Logging call in user code, e.g. logger.info(…) Stop Does a filter attached to logger reject the record? Pass record to handlers of current logger Is propagate true for current logger? Is there a parent logger? Set current logger to parent At least one handler in hierarchy? Use lastResort handler Handler enabled for level of record? Does a filter attached to handler reject the record? Stop Emit (includes formatting) Handler flow Logger enabled for level of call? No Yes Yes No No Yes Yes No No Yes No Yes No Yes Record passed to handler\n\n\n\nLogger objects have a threefold job. First, they expose several methods to application code so that applications can log messages at runtime. Second, logger objects determine which log messages to act upon based upon severity (the default filtering facility) or filter objects. Third, logger objects pass along relevant log messages to all interested log handlers.\nThe most widely used methods on logger objects fall into two categories: configuration and message sending.\nThese are the most common configuration methods:\n\nLogger.setLevel() specifies the lowest-severity log message a logger will handle, where debug is the lowest built-in severity level and critical is the highest built-in severity. For example, if the severity level is INFO, the logger will handle only INFO, WARNING, ERROR, and CRITICAL messages and will ignore DEBUG messages.\nLogger.addHandler() and Logger.removeHandler() add and remove handler objects from the logger object. Handlers are covered in more detail in Handlers.\nLogger.addFilter() and Logger.removeFilter() add and remove filter objects from the logger object. Filters are covered in more detail in Filter Objects.\n\nYou don’t need to always call these methods on every logger you create. See the last two paragraphs in this section.\nWith the logger object configured, the following methods create log messages:\n\nLogger.debug(), Logger.info(), Logger.warning(), Logger.error(), and Logger.critical() all create log records with a message and a level that corresponds to their respective method names. The message is actually a format string, which may contain the standard string substitution syntax of %s, %d, %f, and so on. The rest of their arguments is a list of objects that correspond with the substitution fields in the message. With regard to **kwargs, the logging methods care only about a keyword of exc_info and use it to determine whether to log exception information.\nLogger.exception() creates a log message similar to Logger.error(). The difference is that Logger.exception() dumps a stack trace along with it. Call this method only from an exception handler.\nLogger.log() takes a log level as an explicit argument. This is a little more verbose for logging messages than using the log level convenience methods listed above, but this is how to log at custom log levels.\n\ngetLogger() returns a reference to a logger instance with the specified name if it is provided, or root if not. The names are period-separated hierarchical structures. Multiple calls to getLogger() with the same name will return a reference to the same logger object. Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo.\nLoggers have a concept of effective level. If a level is not explicitly set on a logger, the level of its parent is used instead as its effective level. If the parent has no explicit level set, its parent is examined, and so on - all ancestors are searched until an explicitly set level is found. The root logger always has an explicit level set (WARNING by default). When deciding whether to process an event, the effective level of the logger is used to determine whether the event is passed to the logger’s handlers.\nChild loggers propagate messages up to the handlers associated with their ancestor loggers. Because of this, it is unnecessary to define and configure handlers for all the loggers an application uses. It is sufficient to configure handlers for a top-level logger and create child loggers as needed. (You can, however, turn off propagation by setting the propagate attribute of a logger to False.)\n\n\n\nHandler objects are responsible for dispatching the appropriate log messages (based on the log messages’ severity) to the handler’s specified destination. Logger objects can add zero or more handler objects to themselves with an addHandler() method. As an example scenario, an application may want to send all log messages to a log file, all log messages of error or higher to stdout, and all messages of critical to an email address. This scenario requires three individual handlers where each handler is responsible for sending messages of a specific severity to a specific location.\nThe standard library includes quite a few handler types (see Useful Handlers); the tutorials use mainly StreamHandler and FileHandler in its examples.\nThere are very few methods in a handler for application developers to concern themselves with. The only handler methods that seem relevant for application developers who are using the built-in handler objects (that is, not creating custom handlers) are the following configuration methods:\n\nThe setLevel() method, just as in logger objects, specifies the lowest severity that will be dispatched to the appropriate destination. Why are there two setLevel() methods? The level set in the logger determines which severity of messages it will pass to its handlers. The level set in each handler determines which messages that handler will send on.\nsetFormatter() selects a Formatter object for this handler to use.\naddFilter() and removeFilter() respectively configure and deconfigure filter objects on handlers.\n\nApplication code should not directly instantiate and use instances of Handler. Instead, the Handler class is a base class that defines the interface that all handlers should have and establishes some default behavior that child classes can use (or override).\n\n\n\nFormatter objects configure the final order, structure, and contents of the log message. Unlike the base logging.Handler class, application code may instantiate formatter classes, although you could likely subclass the formatter if your application needs special behavior. The constructor takes three optional arguments – a message format string, a date format string and a style indicator.\nlogging.Formatter.__init__(fmt=None, datefmt=None, style=‘%’)¶\nIf there is no message format string, the default is to use the raw message. If there is no date format string, the default date format is:\nwith the milliseconds tacked on at the end. The style is one of '%', '{', or '$'. If one of these is not specified, then '%' will be used.\nIf the style is '%', the message format string uses %(&lt;dictionary key&gt;)s styled string substitution; the possible keys are documented in LogRecord attributes. If the style is '{', the message format string is assumed to be compatible with str.format() (using keyword arguments), while if the style is '$' then the message format string should conform to what is expected by string.Template.substitute().\nChanged in version 3.2: Added the style parameter.\nThe following message format string will log the time in a human-readable format, the severity of the message, and the contents of the message, in that order:\n'%(asctime)s - %(levelname)s - %(message)s'\n\nFormatters use a user-configurable function to convert the creation time of a record to a tuple. By default, time.localtime() is used; to change this for a particular formatter instance, set the converter attribute of the instance to a function with the same signature as time.localtime() or time.gmtime(). To change it for all formatters, for example if you want all logging times to be shown in GMT, set the converter attribute in the Formatter class (to time.gmtime for GMT display).\n\n\n\nProgrammers can configure logging in three ways:\n\nCreating loggers, handlers, and formatters explicitly using Python code that calls the configuration methods listed above.\nCreating a logging config file and reading it using the fileConfig() function.\nCreating a dictionary of configuration information and passing it to the dictConfig() function.\n\nFor the reference documentation on the last two options, see Configuration functions. The following example configures a very simple logger, a console handler, and a simple formatter using Python code:\nimport logging\n\n# create logger\nlogger = logging.getLogger('simple_example')\nlogger.setLevel(logging.DEBUG)\n\n# create console handler and set level to debug\nch = logging.StreamHandler()\nch.setLevel(logging.DEBUG)\n\n# create formatter\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n# add formatter to ch\nch.setFormatter(formatter)\n\n# add ch to logger\nlogger.addHandler(ch)\n\n# 'application' code\nlogger.debug('debug message')\nlogger.info('info message')\nlogger.warning('warn message')\nlogger.error('error message')\nlogger.critical('critical message')\n\nRunning this module from the command line produces the following output:\n$ python simple_logging_module.py\n2005-03-19 15:10:26,618 - simple_example - DEBUG - debug message\n2005-03-19 15:10:26,620 - simple_example - INFO - info message\n2005-03-19 15:10:26,695 - simple_example - WARNING - warn message\n2005-03-19 15:10:26,697 - simple_example - ERROR - error message\n2005-03-19 15:10:26,773 - simple_example - CRITICAL - critical message\n\nThe following Python module creates a logger, handler, and formatter nearly identical to those in the example listed above, with the only difference being the names of the objects:\nimport logging\nimport logging.config\n\nlogging.config.fileConfig('logging.conf')\n\n# create logger\nlogger = logging.getLogger('simpleExample')\n\n# 'application' code\nlogger.debug('debug message')\nlogger.info('info message')\nlogger.warning('warn message')\nlogger.error('error message')\nlogger.critical('critical message')\n\nHere is the logging.conf file:\n[loggers]\nkeys=root,simpleExample\n\n[handlers]\nkeys=consoleHandler\n\n[formatters]\nkeys=simpleFormatter\n\n[logger_root]\nlevel=DEBUG\nhandlers=consoleHandler\n\n[logger_simpleExample]\nlevel=DEBUG\nhandlers=consoleHandler\nqualname=simpleExample\npropagate=0\n\n[handler_consoleHandler]\nclass=StreamHandler\nlevel=DEBUG\nformatter=simpleFormatter\nargs=(sys.stdout,)\n\n[formatter_simpleFormatter]\nformat=%(asctime)s - %(name)s - %(levelname)s - %(message)s\n\nThe output is nearly identical to that of the non-config-file-based example:\n$ python simple_logging_config.py\n2005-03-19 15:38:55,977 - simpleExample - DEBUG - debug message\n2005-03-19 15:38:55,979 - simpleExample - INFO - info message\n2005-03-19 15:38:56,054 - simpleExample - WARNING - warn message\n2005-03-19 15:38:56,055 - simpleExample - ERROR - error message\n2005-03-19 15:38:56,130 - simpleExample - CRITICAL - critical message\n\nYou can see that the config file approach has a few advantages over the Python code approach, mainly separation of configuration and code and the ability of noncoders to easily modify the logging properties.\nWarning\nThe fileConfig() function takes a default parameter, disable_existing_loggers, which defaults to True for reasons of backward compatibility. This may or may not be what you want, since it will cause any non-root loggers existing before the fileConfig() call to be disabled unless they (or an ancestor) are explicitly named in the configuration. Please refer to the reference documentation for more information, and specify False for this parameter if you wish.\nThe dictionary passed to dictConfig() can also specify a Boolean value with key disable_existing_loggers, which if not specified explicitly in the dictionary also defaults to being interpreted as True. This leads to the logger-disabling behaviour described above, which may not be what you want - in which case, provide the key explicitly with a value of False.\nNote that the class names referenced in config files need to be either relative to the logging module, or absolute values which can be resolved using normal import mechanisms. Thus, you could use either WatchedFileHandler (relative to the logging module) or mypackage.mymodule.MyHandler (for a class defined in package mypackage and module mymodule, where mypackage is available on the Python import path).\nIn Python 3.2, a new means of configuring logging has been introduced, using dictionaries to hold configuration information. This provides a superset of the functionality of the config-file-based approach outlined above, and is the recommended configuration method for new applications and deployments. Because a Python dictionary is used to hold configuration information, and since you can populate that dictionary using different means, you have more options for configuration. For example, you can use a configuration file in JSON format, or, if you have access to YAML processing functionality, a file in YAML format, to populate the configuration dictionary. Or, of course, you can construct the dictionary in Python code, receive it in pickled form over a socket, or use whatever approach makes sense for your application.\nHere’s an example of the same configuration as above, in YAML format for the new dictionary-based approach:\nversion: 1\nformatters:\n  simple:\n    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\nhandlers:\n  console:\n    class: logging.StreamHandler\n    level: DEBUG\n    formatter: simple\n    stream: ext://sys.stdout\nloggers:\n  simpleExample:\n    level: DEBUG\n    handlers: [console]\n    propagate: no\nroot:\n  level: DEBUG\n  handlers: [console]\n\nFor more information about logging using a dictionary, see Configuration functions.\n\n\n\nIf no logging configuration is provided, it is possible to have a situation where a logging event needs to be output, but no handlers can be found to output the event.\nThe event is output using a ‘handler of last resort’, stored in lastResort. This internal handler is not associated with any logger, and acts like a StreamHandler which writes the event description message to the current value of sys.stderr (therefore respecting any redirections which may be in effect). No formatting is done on the message - just the bare event description message is printed. The handler’s level is set to WARNING, so all events at this and greater severities will be output.\nChanged in version 3.2: For versions of Python prior to 3.2, the behaviour is as follows:\n\nIf raiseExceptions is False (production mode), the event is silently dropped.\nIf raiseExceptions is True (development mode), a message ‘No handlers could be found for logger X.Y.Z’ is printed once.\n\nTo obtain the pre-3.2 behaviour, lastResort can be set to None.\n\n\n\nWhen developing a library which uses logging, you should take care to document how the library uses logging - for example, the names of loggers used. Some consideration also needs to be given to its logging configuration. If the using application does not use logging, and library code makes logging calls, then (as described in the previous section) events of severity WARNING and greater will be printed to sys.stderr. This is regarded as the best default behaviour.\nIf for some reason you don’t want these messages printed in the absence of any logging configuration, you can attach a do-nothing handler to the top-level logger for your library. This avoids the message being printed, since a handler will always be found for the library’s events: it just doesn’t produce any output. If the library user configures logging for application use, presumably that configuration will add some handlers, and if levels are suitably configured then logging calls made in library code will send output to those handlers, as normal.\nA do-nothing handler is included in the logging package: NullHandler (since Python 3.1). An instance of this handler could be added to the top-level logger of the logging namespace used by the library (if you want to prevent your library’s logged events being output to sys.stderr in the absence of logging configuration). If all logging by a library foo is done using loggers with names matching ‘foo.x’, ‘foo.x.y’, etc. then the code:\nimport logging\nlogging.getLogger('foo').addHandler(logging.NullHandler())\n\nshould have the desired effect. If an organisation produces a number of libraries, then the logger name specified can be ‘orgname.foo’ rather than just ‘foo’.\nNote\nIt is strongly advised that you do not log to the root logger in your library. Instead, use a logger with a unique and easily identifiable name, such as the __name__ for your library’s top-level package or module. Logging to the root logger will make it difficult or impossible for the application developer to configure the logging verbosity or handlers of your library as they wish.\nNote\nIt is strongly advised that you do not add any handlers other than NullHandler to your library’s loggers. This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers ‘under the hood’, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html#logging-levels",
    "href": "knowledgebase/python/python3_logging_howto.html#logging-levels",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "The numeric values of logging levels are given in the following table. These are primarily of interest if you want to define your own levels, and need them to have specific values relative to the predefined levels. If you define a level with the same numeric value, it overwrites the predefined value; the predefined name is lost.\n\nLevel\n|\nNumeric value\n\n\n\n\n\n\n\n\nCRITICAL\n|\n50\n| |\nERROR\n|\n40\n| |\nWARNING\n|\n30\n| |\nINFO\n|\n20\n| |\nDEBUG\n|\n10\n| |\nNOTSET\n|\n0\n|\nLevels can also be associated with loggers, being set either by the developer or through loading a saved logging configuration. When a logging method is called on a logger, the logger compares its own level with the level associated with the method call. If the logger’s level is higher than the method call’s, no logging message is actually generated. This is the basic mechanism controlling the verbosity of logging output.\nLogging messages are encoded as instances of the LogRecord class. When a logger decides to actually log an event, a LogRecord instance is created from the logging message.\nLogging messages are subjected to a dispatch mechanism through the use of handlers, which are instances of subclasses of the Handler class. Handlers are responsible for ensuring that a logged message (in the form of a LogRecord) ends up in a particular location (or set of locations) which is useful for the target audience for that message (such as end users, support desk staff, system administrators, developers). Handlers are passed LogRecord instances intended for particular destinations. Each logger can have zero, one or more handlers associated with it (via the addHandler() method of Logger). In addition to any handlers directly associated with a logger, all handlers associated with all ancestors of the logger are called to dispatch the message (unless the propagate flag for a logger is set to a false value, at which point the passing to ancestor handlers stops).\nJust as for loggers, handlers can have levels associated with them. A handler’s level acts as a filter in the same way as a logger’s level does. If a handler decides to actually dispatch an event, the emit() method is used to send the message to its destination. Most user-defined subclasses of Handler will need to override this emit().\n\n\nDefining your own levels is possible, but should not be necessary, as the existing levels have been chosen on the basis of practical experience. However, if you are convinced that you need custom levels, great care should be exercised when doing this, and it is possibly a very bad idea to define custom levels if you are developing a library. That’s because if multiple library authors all define their own custom levels, there is a chance that the logging output from such multiple libraries used together will be difficult for the using developer to control and/or interpret, because a given numeric value might mean different things for different libraries."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html#useful-handlers",
    "href": "knowledgebase/python/python3_logging_howto.html#useful-handlers",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "In addition to the base Handler class, many useful subclasses are provided:\n\nStreamHandler instances send messages to streams (file-like objects).\nFileHandler instances send messages to disk files.\nBaseRotatingHandler is the base class for handlers that rotate log files at a certain point. It is not meant to be instantiated directly. Instead, use RotatingFileHandler or TimedRotatingFileHandler.\nRotatingFileHandler instances send messages to disk files, with support for maximum log file sizes and log file rotation.\nTimedRotatingFileHandler instances send messages to disk files, rotating the log file at certain timed intervals.\nSocketHandler instances send messages to TCP/IP sockets. Since 3.4, Unix domain sockets are also supported.\nDatagramHandler instances send messages to UDP sockets. Since 3.4, Unix domain sockets are also supported.\nSMTPHandler instances send messages to a designated email address.\nSysLogHandler instances send messages to a Unix syslog daemon, possibly on a remote machine.\nNTEventLogHandler instances send messages to a Windows NT/2000/XP event log.\nMemoryHandler instances send messages to a buffer in memory, which is flushed whenever specific criteria are met.\nHTTPHandler instances send messages to an HTTP server using either GET or POST semantics.\nWatchedFileHandler instances watch the file they are logging to. If the file changes, it is closed and reopened using the file name. This handler is only useful on Unix-like systems; Windows does not support the underlying mechanism used.\nQueueHandler instances send messages to a queue, such as those implemented in the queue or multiprocessing modules.\nNullHandler instances do nothing with error messages. They are used by library developers who want to use logging, but want to avoid the ‘No handlers could be found for logger XXX’ message which can be displayed if the library user has not configured logging. See Configuring Logging for a Library for more information.\n\nThe NullHandler, StreamHandler and FileHandler classes are defined in the core logging package. The other handlers are defined in a sub-module, logging.handlers. (There is also another sub-module, logging.config, for configuration functionality.)\nLogged messages are formatted for presentation through instances of the Formatter class. They are initialized with a format string suitable for use with the % operator and a dictionary.\nFor formatting multiple messages in a batch, instances of BufferingFormatter can be used. In addition to the format string (which is applied to each message in the batch), there is provision for header and trailer format strings.\nWhen filtering based on logger level and/or handler level is not enough, instances of Filter can be added to both Logger and Handler instances (through their addFilter() method). Before deciding to process a message further, both loggers and handlers consult all their filters for permission. If any filter returns a false value, the message is not processed further.\nThe basic Filter functionality allows filtering by specific logger name. If this feature is used, messages sent to the named logger and its children are allowed through the filter, and all others dropped."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html#exceptions-raised-during-logging",
    "href": "knowledgebase/python/python3_logging_howto.html#exceptions-raised-during-logging",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "The logging package is designed to swallow exceptions which occur while logging in production. This is so that errors which occur while handling logging events - such as logging misconfiguration, network or other similar errors - do not cause the application using logging to terminate prematurely.\nSystemExit and KeyboardInterrupt exceptions are never swallowed. Other exceptions which occur during the emit() method of a Handler subclass are passed to its handleError() method.\nThe default implementation of handleError() in Handler checks to see if a module-level variable, raiseExceptions, is set. If set, a traceback is printed to sys.stderr. If not set, the exception is swallowed.\nNote\nThe default value of raiseExceptions is True. This is because during development, you typically want to be notified of any exceptions that occur. It’s advised that you set raiseExceptions to False for production usage."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html#using-arbitrary-objects-as-messages",
    "href": "knowledgebase/python/python3_logging_howto.html#using-arbitrary-objects-as-messages",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "In the preceding sections and examples, it has been assumed that the message passed when logging the event is a string. However, this is not the only possibility. You can pass an arbitrary object as a message, and its __str__() method will be called when the logging system needs to convert it to a string representation. In fact, if you want to, you can avoid computing a string representation altogether - for example, the SocketHandler emits an event by pickling it and sending it over the wire."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_howto.html#optimization",
    "href": "knowledgebase/python/python3_logging_howto.html#optimization",
    "title": "Logging HOWTO — Python 3.13.1 documentation",
    "section": "",
    "text": "Formatting of message arguments is deferred until it cannot be avoided. However, computing the arguments passed to the logging method can also be expensive, and you may want to avoid doing it if the logger will just throw away your event. To decide what to do, you can call the isEnabledFor() method which takes a level argument and returns true if the event would be created by the Logger for that level of call. You can write code like this:\nif logger.isEnabledFor(logging.DEBUG):\n    logger.debug('Message with %s, %s', expensive_func1(),\n                                        expensive_func2())\n\nso that if the logger’s threshold is set above DEBUG, the calls to expensive_func1 and expensive_func2 are never made.\nNote\nIn some cases, isEnabledFor() can itself be more expensive than you’d like (e.g. for deeply nested loggers where an explicit level is only set high up in the logger hierarchy). In such cases (or if you want to avoid calling a method in tight loops), you can cache the result of a call to isEnabledFor() in a local or instance variable, and use that instead of calling the method each time. Such a cached value would only need to be recomputed when the logging configuration changes dynamically while the application is running (which is not all that common).\nThere are other optimizations which can be made for specific applications which need more precise control over what logging information is collected. Here’s a list of things you can do to avoid processing during logging which you don’t need:\n\nWhat you don’t want to collect\n|\nHow to avoid collecting it\n\n\n\n\n\n\n\n\nInformation about where calls were made from.\n|\nSet logging._srcfile to None. This avoids calling sys._getframe(), which may help to speed up your code in environments like PyPy (which can’t speed up code that uses sys._getframe()).\n| |\nThreading information.\n|\nSet logging.logThreads to False.\n| |\nCurrent process ID (os.getpid())\n|\nSet logging.logProcesses to False.\n| |\nCurrent process name when using multiprocessing to manage multiple processes.\n|\nSet logging.logMultiprocessing to False.\n| |\nCurrent asyncio.Task name when using asyncio.\n|\nSet logging.logAsyncioTasks to False.\n|\nAlso note that the core logging module only includes the basic handlers. If you don’t import logging.handlers and logging.config, they won’t take up any memory."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html",
    "href": "knowledgebase/python/python3_logging_cookbook.html",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "This page contains a number of recipes related to logging, which have been found useful in the past. For links to tutorial and reference information, please see Other resources.\n\n\nMultiple calls to logging.getLogger('someLogger') return a reference to the same logger object. This is true not only within the same module, but also across modules as long as it is in the same Python interpreter process. It is true for references to the same object; additionally, application code can define and configure a parent logger in one module and create (but not configure) a child logger in a separate module, and all logger calls to the child will pass up to the parent. Here is a main module:\nimport logging\nimport auxiliary_module\n\n# create logger with 'spam_application'\nlogger = logging.getLogger('spam_application')\nlogger.setLevel(logging.DEBUG)\n# create file handler which logs even debug messages\nfh = logging.FileHandler('spam.log')\nfh.setLevel(logging.DEBUG)\n# create console handler with a higher log level\nch = logging.StreamHandler()\nch.setLevel(logging.ERROR)\n# create formatter and add it to the handlers\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\n# add the handlers to the logger\nlogger.addHandler(fh)\nlogger.addHandler(ch)\n\nlogger.info('creating an instance of auxiliary_module.Auxiliary')\na = auxiliary_module.Auxiliary()\nlogger.info('created an instance of auxiliary_module.Auxiliary')\nlogger.info('calling auxiliary_module.Auxiliary.do_something')\na.do_something()\nlogger.info('finished auxiliary_module.Auxiliary.do_something')\nlogger.info('calling auxiliary_module.some_function()')\nauxiliary_module.some_function()\nlogger.info('done with auxiliary_module.some_function()')\n\nHere is the auxiliary module:\nimport logging\n\n# create logger\nmodule_logger = logging.getLogger('spam_application.auxiliary')\n\nclass Auxiliary:\n    def __init__(self):\n        self.logger = logging.getLogger('spam_application.auxiliary.Auxiliary')\n        self.logger.info('creating an instance of Auxiliary')\n\n    def do_something(self):\n        self.logger.info('doing something')\n        a = 1 + 1\n        self.logger.info('done doing something')\n\ndef some_function():\n    module_logger.info('received a call to \"some_function\"')\n\nThe output looks like this:\n2005-03-23 23:47:11,663 - spam_application - INFO -\n   creating an instance of auxiliary_module.Auxiliary\n2005-03-23 23:47:11,665 - spam_application.auxiliary.Auxiliary - INFO -\n   creating an instance of Auxiliary\n2005-03-23 23:47:11,665 - spam_application - INFO -\n   created an instance of auxiliary_module.Auxiliary\n2005-03-23 23:47:11,668 - spam_application - INFO -\n   calling auxiliary_module.Auxiliary.do_something\n2005-03-23 23:47:11,668 - spam_application.auxiliary.Auxiliary - INFO -\n   doing something\n2005-03-23 23:47:11,669 - spam_application.auxiliary.Auxiliary - INFO -\n   done doing something\n2005-03-23 23:47:11,670 - spam_application - INFO -\n   finished auxiliary_module.Auxiliary.do_something\n2005-03-23 23:47:11,671 - spam_application - INFO -\n   calling auxiliary_module.some_function()\n2005-03-23 23:47:11,672 - spam_application.auxiliary - INFO -\n   received a call to 'some_function'\n2005-03-23 23:47:11,673 - spam_application - INFO -\n   done with auxiliary_module.some_function()\n\n\n\n\nLogging from multiple threads requires no special effort. The following example shows logging from the main (initial) thread and another thread:\nimport logging\nimport threading\nimport time\n\ndef worker(arg):\n    while not arg['stop']:\n        logging.debug('Hi from myfunc')\n        time.sleep(0.5)\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG, format='%(relativeCreated)6d %(threadName)s %(message)s')\n    info = {'stop': False}\n    thread = threading.Thread(target=worker, args=(info,))\n    thread.start()\n    while True:\n        try:\n            logging.debug('Hello from main')\n            time.sleep(0.75)\n        except KeyboardInterrupt:\n            info['stop'] = True\n            break\n    thread.join()\n\nif __name__ == '__main__':\n    main()\n\nWhen run, the script should print something like the following:\n   0 Thread-1 Hi from myfunc\n   3 MainThread Hello from main\n 505 Thread-1 Hi from myfunc\n 755 MainThread Hello from main\n1007 Thread-1 Hi from myfunc\n1507 MainThread Hello from main\n1508 Thread-1 Hi from myfunc\n2010 Thread-1 Hi from myfunc\n2258 MainThread Hello from main\n2512 Thread-1 Hi from myfunc\n3009 MainThread Hello from main\n3013 Thread-1 Hi from myfunc\n3515 Thread-1 Hi from myfunc\n3761 MainThread Hello from main\n4017 Thread-1 Hi from myfunc\n4513 MainThread Hello from main\n4518 Thread-1 Hi from myfunc\n\nThis shows the logging output interspersed as one might expect. This approach works for more threads than shown here, of course.\n\n\n\nLoggers are plain Python objects. The addHandler() method has no minimum or maximum quota for the number of handlers you may add. Sometimes it will be beneficial for an application to log all messages of all severities to a text file while simultaneously logging errors or above to the console. To set this up, simply configure the appropriate handlers. The logging calls in the application code will remain unchanged. Here is a slight modification to the previous simple module-based configuration example:\nimport logging\n\nlogger = logging.getLogger('simple_example')\nlogger.setLevel(logging.DEBUG)\n# create file handler which logs even debug messages\nfh = logging.FileHandler('spam.log')\nfh.setLevel(logging.DEBUG)\n# create console handler with a higher log level\nch = logging.StreamHandler()\nch.setLevel(logging.ERROR)\n# create formatter and add it to the handlers\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nch.setFormatter(formatter)\nfh.setFormatter(formatter)\n# add the handlers to logger\nlogger.addHandler(ch)\nlogger.addHandler(fh)\n\n# 'application' code\nlogger.debug('debug message')\nlogger.info('info message')\nlogger.warning('warn message')\nlogger.error('error message')\nlogger.critical('critical message')\n\nNotice that the ‘application’ code does not care about multiple handlers. All that changed was the addition and configuration of a new handler named fh.\nThe ability to create new handlers with higher- or lower-severity filters can be very helpful when writing and testing an application. Instead of using many print statements for debugging, use logger.debug: Unlike the print statements, which you will have to delete or comment out later, the logger.debug statements can remain intact in the source code and remain dormant until you need them again. At that time, the only change that needs to happen is to modify the severity level of the logger and/or handler to debug.\n\n\n\nLet’s say you want to log to console and file with different message formats and in differing circumstances. Say you want to log messages with levels of DEBUG and higher to file, and those messages at level INFO and higher to the console. Let’s also assume that the file should contain timestamps, but the console messages should not. Here’s how you can achieve this:\nimport logging\n\n# set up logging to file - see previous section for more details\nlogging.basicConfig(level=logging.DEBUG,\n                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n                    datefmt='%m-%d %H:%M',\n                    filename='/tmp/myapp.log',\n                    filemode='w')\n# define a Handler which writes INFO messages or higher to the sys.stderr\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n# set a format which is simpler for console use\nformatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n# tell the handler to use this format\nconsole.setFormatter(formatter)\n# add the handler to the root logger\nlogging.getLogger('').addHandler(console)\n\n# Now, we can log to the root logger, or any other logger. First the root...\nlogging.info('Jackdaws love my big sphinx of quartz.')\n\n# Now, define a couple of other loggers which might represent areas in your\n# application:\n\nlogger1 = logging.getLogger('myapp.area1')\nlogger2 = logging.getLogger('myapp.area2')\n\nlogger1.debug('Quick zephyrs blow, vexing daft Jim.')\nlogger1.info('How quickly daft jumping zebras vex.')\nlogger2.warning('Jail zesty vixen who grabbed pay from quack.')\nlogger2.error('The five boxing wizards jump quickly.')\n\nWhen you run this, on the console you will see\nroot        : INFO     Jackdaws love my big sphinx of quartz.\nmyapp.area1 : INFO     How quickly daft jumping zebras vex.\nmyapp.area2 : WARNING  Jail zesty vixen who grabbed pay from quack.\nmyapp.area2 : ERROR    The five boxing wizards jump quickly.\n\nand in the file you will see something like\n10-22 22:19 root         INFO     Jackdaws love my big sphinx of quartz.\n10-22 22:19 myapp.area1  DEBUG    Quick zephyrs blow, vexing daft Jim.\n10-22 22:19 myapp.area1  INFO     How quickly daft jumping zebras vex.\n10-22 22:19 myapp.area2  WARNING  Jail zesty vixen who grabbed pay from quack.\n10-22 22:19 myapp.area2  ERROR    The five boxing wizards jump quickly.\n\nAs you can see, the DEBUG message only shows up in the file. The other messages are sent to both destinations.\nThis example uses console and file handlers, but you can use any number and combination of handlers you choose.\nNote that the above choice of log filename /tmp/myapp.log implies use of a standard location for temporary files on POSIX systems. On Windows, you may need to choose a different directory name for the log - just ensure that the directory exists and that you have the permissions to create and update files in it.\n\n\n\nSometimes, you might want to do something slightly different from the standard handling of levels in handlers, where all levels above a threshold get processed by a handler. To do this, you need to use filters. Let’s look at a scenario where you want to arrange things as follows:\n\nSend messages of severity INFO and WARNING to sys.stdout\nSend messages of severity ERROR and above to sys.stderr\nSend messages of severity DEBUG and above to file app.log\n\nSuppose you configure logging with the following JSON:\n{\n    \"version\": 1,\n    \"disable_existing_loggers\": false,\n    \"formatters\": {\n        \"simple\": {\n            \"format\": \"%(levelname)-8s - %(message)s\"\n        }\n    },\n    \"handlers\": {\n        \"stdout\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"INFO\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stdout\"\n        },\n        \"stderr\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"ERROR\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stderr\"\n        },\n        \"file\": {\n            \"class\": \"logging.FileHandler\",\n            \"formatter\": \"simple\",\n            \"filename\": \"app.log\",\n            \"mode\": \"w\"\n        }\n    },\n    \"root\": {\n        \"level\": \"DEBUG\",\n        \"handlers\": [\n            \"stderr\",\n            \"stdout\",\n            \"file\"\n        ]\n    }\n}\n\nThis configuration does almost what we want, except that sys.stdout would show messages of severity ERROR and only events of this severity and higher will be tracked as well as INFO and WARNING messages. To prevent this, we can set up a filter which excludes those messages and add it to the relevant handler. This can be configured by adding a filters section parallel to formatters and handlers:\n{\n    \"filters\": {\n        \"warnings_and_below\": {\n            \"()\" : \"__main__.filter_maker\",\n            \"level\": \"WARNING\"\n        }\n    }\n}\n\nand changing the section on the stdout handler to add it:\n{\n    \"stdout\": {\n        \"class\": \"logging.StreamHandler\",\n        \"level\": \"INFO\",\n        \"formatter\": \"simple\",\n        \"stream\": \"ext://sys.stdout\",\n        \"filters\": [\"warnings_and_below\"]\n    }\n}\n\nA filter is just a function, so we can define the filter_maker (a factory function) as follows:\ndef filter_maker(level):\n    level = getattr(logging, level)\n\n    def filter(record):\n        return record.levelno &lt;= level\n\n    return filter\n\nThis converts the string argument passed in to a numeric level, and returns a function which only returns True if the level of the passed in record is at or below the specified level. Note that in this example I have defined the filter_maker in a test script main.py that I run from the command line, so its module will be __main__ - hence the __main__.filter_maker in the filter configuration. You will need to change that if you define it in a different module.\nWith the filter added, we can run main.py, which in full is:\nimport json\nimport logging\nimport logging.config\n\nCONFIG = '''\n{\n    \"version\": 1,\n    \"disable_existing_loggers\": false,\n    \"formatters\": {\n        \"simple\": {\n            \"format\": \"%(levelname)-8s - %(message)s\"\n        }\n    },\n    \"filters\": {\n        \"warnings_and_below\": {\n            \"()\" : \"__main__.filter_maker\",\n            \"level\": \"WARNING\"\n        }\n    },\n    \"handlers\": {\n        \"stdout\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"INFO\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stdout\",\n            \"filters\": [\"warnings_and_below\"]\n        },\n        \"stderr\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"ERROR\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stderr\"\n        },\n        \"file\": {\n            \"class\": \"logging.FileHandler\",\n            \"formatter\": \"simple\",\n            \"filename\": \"app.log\",\n            \"mode\": \"w\"\n        }\n    },\n    \"root\": {\n        \"level\": \"DEBUG\",\n        \"handlers\": [\n            \"stderr\",\n            \"stdout\",\n            \"file\"\n        ]\n    }\n}\n'''\n\ndef filter_maker(level):\n    level = getattr(logging, level)\n\n    def filter(record):\n        return record.levelno &lt;= level\n\n    return filter\n\nlogging.config.dictConfig(json.loads(CONFIG))\nlogging.debug('A DEBUG message')\nlogging.info('An INFO message')\nlogging.warning('A WARNING message')\nlogging.error('An ERROR message')\nlogging.critical('A CRITICAL message')\n\nAnd after running it like this:\npython main.py 2&gt;stderr.log &gt;stdout.log\n\nWe can see the results are as expected:\n$ more *.log\n::::::::::::::\napp.log\n::::::::::::::\nDEBUG    - A DEBUG message\nINFO     - An INFO message\nWARNING  - A WARNING message\nERROR    - An ERROR message\nCRITICAL - A CRITICAL message\n::::::::::::::\nstderr.log\n::::::::::::::\nERROR    - An ERROR message\nCRITICAL - A CRITICAL message\n::::::::::::::\nstdout.log\n::::::::::::::\nINFO     - An INFO message\nWARNING  - A WARNING message\n\n\n\n\nHere is an example of a module using the logging configuration server:\nimport logging\nimport logging.config\nimport time\nimport os\n\n# read initial config file\nlogging.config.fileConfig('logging.conf')\n\n# create and start listener on port 9999\nt = logging.config.listen(9999)\nt.start()\n\nlogger = logging.getLogger('simpleExample')\n\ntry:\n    # loop through logging calls to see the difference\n    # new configurations make, until Ctrl+C is pressed\n    while True:\n        logger.debug('debug message')\n        logger.info('info message')\n        logger.warning('warn message')\n        logger.error('error message')\n        logger.critical('critical message')\n        time.sleep(5)\nexcept KeyboardInterrupt:\n    # cleanup\n    logging.config.stopListening()\n    t.join()\n\nAnd here is a script that takes a filename and sends that file to the server, properly preceded with the binary-encoded length, as the new logging configuration:\n#!/usr/bin/env python\nimport socket, sys, struct\n\nwith open(sys.argv[1], 'rb') as f:\n    data_to_send = f.read()\n\nHOST = 'localhost'\nPORT = 9999\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nprint('connecting...')\ns.connect((HOST, PORT))\nprint('sending config...')\ns.send(struct.pack('&gt;L', len(data_to_send)))\ns.send(data_to_send)\ns.close()\nprint('complete')\n\n\n\n\nSometimes you have to get your logging handlers to do their work without blocking the thread you’re logging from. This is common in web applications, though of course it also occurs in other scenarios.\nA common culprit which demonstrates sluggish behaviour is the SMTPHandler: sending emails can take a long time, for a number of reasons outside the developer’s control (for example, a poorly performing mail or network infrastructure). But almost any network-based handler can block: Even a SocketHandler operation may do a DNS query under the hood which is too slow (and this query can be deep in the socket library code, below the Python layer, and outside your control).\nOne solution is to use a two-part approach. For the first part, attach only a QueueHandler to those loggers which are accessed from performance-critical threads. They simply write to their queue, which can be sized to a large enough capacity or initialized with no upper bound to their size. The write to the queue will typically be accepted quickly, though you will probably need to catch the queue.Full exception as a precaution in your code. If you are a library developer who has performance-critical threads in their code, be sure to document this (together with a suggestion to attach only QueueHandlers to your loggers) for the benefit of other developers who will use your code.\nThe second part of the solution is QueueListener, which has been designed as the counterpart to QueueHandler. A QueueListener is very simple: it’s passed a queue and some handlers, and it fires up an internal thread which listens to its queue for LogRecords sent from QueueHandlers (or any other source of LogRecords, for that matter). The LogRecords are removed from the queue and passed to the handlers for processing.\nThe advantage of having a separate QueueListener class is that you can use the same instance to service multiple QueueHandlers. This is more resource-friendly than, say, having threaded versions of the existing handler classes, which would eat up one thread per handler for no particular benefit.\nAn example of using these two classes follows (imports omitted):\nque = queue.Queue(-1)  # no limit on size\nqueue_handler = QueueHandler(que)\nhandler = logging.StreamHandler()\nlistener = QueueListener(que, handler)\nroot = logging.getLogger()\nroot.addHandler(queue_handler)\nformatter = logging.Formatter('%(threadName)s: %(message)s')\nhandler.setFormatter(formatter)\nlistener.start()\n# The log output will display the thread which generated\n# the event (the main thread) rather than the internal\n# thread which monitors the internal queue. This is what\n# you want to happen.\nroot.warning('Look out!')\nlistener.stop()\n\nwhich, when run, will produce:\nNote\nAlthough the earlier discussion wasn’t specifically talking about async code, but rather about slow logging handlers, it should be noted that when logging from async code, network and even file handlers could lead to problems (blocking the event loop) because some logging is done from asyncio internals. It might be best, if any async code is used in an application, to use the above approach for logging, so that any blocking code runs only in the QueueListener thread.\nChanged in version 3.5: Prior to Python 3.5, the QueueListener always passed every message received from the queue to every handler it was initialized with. (This was because it was assumed that level filtering was all done on the other side, where the queue is filled.) From 3.5 onwards, this behaviour can be changed by passing a keyword argument respect_handler_level=True to the listener’s constructor. When this is done, the listener compares the level of each message with the handler’s level, and only passes a message to a handler if it’s appropriate to do so.\n\n\n\nLet’s say you want to send logging events across a network, and handle them at the receiving end. A simple way of doing this is attaching a SocketHandler instance to the root logger at the sending end:\nimport logging, logging.handlers\n\nrootLogger = logging.getLogger('')\nrootLogger.setLevel(logging.DEBUG)\nsocketHandler = logging.handlers.SocketHandler('localhost',\n                    logging.handlers.DEFAULT_TCP_LOGGING_PORT)\n# don't bother with a formatter, since a socket handler sends the event as\n# an unformatted pickle\nrootLogger.addHandler(socketHandler)\n\n# Now, we can log to the root logger, or any other logger. First the root...\nlogging.info('Jackdaws love my big sphinx of quartz.')\n\n# Now, define a couple of other loggers which might represent areas in your\n# application:\n\nlogger1 = logging.getLogger('myapp.area1')\nlogger2 = logging.getLogger('myapp.area2')\n\nlogger1.debug('Quick zephyrs blow, vexing daft Jim.')\nlogger1.info('How quickly daft jumping zebras vex.')\nlogger2.warning('Jail zesty vixen who grabbed pay from quack.')\nlogger2.error('The five boxing wizards jump quickly.')\n\nAt the receiving end, you can set up a receiver using the socketserver module. Here is a basic working example:\nimport pickle\nimport logging\nimport logging.handlers\nimport socketserver\nimport struct\n\n\nclass LogRecordStreamHandler(socketserver.StreamRequestHandler):\n    \"\"\"Handler for a streaming logging request.\n\n    This basically logs the record using whatever logging policy is\n    configured locally.\n    \"\"\"\n\n    def handle(self):\n        \"\"\"\n        Handle multiple requests - each expected to be a 4-byte length,\n        followed by the LogRecord in pickle format. Logs the record\n        according to whatever policy is configured locally.\n        \"\"\"\n        while True:\n            chunk = self.connection.recv(4)\n            if len(chunk) &lt; 4:\n                break\n            slen = struct.unpack('&gt;L', chunk)[0]\n            chunk = self.connection.recv(slen)\n            while len(chunk) &lt; slen:\n                chunk = chunk + self.connection.recv(slen - len(chunk))\n            obj = self.unPickle(chunk)\n            record = logging.makeLogRecord(obj)\n            self.handleLogRecord(record)\n\n    def unPickle(self, data):\n        return pickle.loads(data)\n\n    def handleLogRecord(self, record):\n        # if a name is specified, we use the named logger rather than the one\n        # implied by the record.\n        if self.server.logname is not None:\n            name = self.server.logname\n        else:\n            name = record.name\n        logger = logging.getLogger(name)\n        # N.B. EVERY record gets logged. This is because Logger.handle\n        # is normally called AFTER logger-level filtering. If you want\n        # to do filtering, do it at the client end to save wasting\n        # cycles and network bandwidth!\n        logger.handle(record)\n\nclass LogRecordSocketReceiver(socketserver.ThreadingTCPServer):\n    \"\"\"\n    Simple TCP socket-based logging receiver suitable for testing.\n    \"\"\"\n\n    allow_reuse_address = True\n\n    def __init__(self, host='localhost',\n                 port=logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n                 handler=LogRecordStreamHandler):\n        socketserver.ThreadingTCPServer.__init__(self, (host, port), handler)\n        self.abort = 0\n        self.timeout = 1\n        self.logname = None\n\n    def serve_until_stopped(self):\n        import select\n        abort = 0\n        while not abort:\n            rd, wr, ex = select.select([self.socket.fileno()],\n                                       [], [],\n                                       self.timeout)\n            if rd:\n                self.handle_request()\n            abort = self.abort\n\ndef main():\n    logging.basicConfig(\n        format='%(relativeCreated)5d %(name)-15s %(levelname)-8s %(message)s')\n    tcpserver = LogRecordSocketReceiver()\n    print('About to start TCP server...')\n    tcpserver.serve_until_stopped()\n\nif __name__ == '__main__':\n    main()\n\nFirst run the server, and then the client. On the client side, nothing is printed on the console; on the server side, you should see something like:\nAbout to start TCP server...\n   59 root            INFO     Jackdaws love my big sphinx of quartz.\n   59 myapp.area1     DEBUG    Quick zephyrs blow, vexing daft Jim.\n   69 myapp.area1     INFO     How quickly daft jumping zebras vex.\n   69 myapp.area2     WARNING  Jail zesty vixen who grabbed pay from quack.\n   69 myapp.area2     ERROR    The five boxing wizards jump quickly.\n\nNote that there are some security issues with pickle in some scenarios. If these affect you, you can use an alternative serialization scheme by overriding the makePickle() method and implementing your alternative there, as well as adapting the above script to use your alternative serialization.\n\n\nTo run a logging listener in production, you may need to use a process-management tool such as Supervisor. Here is a Gist which provides the bare-bones files to run the above functionality using Supervisor. It consists of the following files:\n\nFile\n|\nPurpose\n\n\n\n\n\n\n\n\nprepare.sh\n|\nA Bash script to prepare the environment for testing\n| |\nsupervisor.conf\n|\nThe Supervisor configuration file, which has entries for the listener and a multi-process web application\n| |\nensure_app.sh\n|\nA Bash script to ensure that Supervisor is running with the above configuration\n| |\nlog_listener.py\n|\nThe socket listener program which receives log events and records them to a file\n| |\nmain.py\n|\nA simple web application which performs logging via a socket connected to the listener\n| |\nwebapp.json\n|\nA JSON configuration file for the web application\n| |\nclient.py\n|\nA Python script to exercise the web application\n|\nThe web application uses Gunicorn, which is a popular web application server that starts multiple worker processes to handle requests. This example setup shows how the workers can write to the same log file without conflicting with one another — they all go through the socket listener.\nTo test these files, do the following in a POSIX environment:\n\nDownload the Gist as a ZIP archive using the Download ZIP button.\nUnzip the above files from the archive into a scratch directory.\nIn the scratch directory, run bash prepare.sh to get things ready. This creates a run subdirectory to contain Supervisor-related and log files, and a venv subdirectory to contain a virtual environment into which bottle, gunicorn and supervisor are installed.\nRun bash ensure_app.sh to ensure that Supervisor is running with the above configuration.\nRun venv/bin/python client.py to exercise the web application, which will lead to records being written to the log.\nInspect the log files in the run subdirectory. You should see the most recent log lines in files matching the pattern app.log*. They won’t be in any particular order, since they have been handled concurrently by different worker processes in a non-deterministic way.\nYou can shut down the listener and the web application by running venv/bin/supervisorctl -c supervisor.conf shutdown.\n\nYou may need to tweak the configuration files in the unlikely event that the configured ports clash with something else in your test environment.\n\n\n\n\nSometimes you want logging output to contain contextual information in addition to the parameters passed to the logging call. For example, in a networked application, it may be desirable to log client-specific information in the log (e.g. remote client’s username, or IP address). Although you could use the extra parameter to achieve this, it’s not always convenient to pass the information in this way. While it might be tempting to create Logger instances on a per-connection basis, this is not a good idea because these instances are not garbage collected. While this is not a problem in practice, when the number of Logger instances is dependent on the level of granularity you want to use in logging an application, it could be hard to manage if the number of Logger instances becomes effectively unbounded.\n\n\nAn easy way in which you can pass contextual information to be output along with logging event information is to use the LoggerAdapter class. This class is designed to look like a Logger, so that you can call debug(), info(), warning(), error(), exception(), critical() and log(). These methods have the same signatures as their counterparts in Logger, so you can use the two types of instances interchangeably.\nWhen you create an instance of LoggerAdapter, you pass it a Logger instance and a dict-like object which contains your contextual information. When you call one of the logging methods on an instance of LoggerAdapter, it delegates the call to the underlying instance of Logger passed to its constructor, and arranges to pass the contextual information in the delegated call. Here’s a snippet from the code of LoggerAdapter:\ndef debug(self, msg, /, *args, **kwargs):\n    \"\"\"\n    Delegate a debug call to the underlying logger, after adding\n    contextual information from this adapter instance.\n    \"\"\"\n    msg, kwargs = self.process(msg, kwargs)\n    self.logger.debug(msg, *args, **kwargs)\n\nThe process() method of LoggerAdapter is where the contextual information is added to the logging output. It’s passed the message and keyword arguments of the logging call, and it passes back (potentially) modified versions of these to use in the call to the underlying logger. The default implementation of this method leaves the message alone, but inserts an ‘extra’ key in the keyword argument whose value is the dict-like object passed to the constructor. Of course, if you had passed an ‘extra’ keyword argument in the call to the adapter, it will be silently overwritten.\nThe advantage of using ‘extra’ is that the values in the dict-like object are merged into the LogRecord instance’s __dict__, allowing you to use customized strings with your Formatter instances which know about the keys of the dict-like object. If you need a different method, e.g. if you want to prepend or append the contextual information to the message string, you just need to subclass LoggerAdapter and override process() to do what you need. Here is a simple example:\nclass CustomAdapter(logging.LoggerAdapter):\n    \"\"\"\n    This example adapter expects the passed in dict-like object to have a\n    'connid' key, whose value in brackets is prepended to the log message.\n    \"\"\"\n    def process(self, msg, kwargs):\n        return '[%s] %s' % (self.extra['connid'], msg), kwargs\n\nwhich you can use like this:\nlogger = logging.getLogger(__name__)\nadapter = CustomAdapter(logger, {'connid': some_conn_id})\n\nThen any events that you log to the adapter will have the value of some_conn_id prepended to the log messages.\n\n\nYou don’t need to pass an actual dict to a LoggerAdapter - you could pass an instance of a class which implements __getitem__ and __iter__ so that it looks like a dict to logging. This would be useful if you want to generate values dynamically (whereas the values in a dict would be constant).\n\n\n\n\nYou can also add contextual information to log output using a user-defined Filter. Filter instances are allowed to modify the LogRecords passed to them, including adding additional attributes which can then be output using a suitable format string, or if needed a custom Formatter.\nFor example in a web application, the request being processed (or at least, the interesting parts of it) can be stored in a threadlocal (threading.local) variable, and then accessed from a Filter to add, say, information from the request - say, the remote IP address and remote user’s username - to the LogRecord, using the attribute names ‘ip’ and ‘user’ as in the LoggerAdapter example above. In that case, the same format string can be used to get similar output to that shown above. Here’s an example script:\nimport logging\nfrom random import choice\n\nclass ContextFilter(logging.Filter):\n    \"\"\"\n    This is a filter which injects contextual information into the log.\n\n    Rather than use actual contextual information, we just use random\n    data in this demo.\n    \"\"\"\n\n    USERS = ['jim', 'fred', 'sheila']\n    IPS = ['123.231.231.123', '127.0.0.1', '192.168.0.1']\n\n    def filter(self, record):\n\n        record.ip = choice(ContextFilter.IPS)\n        record.user = choice(ContextFilter.USERS)\n        return True\n\nif __name__ == '__main__':\n    levels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL)\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)-15s %(name)-5s %(levelname)-8s IP: %(ip)-15s User: %(user)-8s %(message)s')\n    a1 = logging.getLogger('a.b.c')\n    a2 = logging.getLogger('d.e.f')\n\n    f = ContextFilter()\n    a1.addFilter(f)\n    a2.addFilter(f)\n    a1.debug('A debug message')\n    a1.info('An info message with %s', 'some parameters')\n    for x in range(10):\n        lvl = choice(levels)\n        lvlname = logging.getLevelName(lvl)\n        a2.log(lvl, 'A message at %s level with %d %s', lvlname, 2, 'parameters')\n\nwhich, when run, produces something like:\n2010-09-06 22:38:15,292 a.b.c DEBUG    IP: 123.231.231.123 User: fred     A debug message\n2010-09-06 22:38:15,300 a.b.c INFO     IP: 192.168.0.1     User: sheila   An info message with some parameters\n2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f ERROR    IP: 127.0.0.1       User: jim      A message at ERROR level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 127.0.0.1       User: sheila   A message at DEBUG level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f ERROR    IP: 123.231.231.123 User: fred     A message at ERROR level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 192.168.0.1     User: jim      A message at CRITICAL level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 192.168.0.1     User: jim      A message at DEBUG level with 2 parameters\n2010-09-06 22:38:15,301 d.e.f ERROR    IP: 127.0.0.1       User: sheila   A message at ERROR level with 2 parameters\n2010-09-06 22:38:15,301 d.e.f DEBUG    IP: 123.231.231.123 User: fred     A message at DEBUG level with 2 parameters\n2010-09-06 22:38:15,301 d.e.f INFO     IP: 123.231.231.123 User: fred     A message at INFO level with 2 parameters\n\n\n\n\n\nSince Python 3.7, the contextvars module has provided context-local storage which works for both threading and asyncio processing needs. This type of storage may thus be generally preferable to thread-locals. The following example shows how, in a multi-threaded environment, logs can populated with contextual information such as, for example, request attributes handled by web applications.\nFor the purposes of illustration, say that you have different web applications, each independent of the other but running in the same Python process and using a library common to them. How can each of these applications have their own log, where all logging messages from the library (and other request processing code) are directed to the appropriate application’s log file, while including in the log additional contextual information such as client IP, HTTP request method and client username?\nLet’s assume that the library can be simulated by the following code:\n# webapplib.py\nimport logging\nimport time\n\nlogger = logging.getLogger(__name__)\n\ndef useful():\n    # Just a representative event logged from the library\n    logger.debug('Hello from webapplib!')\n    # Just sleep for a bit so other threads get to run\n    time.sleep(0.01)\n\nWe can simulate the multiple web applications by means of two simple classes, Request and WebApp. These simulate how real threaded web applications work - each request is handled by a thread:\n# main.py\nimport argparse\nfrom contextvars import ContextVar\nimport logging\nimport os\nfrom random import choice\nimport threading\nimport webapplib\n\nlogger = logging.getLogger(__name__)\nroot = logging.getLogger()\nroot.setLevel(logging.DEBUG)\n\nclass Request:\n    \"\"\"\n    A simple dummy request class which just holds dummy HTTP request method,\n    client IP address and client username\n    \"\"\"\n    def __init__(self, method, ip, user):\n        self.method = method\n        self.ip = ip\n        self.user = user\n\n# A dummy set of requests which will be used in the simulation - we'll just pick\n# from this list randomly. Note that all GET requests are from 192.168.2.XXX\n# addresses, whereas POST requests are from 192.16.3.XXX addresses. Three users\n# are represented in the sample requests.\n\nREQUESTS = [\n    Request('GET', '192.168.2.20', 'jim'),\n    Request('POST', '192.168.3.20', 'fred'),\n    Request('GET', '192.168.2.21', 'sheila'),\n    Request('POST', '192.168.3.21', 'jim'),\n    Request('GET', '192.168.2.22', 'fred'),\n    Request('POST', '192.168.3.22', 'sheila'),\n]\n\n# Note that the format string includes references to request context information\n# such as HTTP method, client IP and username\n\nformatter = logging.Formatter('%(threadName)-11s %(appName)s %(name)-9s %(user)-6s %(ip)s %(method)-4s %(message)s')\n\n# Create our context variables. These will be filled at the start of request\n# processing, and used in the logging that happens during that processing\n\nctx_request = ContextVar('request')\nctx_appname = ContextVar('appname')\n\nclass InjectingFilter(logging.Filter):\n    \"\"\"\n    A filter which injects context-specific information into logs and ensures\n    that only information for a specific webapp is included in its log\n    \"\"\"\n    def __init__(self, app):\n        self.app = app\n\n    def filter(self, record):\n        request = ctx_request.get()\n        record.method = request.method\n        record.ip = request.ip\n        record.user = request.user\n        record.appName = appName = ctx_appname.get()\n        return appName == self.app.name\n\nclass WebApp:\n    \"\"\"\n    A dummy web application class which has its own handler and filter for a\n    webapp-specific log.\n    \"\"\"\n    def __init__(self, name):\n        self.name = name\n        handler = logging.FileHandler(name + '.log', 'w')\n        f = InjectingFilter(self)\n        handler.setFormatter(formatter)\n        handler.addFilter(f)\n        root.addHandler(handler)\n        self.num_requests = 0\n\n    def process_request(self, request):\n        \"\"\"\n        This is the dummy method for processing a request. It's called on a\n        different thread for every request. We store the context information into\n        the context vars before doing anything else.\n        \"\"\"\n        ctx_request.set(request)\n        ctx_appname.set(self.name)\n        self.num_requests += 1\n        logger.debug('Request processing started')\n        webapplib.useful()\n        logger.debug('Request processing finished')\n\ndef main():\n    fn = os.path.splitext(os.path.basename(__file__))[0]\n    adhf = argparse.ArgumentDefaultsHelpFormatter\n    ap = argparse.ArgumentParser(formatter_class=adhf, prog=fn,\n                                 description='Simulate a couple of web '\n                                             'applications handling some '\n                                             'requests, showing how request '\n                                             'context can be used to '\n                                             'populate logs')\n    aa = ap.add_argument\n    aa('--count', '-c', type=int, default=100, help='How many requests to simulate')\n    options = ap.parse_args()\n\n    # Create the dummy webapps and put them in a list which we can use to select\n    # from randomly\n    app1 = WebApp('app1')\n    app2 = WebApp('app2')\n    apps = [app1, app2]\n    threads = []\n    # Add a common handler which will capture all events\n    handler = logging.FileHandler('app.log', 'w')\n    handler.setFormatter(formatter)\n    root.addHandler(handler)\n\n    # Generate calls to process requests\n    for i in range(options.count):\n        try:\n            # Pick an app at random and a request for it to process\n            app = choice(apps)\n            request = choice(REQUESTS)\n            # Process the request in its own thread\n            t = threading.Thread(target=app.process_request, args=(request,))\n            threads.append(t)\n            t.start()\n        except KeyboardInterrupt:\n            break\n\n    # Wait for the threads to terminate\n    for t in threads:\n        t.join()\n\n    for app in apps:\n        print('%s processed %s requests' % (app.name, app.num_requests))\n\nif __name__ == '__main__':\n    main()\n\nIf you run the above, you should find that roughly half the requests go into app1.log and the rest into app2.log, and the all the requests are logged to app.log. Each webapp-specific log will contain only log entries for only that webapp, and the request information will be displayed consistently in the log (i.e. the information in each dummy request will always appear together in a log line). This is illustrated by the following shell output:\n~/logging-contextual-webapp$ python main.py\napp1 processed 51 requests\napp2 processed 49 requests\n~/logging-contextual-webapp$ wc -l *.log\n  153 app1.log\n  147 app2.log\n  300 app.log\n  600 total\n~/logging-contextual-webapp$ head -3 app1.log\nThread-3 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\nThread-3 (process_request) app1 webapplib jim    192.168.3.21 POST Hello from webapplib!\nThread-5 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\n~/logging-contextual-webapp$ head -3 app2.log\nThread-1 (process_request) app2 __main__  sheila 192.168.2.21 GET  Request processing started\nThread-1 (process_request) app2 webapplib sheila 192.168.2.21 GET  Hello from webapplib!\nThread-2 (process_request) app2 __main__  jim    192.168.2.20 GET  Request processing started\n~/logging-contextual-webapp$ head app.log\nThread-1 (process_request) app2 __main__  sheila 192.168.2.21 GET  Request processing started\nThread-1 (process_request) app2 webapplib sheila 192.168.2.21 GET  Hello from webapplib!\nThread-2 (process_request) app2 __main__  jim    192.168.2.20 GET  Request processing started\nThread-3 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\nThread-2 (process_request) app2 webapplib jim    192.168.2.20 GET  Hello from webapplib!\nThread-3 (process_request) app1 webapplib jim    192.168.3.21 POST Hello from webapplib!\nThread-4 (process_request) app2 __main__  fred   192.168.2.22 GET  Request processing started\nThread-5 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\nThread-4 (process_request) app2 webapplib fred   192.168.2.22 GET  Hello from webapplib!\nThread-6 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\n~/logging-contextual-webapp$ grep app1 app1.log | wc -l\n153\n~/logging-contextual-webapp$ grep app2 app2.log | wc -l\n147\n~/logging-contextual-webapp$ grep app1 app.log | wc -l\n153\n~/logging-contextual-webapp$ grep app2 app.log | wc -l\n147\n\n\n\n\nEach Handler has its own chain of filters. If you want to add contextual information to a LogRecord without leaking it to other handlers, you can use a filter that returns a new LogRecord instead of modifying it in-place, as shown in the following script:\nimport copy\nimport logging\n\ndef filter(record: logging.LogRecord):\n    record = copy.copy(record)\n    record.user = 'jim'\n    return record\n\nif __name__ == '__main__':\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter('%(message)s from %(user)-8s')\n    handler.setFormatter(formatter)\n    handler.addFilter(filter)\n    logger.addHandler(handler)\n\n    logger.info('A log message')\n\n\n\n\nAlthough logging is thread-safe, and logging to a single file from multiple threads in a single process is supported, logging to a single file from multiple processes is not supported, because there is no standard way to serialize access to a single file across multiple processes in Python. If you need to log to a single file from multiple processes, one way of doing this is to have all the processes log to a SocketHandler, and have a separate process which implements a socket server which reads from the socket and logs to file. (If you prefer, you can dedicate one thread in one of the existing processes to perform this function.) This section documents this approach in more detail and includes a working socket receiver which can be used as a starting point for you to adapt in your own applications.\nYou could also write your own handler which uses the Lock class from the multiprocessing module to serialize access to the file from your processes. The stdlib FileHandler and subclasses do not make use of multiprocessing.\nAlternatively, you can use a Queue and a QueueHandler to send all logging events to one of the processes in your multi-process application. The following example script demonstrates how you can do this; in the example a separate listener process listens for events sent by other processes and logs them according to its own logging configuration. Although the example only demonstrates one way of doing it (for example, you may want to use a listener thread rather than a separate listener process – the implementation would be analogous) it does allow for completely different logging configurations for the listener and the other processes in your application, and can be used as the basis for code meeting your own specific requirements:\n# You'll need these imports in your own code\nimport logging\nimport logging.handlers\nimport multiprocessing\n\n# Next two import lines for this demo only\nfrom random import choice, random\nimport time\n\n#\n# Because you'll want to define the logging configurations for listener and workers, the\n# listener and worker process functions take a configurer parameter which is a callable\n# for configuring logging for that process. These functions are also passed the queue,\n# which they use for communication.\n#\n# In practice, you can configure the listener however you want, but note that in this\n# simple example, the listener does not apply level or filter logic to received records.\n# In practice, you would probably want to do this logic in the worker processes, to avoid\n# sending events which would be filtered out between processes.\n#\n# The size of the rotated files is made small so you can see the results easily.\ndef listener_configurer():\n    root = logging.getLogger()\n    h = logging.handlers.RotatingFileHandler('mptest.log', 'a', 300, 10)\n    f = logging.Formatter('%(asctime)s %(processName)-10s %(name)s %(levelname)-8s %(message)s')\n    h.setFormatter(f)\n    root.addHandler(h)\n\n# This is the listener process top-level loop: wait for logging events\n# (LogRecords)on the queue and handle them, quit when you get a None for a\n# LogRecord.\ndef listener_process(queue, configurer):\n    configurer()\n    while True:\n        try:\n            record = queue.get()\n            if record is None:  # We send this as a sentinel to tell the listener to quit.\n                break\n            logger = logging.getLogger(record.name)\n            logger.handle(record)  # No level or filter logic applied - just do it!\n        except Exception:\n            import sys, traceback\n            print('Whoops! Problem:', file=sys.stderr)\n            traceback.print_exc(file=sys.stderr)\n\n# Arrays used for random selections in this demo\n\nLEVELS = [logging.DEBUG, logging.INFO, logging.WARNING,\n          logging.ERROR, logging.CRITICAL]\n\nLOGGERS = ['a.b.c', 'd.e.f']\n\nMESSAGES = [\n    'Random message #1',\n    'Random message #2',\n    'Random message #3',\n]\n\n# The worker configuration is done at the start of the worker process run.\n# Note that on Windows you can't rely on fork semantics, so each process\n# will run the logging configuration code when it starts.\ndef worker_configurer(queue):\n    h = logging.handlers.QueueHandler(queue)  # Just the one handler needed\n    root = logging.getLogger()\n    root.addHandler(h)\n    # send all messages, for demo; no other level or filter logic applied.\n    root.setLevel(logging.DEBUG)\n\n# This is the worker process top-level loop, which just logs ten events with\n# random intervening delays before terminating.\n# The print messages are just so you know it's doing something!\ndef worker_process(queue, configurer):\n    configurer(queue)\n    name = multiprocessing.current_process().name\n    print('Worker started: %s' % name)\n    for i in range(10):\n        time.sleep(random())\n        logger = logging.getLogger(choice(LOGGERS))\n        level = choice(LEVELS)\n        message = choice(MESSAGES)\n        logger.log(level, message)\n    print('Worker finished: %s' % name)\n\n# Here's where the demo gets orchestrated. Create the queue, create and start\n# the listener, create ten workers and start them, wait for them to finish,\n# then send a None to the queue to tell the listener to finish.\ndef main():\n    queue = multiprocessing.Queue(-1)\n    listener = multiprocessing.Process(target=listener_process,\n                                       args=(queue, listener_configurer))\n    listener.start()\n    workers = []\n    for i in range(10):\n        worker = multiprocessing.Process(target=worker_process,\n                                         args=(queue, worker_configurer))\n        workers.append(worker)\n        worker.start()\n    for w in workers:\n        w.join()\n    queue.put_nowait(None)\n    listener.join()\n\nif __name__ == '__main__':\n    main()\n\nA variant of the above script keeps the logging in the main process, in a separate thread:\nimport logging\nimport logging.config\nimport logging.handlers\nfrom multiprocessing import Process, Queue\nimport random\nimport threading\nimport time\n\ndef logger_thread(q):\n    while True:\n        record = q.get()\n        if record is None:\n            break\n        logger = logging.getLogger(record.name)\n        logger.handle(record)\n\n\ndef worker_process(q):\n    qh = logging.handlers.QueueHandler(q)\n    root = logging.getLogger()\n    root.setLevel(logging.DEBUG)\n    root.addHandler(qh)\n    levels = [logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n              logging.CRITICAL]\n    loggers = ['foo', 'foo.bar', 'foo.bar.baz',\n               'spam', 'spam.ham', 'spam.ham.eggs']\n    for i in range(100):\n        lvl = random.choice(levels)\n        logger = logging.getLogger(random.choice(loggers))\n        logger.log(lvl, 'Message no. %d', i)\n\nif __name__ == '__main__':\n    q = Queue()\n    d = {\n        'version': 1,\n        'formatters': {\n            'detailed': {\n                'class': 'logging.Formatter',\n                'format': '%(asctime)s %(name)-15s %(levelname)-8s %(processName)-10s %(message)s'\n            }\n        },\n        'handlers': {\n            'console': {\n                'class': 'logging.StreamHandler',\n                'level': 'INFO',\n            },\n            'file': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog.log',\n                'mode': 'w',\n                'formatter': 'detailed',\n            },\n            'foofile': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-foo.log',\n                'mode': 'w',\n                'formatter': 'detailed',\n            },\n            'errors': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-errors.log',\n                'mode': 'w',\n                'level': 'ERROR',\n                'formatter': 'detailed',\n            },\n        },\n        'loggers': {\n            'foo': {\n                'handlers': ['foofile']\n            }\n        },\n        'root': {\n            'level': 'DEBUG',\n            'handlers': ['console', 'file', 'errors']\n        },\n    }\n    workers = []\n    for i in range(5):\n        wp = Process(target=worker_process, name='worker %d' % (i + 1), args=(q,))\n        workers.append(wp)\n        wp.start()\n    logging.config.dictConfig(d)\n    lp = threading.Thread(target=logger_thread, args=(q,))\n    lp.start()\n    # At this point, the main process could do some useful work of its own\n    # Once it's done that, it can wait for the workers to terminate...\n    for wp in workers:\n        wp.join()\n    # And now tell the logging thread to finish up, too\n    q.put(None)\n    lp.join()\n\nThis variant shows how you can e.g. apply configuration for particular loggers - e.g. the foo logger has a special handler which stores all events in the foo subsystem in a file mplog-foo.log. This will be used by the logging machinery in the main process (even though the logging events are generated in the worker processes) to direct the messages to the appropriate destinations.\n\n\nIf you want to use concurrent.futures.ProcessPoolExecutor to start your worker processes, you need to create the queue slightly differently. Instead of\nqueue = multiprocessing.Queue(-1)\n\nyou should use\nqueue = multiprocessing.Manager().Queue(-1)  # also works with the examples above\n\nand you can then replace the worker creation from this:\nworkers = []\nfor i in range(10):\n    worker = multiprocessing.Process(target=worker_process,\n                                     args=(queue, worker_configurer))\n    workers.append(worker)\n    worker.start()\nfor w in workers:\n    w.join()\n\nto this (remembering to first import concurrent.futures):\nwith concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n    for i in range(10):\n        executor.submit(worker_process, queue, worker_configurer)\n\n\n\n\nWhen deploying Web applications using Gunicorn or uWSGI (or similar), multiple worker processes are created to handle client requests. In such environments, avoid creating file-based handlers directly in your web application. Instead, use a SocketHandler to log from the web application to a listener in a separate process. This can be set up using a process management tool such as Supervisor - see Running a logging socket listener in production for more details.\n\n\n\n\nSometimes you want to let a log file grow to a certain size, then open a new file and log to that. You may want to keep a certain number of these files, and when that many files have been created, rotate the files so that the number of files and the size of the files both remain bounded. For this usage pattern, the logging package provides a RotatingFileHandler:\nimport glob\nimport logging\nimport logging.handlers\n\nLOG_FILENAME = 'logging_rotatingfile_example.out'\n\n# Set up a specific logger with our desired output level\nmy_logger = logging.getLogger('MyLogger')\nmy_logger.setLevel(logging.DEBUG)\n\n# Add the log message handler to the logger\nhandler = logging.handlers.RotatingFileHandler(\n              LOG_FILENAME, maxBytes=20, backupCount=5)\n\nmy_logger.addHandler(handler)\n\n# Log some messages\nfor i in range(20):\n    my_logger.debug('i = %d' % i)\n\n# See what files are created\nlogfiles = glob.glob('%s*' % LOG_FILENAME)\n\nfor filename in logfiles:\n    print(filename)\n\nThe result should be 6 separate files, each with part of the log history for the application:\nlogging_rotatingfile_example.out\nlogging_rotatingfile_example.out.1\nlogging_rotatingfile_example.out.2\nlogging_rotatingfile_example.out.3\nlogging_rotatingfile_example.out.4\nlogging_rotatingfile_example.out.5\n\nThe most current file is always logging_rotatingfile_example.out, and each time it reaches the size limit it is renamed with the suffix .1. Each of the existing backup files is renamed to increment the suffix (.1 becomes .2, etc.) and the .6 file is erased.\nObviously this example sets the log length much too small as an extreme example. You would want to set maxBytes to an appropriate value.\n\n\n\nWhen logging was added to the Python standard library, the only way of formatting messages with variable content was to use the %-formatting method. Since then, Python has gained two new formatting approaches: string.Template (added in Python 2.4) and str.format() (added in Python 2.6).\nLogging (as of 3.2) provides improved support for these two additional formatting styles. The Formatter class been enhanced to take an additional, optional keyword parameter named style. This defaults to '%', but other possible values are '{' and '$', which correspond to the other two formatting styles. Backwards compatibility is maintained by default (as you would expect), but by explicitly specifying a style parameter, you get the ability to specify format strings which work with str.format() or string.Template. Here’s an example console session to show the possibilities:\n&gt;&gt;&gt;\n&gt;&gt;&gt; import logging\n&gt;&gt;&gt; root = logging.getLogger()\n&gt;&gt;&gt; root.setLevel(logging.DEBUG)\n&gt;&gt;&gt; handler = logging.StreamHandler()\n&gt;&gt;&gt; bf = logging.Formatter('{asctime} {name} {levelname:8s} {message}',\n...                        style='{')\n&gt;&gt;&gt; handler.setFormatter(bf)\n&gt;&gt;&gt; root.addHandler(handler)\n&gt;&gt;&gt; logger = logging.getLogger('foo.bar')\n&gt;&gt;&gt; logger.debug('This is a DEBUG message')\n2010-10-28 15:11:55,341 foo.bar DEBUG    This is a DEBUG message\n&gt;&gt;&gt; logger.critical('This is a CRITICAL message')\n2010-10-28 15:12:11,526 foo.bar CRITICAL This is a CRITICAL message\n&gt;&gt;&gt; df = logging.Formatter('$asctime $name ${levelname} $message',\n...                        style='$')\n&gt;&gt;&gt; handler.setFormatter(df)\n&gt;&gt;&gt; logger.debug('This is a DEBUG message')\n2010-10-28 15:13:06,924 foo.bar DEBUG This is a DEBUG message\n&gt;&gt;&gt; logger.critical('This is a CRITICAL message')\n2010-10-28 15:13:11,494 foo.bar CRITICAL This is a CRITICAL message\n&gt;&gt;&gt;\n\nNote that the formatting of logging messages for final output to logs is completely independent of how an individual logging message is constructed. That can still use %-formatting, as shown here:\n&gt;&gt;&gt;\n&gt;&gt;&gt; logger.error('This is an%s %s %s', 'other,', 'ERROR,', 'message')\n2010-10-28 15:19:29,833 foo.bar ERROR This is another, ERROR, message\n&gt;&gt;&gt;\n\nLogging calls (logger.debug(), logger.info() etc.) only take positional parameters for the actual logging message itself, with keyword parameters used only for determining options for how to handle the actual logging call (e.g. the exc_info keyword parameter to indicate that traceback information should be logged, or the extra keyword parameter to indicate additional contextual information to be added to the log). So you cannot directly make logging calls using str.format() or string.Template syntax, because internally the logging package uses %-formatting to merge the format string and the variable arguments. There would be no changing this while preserving backward compatibility, since all logging calls which are out there in existing code will be using %-format strings.\nThere is, however, a way that you can use {}- and $- formatting to construct your individual log messages. Recall that for a message you can use an arbitrary object as a message format string, and that the logging package will call str() on that object to get the actual format string. Consider the following two classes:\nclass BraceMessage:\n    def __init__(self, fmt, /, *args, **kwargs):\n        self.fmt = fmt\n        self.args = args\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return self.fmt.format(*self.args, **self.kwargs)\n\nclass DollarMessage:\n    def __init__(self, fmt, /, **kwargs):\n        self.fmt = fmt\n        self.kwargs = kwargs\n\n    def __str__(self):\n        from string import Template\n        return Template(self.fmt).substitute(**self.kwargs)\n\nEither of these can be used in place of a format string, to allow {}- or \\(-formatting to be used to build the actual “message” part which appears in the formatted log output in place of “%(message)s” or “{message}” or “\\)message”. It’s a little unwieldy to use the class names whenever you want to log something, but it’s quite palatable if you use an alias such as __ (double underscore — not to be confused with _, the single underscore used as a synonym/alias for gettext.gettext() or its brethren).\nThe above classes are not included in Python, though they’re easy enough to copy and paste into your own code. They can be used as follows (assuming that they’re declared in a module called wherever):\n&gt;&gt;&gt;\n&gt;&gt;&gt; from wherever import BraceMessage as __\n&gt;&gt;&gt; print(__('Message with {0} {name}', 2, name='placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt; class Point: pass\n...\n&gt;&gt;&gt; p = Point()\n&gt;&gt;&gt; p.x = 0.5\n&gt;&gt;&gt; p.y = 0.5\n&gt;&gt;&gt; print(__('Message with coordinates: ({point.x:.2f}, {point.y:.2f})',\n...       point=p))\nMessage with coordinates: (0.50, 0.50)\n&gt;&gt;&gt; from wherever import DollarMessage as __\n&gt;&gt;&gt; print(__('Message with $num $what', num=2, what='placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt;\n\nWhile the above examples use print() to show how the formatting works, you would of course use logger.debug() or similar to actually log using this approach.\nOne thing to note is that you pay no significant performance penalty with this approach: the actual formatting happens not when you make the logging call, but when (and if) the logged message is actually about to be output to a log by a handler. So the only slightly unusual thing which might trip you up is that the parentheses go around the format string and the arguments, not just the format string. That’s because the __ notation is just syntax sugar for a constructor call to one of the _XXX_Message classes.\nIf you prefer, you can use a LoggerAdapter to achieve a similar effect to the above, as in the following example:\nimport logging\n\nclass Message:\n    def __init__(self, fmt, args):\n        self.fmt = fmt\n        self.args = args\n\n    def __str__(self):\n        return self.fmt.format(*self.args)\n\nclass StyleAdapter(logging.LoggerAdapter):\n    def log(self, level, msg, /, *args, stacklevel=1, **kwargs):\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, Message(msg, args), **kwargs,\n                            stacklevel=stacklevel+1)\n\nlogger = StyleAdapter(logging.getLogger(__name__))\n\ndef main():\n    logger.debug('Hello, {}', 'world!')\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.DEBUG)\n    main()\n\nThe above script should log the message Hello, world! when run with Python 3.8 or later.\n\n\n\nEvery logging event is represented by a LogRecord instance. When an event is logged and not filtered out by a logger’s level, a LogRecord is created, populated with information about the event and then passed to the handlers for that logger (and its ancestors, up to and including the logger where further propagation up the hierarchy is disabled). Before Python 3.2, there were only two places where this creation was done:\n\nLogger.makeRecord(), which is called in the normal process of logging an event. This invoked LogRecord directly to create an instance.\nmakeLogRecord(), which is called with a dictionary containing attributes to be added to the LogRecord. This is typically invoked when a suitable dictionary has been received over the network (e.g. in pickle form via a SocketHandler, or in JSON form via an HTTPHandler).\n\nThis has usually meant that if you need to do anything special with a LogRecord, you’ve had to do one of the following.\n\nCreate your own Logger subclass, which overrides Logger.makeRecord(), and set it using setLoggerClass() before any loggers that you care about are instantiated.\nAdd a Filter to a logger or handler, which does the necessary special manipulation you need when its filter() method is called.\n\nThe first approach would be a little unwieldy in the scenario where (say) several different libraries wanted to do different things. Each would attempt to set its own Logger subclass, and the one which did this last would win.\nThe second approach works reasonably well for many cases, but does not allow you to e.g. use a specialized subclass of LogRecord. Library developers can set a suitable filter on their loggers, but they would have to remember to do this every time they introduced a new logger (which they would do simply by adding new packages or modules and doing\nlogger = logging.getLogger(__name__)\n\nat module level). It’s probably one too many things to think about. Developers could also add the filter to a NullHandler attached to their top-level logger, but this would not be invoked if an application developer attached a handler to a lower-level library logger — so output from that handler would not reflect the intentions of the library developer.\nIn Python 3.2 and later, LogRecord creation is done through a factory, which you can specify. The factory is just a callable you can set with setLogRecordFactory(), and interrogate with getLogRecordFactory(). The factory is invoked with the same signature as the LogRecord constructor, as LogRecord is the default setting for the factory.\nThis approach allows a custom factory to control all aspects of LogRecord creation. For example, you could return a subclass, or just add some additional attributes to the record once created, using a pattern similar to this:\nold_factory = logging.getLogRecordFactory()\n\ndef record_factory(*args, **kwargs):\n    record = old_factory(*args, **kwargs)\n    record.custom_attribute = 0xdecafbad\n    return record\n\nlogging.setLogRecordFactory(record_factory)\n\nThis pattern allows different libraries to chain factories together, and as long as they don’t overwrite each other’s attributes or unintentionally overwrite the attributes provided as standard, there should be no surprises. However, it should be borne in mind that each link in the chain adds run-time overhead to all logging operations, and the technique should only be used when the use of a Filter does not provide the desired result.\n\n\n\n\n\nYou can use a QueueHandler subclass to send messages to other kinds of queues, for example a ZeroMQ ‘publish’ socket. In the example below,the socket is created separately and passed to the handler (as its ‘queue’):\nimport zmq   # using pyzmq, the Python binding for ZeroMQ\nimport json  # for serializing records portably\n\nctx = zmq.Context()\nsock = zmq.Socket(ctx, zmq.PUB)  # or zmq.PUSH, or other suitable value\nsock.bind('tcp://*:5556')        # or wherever\n\nclass ZeroMQSocketHandler(QueueHandler):\n    def enqueue(self, record):\n        self.queue.send_json(record.__dict__)\n\n\nhandler = ZeroMQSocketHandler(sock)\n\nOf course there are other ways of organizing this, for example passing in the data needed by the handler to create the socket:\nclass ZeroMQSocketHandler(QueueHandler):\n    def __init__(self, uri, socktype=zmq.PUB, ctx=None):\n        self.ctx = ctx or zmq.Context()\n        socket = zmq.Socket(self.ctx, socktype)\n        socket.bind(uri)\n        super().__init__(socket)\n\n    def enqueue(self, record):\n        self.queue.send_json(record.__dict__)\n\n    def close(self):\n        self.queue.close()\n\n\n\n\nYou can also subclass QueueListener to get messages from other kinds of queues, for example a ZeroMQ ‘subscribe’ socket. Here’s an example:\nclass ZeroMQSocketListener(QueueListener):\n    def __init__(self, uri, /, *handlers, **kwargs):\n        self.ctx = kwargs.get('ctx') or zmq.Context()\n        socket = zmq.Socket(self.ctx, zmq.SUB)\n        socket.setsockopt_string(zmq.SUBSCRIBE, '')  # subscribe to everything\n        socket.connect(uri)\n        super().__init__(socket, *handlers, **kwargs)\n\n    def dequeue(self):\n        msg = self.queue.recv_json()\n        return logging.makeLogRecord(msg)\n\n\n\n\n\nIn a similar way to the above section, we can implement a listener and handler using pynng, which is a Python binding to NNG, billed as a spiritual successor to ZeroMQ. The following snippets illustrate – you can test them in an environment which has pynng installed. Just for variety, we present the listener first.\n\n\n# listener.py\nimport json\nimport logging\nimport logging.handlers\n\nimport pynng\n\nDEFAULT_ADDR = \"tcp://localhost:13232\"\n\ninterrupted = False\n\nclass NNGSocketListener(logging.handlers.QueueListener):\n\n    def __init__(self, uri, /, *handlers, **kwargs):\n        # Have a timeout for interruptability, and open a\n        # subscriber socket\n        socket = pynng.Sub0(listen=uri, recv_timeout=500)\n        # The b'' subscription matches all topics\n        topics = kwargs.pop('topics', None) or b''\n        socket.subscribe(topics)\n        # We treat the socket as a queue\n        super().__init__(socket, *handlers, **kwargs)\n\n    def dequeue(self, block):\n        data = None\n        # Keep looping while not interrupted and no data received over the\n        # socket\n        while not interrupted:\n            try:\n                data = self.queue.recv(block=block)\n                break\n            except pynng.Timeout:\n                pass\n            except pynng.Closed:  # sometimes happens when you hit Ctrl-C\n                break\n        if data is None:\n            return None\n        # Get the logging event sent from a publisher\n        event = json.loads(data.decode('utf-8'))\n        return logging.makeLogRecord(event)\n\n    def enqueue_sentinel(self):\n        # Not used in this implementation, as the socket isn't really a\n        # queue\n        pass\n\nlogging.getLogger('pynng').propagate = False\nlistener = NNGSocketListener(DEFAULT_ADDR, logging.StreamHandler(), topics=b'')\nlistener.start()\nprint('Press Ctrl-C to stop.')\ntry:\n    while True:\n        pass\nexcept KeyboardInterrupt:\n    interrupted = True\nfinally:\n    listener.stop()\n\n\n\n\n# sender.py\nimport json\nimport logging\nimport logging.handlers\nimport time\nimport random\n\nimport pynng\n\nDEFAULT_ADDR = \"tcp://localhost:13232\"\n\nclass NNGSocketHandler(logging.handlers.QueueHandler):\n\n    def __init__(self, uri):\n        socket = pynng.Pub0(dial=uri, send_timeout=500)\n        super().__init__(socket)\n\n    def enqueue(self, record):\n        # Send the record as UTF-8 encoded JSON\n        d = dict(record.__dict__)\n        data = json.dumps(d)\n        self.queue.send(data.encode('utf-8'))\n\n    def close(self):\n        self.queue.close()\n\nlogging.getLogger('pynng').propagate = False\nhandler = NNGSocketHandler(DEFAULT_ADDR)\n# Make sure the process ID is in the output\nlogging.basicConfig(level=logging.DEBUG,\n                    handlers=[logging.StreamHandler(), handler],\n                    format='%(levelname)-8s %(name)10s %(process)6s %(message)s')\nlevels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n          logging.CRITICAL)\nlogger_names = ('myapp', 'myapp.lib1', 'myapp.lib2')\nmsgno = 1\nwhile True:\n    # Just randomly select some loggers and levels and log away\n    level = random.choice(levels)\n    logger = logging.getLogger(random.choice(logger_names))\n    logger.log(level, 'Message no. %5d' % msgno)\n    msgno += 1\n    delay = random.random() * 2 + 0.5\n    time.sleep(delay)\n\nYou can run the above two snippets in separate command shells. If we run the listener in one shell and run the sender in two separate shells, we should see something like the following. In the first sender shell:\n$ python sender.py\nDEBUG         myapp    613 Message no.     1\nWARNING  myapp.lib2    613 Message no.     2\nCRITICAL myapp.lib2    613 Message no.     3\nWARNING  myapp.lib2    613 Message no.     4\nCRITICAL myapp.lib1    613 Message no.     5\nDEBUG         myapp    613 Message no.     6\nCRITICAL myapp.lib1    613 Message no.     7\nINFO     myapp.lib1    613 Message no.     8\n(and so on)\n\nIn the second sender shell:\n$ python sender.py\nINFO     myapp.lib2    657 Message no.     1\nCRITICAL myapp.lib2    657 Message no.     2\nCRITICAL      myapp    657 Message no.     3\nCRITICAL myapp.lib1    657 Message no.     4\nINFO     myapp.lib1    657 Message no.     5\nWARNING  myapp.lib2    657 Message no.     6\nCRITICAL      myapp    657 Message no.     7\nDEBUG    myapp.lib1    657 Message no.     8\n(and so on)\n\nIn the listener shell:\n$ python listener.py\nPress Ctrl-C to stop.\nDEBUG         myapp    613 Message no.     1\nWARNING  myapp.lib2    613 Message no.     2\nINFO     myapp.lib2    657 Message no.     1\nCRITICAL myapp.lib2    613 Message no.     3\nCRITICAL myapp.lib2    657 Message no.     2\nCRITICAL      myapp    657 Message no.     3\nWARNING  myapp.lib2    613 Message no.     4\nCRITICAL myapp.lib1    613 Message no.     5\nCRITICAL myapp.lib1    657 Message no.     4\nINFO     myapp.lib1    657 Message no.     5\nDEBUG         myapp    613 Message no.     6\nWARNING  myapp.lib2    657 Message no.     6\nCRITICAL      myapp    657 Message no.     7\nCRITICAL myapp.lib1    613 Message no.     7\nINFO     myapp.lib1    613 Message no.     8\nDEBUG    myapp.lib1    657 Message no.     8\n(and so on)\n\nAs you can see, the logging from the two sender processes is interleaved in the listener’s output.\n\n\n\n\nBelow is an example of a logging configuration dictionary - it’s taken from the documentation on the Django project. This dictionary is passed to dictConfig() to put the configuration into effect:\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'verbose': {\n            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',\n            'style': '{',\n        },\n        'simple': {\n            'format': '{levelname} {message}',\n            'style': '{',\n        },\n    },\n    'filters': {\n        'special': {\n            '()': 'project.logging.SpecialFilter',\n            'foo': 'bar',\n        },\n    },\n    'handlers': {\n        'console': {\n            'level': 'INFO',\n            'class': 'logging.StreamHandler',\n            'formatter': 'simple',\n        },\n        'mail_admins': {\n            'level': 'ERROR',\n            'class': 'django.utils.log.AdminEmailHandler',\n            'filters': ['special']\n        }\n    },\n    'loggers': {\n        'django': {\n            'handlers': ['console'],\n            'propagate': True,\n        },\n        'django.request': {\n            'handlers': ['mail_admins'],\n            'level': 'ERROR',\n            'propagate': False,\n        },\n        'myproject.custom': {\n            'handlers': ['console', 'mail_admins'],\n            'level': 'INFO',\n            'filters': ['special']\n        }\n    }\n}\n\nFor more information about this configuration, you can see the relevant section of the Django documentation.\n\n\n\nAn example of how you can define a namer and rotator is given in the following runnable script, which shows gzip compression of the log file:\nimport gzip\nimport logging\nimport logging.handlers\nimport os\nimport shutil\n\ndef namer(name):\n    return name + \".gz\"\n\ndef rotator(source, dest):\n    with open(source, 'rb') as f_in:\n        with gzip.open(dest, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    os.remove(source)\n\n\nrh = logging.handlers.RotatingFileHandler('rotated.log', maxBytes=128, backupCount=5)\nrh.rotator = rotator\nrh.namer = namer\n\nroot = logging.getLogger()\nroot.setLevel(logging.INFO)\nroot.addHandler(rh)\nf = logging.Formatter('%(asctime)s %(message)s')\nrh.setFormatter(f)\nfor i in range(1000):\n    root.info(f'Message no. {i + 1}')\n\nAfter running this, you will see six new files, five of which are compressed:\n$ ls rotated.log*\nrotated.log       rotated.log.2.gz  rotated.log.4.gz\nrotated.log.1.gz  rotated.log.3.gz  rotated.log.5.gz\n$ zcat rotated.log.1.gz\n2023-01-20 02:28:17,767 Message no. 996\n2023-01-20 02:28:17,767 Message no. 997\n2023-01-20 02:28:17,767 Message no. 998\n\n\n\n\nThe following working example shows how logging can be used with multiprocessing using configuration files. The configurations are fairly simple, but serve to illustrate how more complex ones could be implemented in a real multiprocessing scenario.\nIn the example, the main process spawns a listener process and some worker processes. Each of the main process, the listener and the workers have three separate configurations (the workers all share the same configuration). We can see logging in the main process, how the workers log to a QueueHandler and how the listener implements a QueueListener and a more complex logging configuration, and arranges to dispatch events received via the queue to the handlers specified in the configuration. Note that these configurations are purely illustrative, but you should be able to adapt this example to your own scenario.\nHere’s the script - the docstrings and the comments hopefully explain how it works:\nimport logging\nimport logging.config\nimport logging.handlers\nfrom multiprocessing import Process, Queue, Event, current_process\nimport os\nimport random\nimport time\n\nclass MyHandler:\n    \"\"\"\n    A simple handler for logging events. It runs in the listener process and\n    dispatches events to loggers based on the name in the received record,\n    which then get dispatched, by the logging system, to the handlers\n    configured for those loggers.\n    \"\"\"\n\n    def handle(self, record):\n        if record.name == \"root\":\n            logger = logging.getLogger()\n        else:\n            logger = logging.getLogger(record.name)\n\n        if logger.isEnabledFor(record.levelno):\n            # The process name is transformed just to show that it's the listener\n            # doing the logging to files and console\n            record.processName = '%s (for %s)' % (current_process().name, record.processName)\n            logger.handle(record)\n\ndef listener_process(q, stop_event, config):\n    \"\"\"\n    This could be done in the main process, but is just done in a separate\n    process for illustrative purposes.\n\n    This initialises logging according to the specified configuration,\n    starts the listener and waits for the main process to signal completion\n    via the event. The listener is then stopped, and the process exits.\n    \"\"\"\n    logging.config.dictConfig(config)\n    listener = logging.handlers.QueueListener(q, MyHandler())\n    listener.start()\n    if os.name == 'posix':\n        # On POSIX, the setup logger will have been configured in the\n        # parent process, but should have been disabled following the\n        # dictConfig call.\n        # On Windows, since fork isn't used, the setup logger won't\n        # exist in the child, so it would be created and the message\n        # would appear - hence the \"if posix\" clause.\n        logger = logging.getLogger('setup')\n        logger.critical('Should not appear, because of disabled logger ...')\n    stop_event.wait()\n    listener.stop()\n\ndef worker_process(config):\n    \"\"\"\n    A number of these are spawned for the purpose of illustration. In\n    practice, they could be a heterogeneous bunch of processes rather than\n    ones which are identical to each other.\n\n    This initialises logging according to the specified configuration,\n    and logs a hundred messages with random levels to randomly selected\n    loggers.\n\n    A small sleep is added to allow other processes a chance to run. This\n    is not strictly needed, but it mixes the output from the different\n    processes a bit more than if it's left out.\n    \"\"\"\n    logging.config.dictConfig(config)\n    levels = [logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n              logging.CRITICAL]\n    loggers = ['foo', 'foo.bar', 'foo.bar.baz',\n               'spam', 'spam.ham', 'spam.ham.eggs']\n    if os.name == 'posix':\n        # On POSIX, the setup logger will have been configured in the\n        # parent process, but should have been disabled following the\n        # dictConfig call.\n        # On Windows, since fork isn't used, the setup logger won't\n        # exist in the child, so it would be created and the message\n        # would appear - hence the \"if posix\" clause.\n        logger = logging.getLogger('setup')\n        logger.critical('Should not appear, because of disabled logger ...')\n    for i in range(100):\n        lvl = random.choice(levels)\n        logger = logging.getLogger(random.choice(loggers))\n        logger.log(lvl, 'Message no. %d', i)\n        time.sleep(0.01)\n\ndef main():\n    q = Queue()\n    # The main process gets a simple configuration which prints to the console.\n    config_initial = {\n        'version': 1,\n        'handlers': {\n            'console': {\n                'class': 'logging.StreamHandler',\n                'level': 'INFO'\n            }\n        },\n        'root': {\n            'handlers': ['console'],\n            'level': 'DEBUG'\n        }\n    }\n    # The worker process configuration is just a QueueHandler attached to the\n    # root logger, which allows all messages to be sent to the queue.\n    # We disable existing loggers to disable the \"setup\" logger used in the\n    # parent process. This is needed on POSIX because the logger will\n    # be there in the child following a fork().\n    config_worker = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'handlers': {\n            'queue': {\n                'class': 'logging.handlers.QueueHandler',\n                'queue': q\n            }\n        },\n        'root': {\n            'handlers': ['queue'],\n            'level': 'DEBUG'\n        }\n    }\n    # The listener process configuration shows that the full flexibility of\n    # logging configuration is available to dispatch events to handlers however\n    # you want.\n    # We disable existing loggers to disable the \"setup\" logger used in the\n    # parent process. This is needed on POSIX because the logger will\n    # be there in the child following a fork().\n    config_listener = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'detailed': {\n                'class': 'logging.Formatter',\n                'format': '%(asctime)s %(name)-15s %(levelname)-8s %(processName)-10s %(message)s'\n            },\n            'simple': {\n                'class': 'logging.Formatter',\n                'format': '%(name)-15s %(levelname)-8s %(processName)-10s %(message)s'\n            }\n        },\n        'handlers': {\n            'console': {\n                'class': 'logging.StreamHandler',\n                'formatter': 'simple',\n                'level': 'INFO'\n            },\n            'file': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog.log',\n                'mode': 'w',\n                'formatter': 'detailed'\n            },\n            'foofile': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-foo.log',\n                'mode': 'w',\n                'formatter': 'detailed'\n            },\n            'errors': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-errors.log',\n                'mode': 'w',\n                'formatter': 'detailed',\n                'level': 'ERROR'\n            }\n        },\n        'loggers': {\n            'foo': {\n                'handlers': ['foofile']\n            }\n        },\n        'root': {\n            'handlers': ['console', 'file', 'errors'],\n            'level': 'DEBUG'\n        }\n    }\n    # Log some initial events, just to show that logging in the parent works\n    # normally.\n    logging.config.dictConfig(config_initial)\n    logger = logging.getLogger('setup')\n    logger.info('About to create workers ...')\n    workers = []\n    for i in range(5):\n        wp = Process(target=worker_process, name='worker %d' % (i + 1),\n                     args=(config_worker,))\n        workers.append(wp)\n        wp.start()\n        logger.info('Started worker: %s', wp.name)\n    logger.info('About to create listener ...')\n    stop_event = Event()\n    lp = Process(target=listener_process, name='listener',\n                 args=(q, stop_event, config_listener))\n    lp.start()\n    logger.info('Started listener')\n    # We now hang around for the workers to finish their work.\n    for wp in workers:\n        wp.join()\n    # Workers all done, listening can now stop.\n    # Logging in the parent still works normally.\n    logger.info('Telling listener to stop ...')\n    stop_event.set()\n    lp.join()\n    logger.info('All done.')\n\nif __name__ == '__main__':\n    main()\n\n\n\n\nRFC 5424 requires that a Unicode message be sent to a syslog daemon as a set of bytes which have the following structure: an optional pure-ASCII component, followed by a UTF-8 Byte Order Mark (BOM), followed by Unicode encoded using UTF-8. (See the relevant section of the specification.)\nIn Python 3.1, code was added to SysLogHandler to insert a BOM into the message, but unfortunately, it was implemented incorrectly, with the BOM appearing at the beginning of the message and hence not allowing any pure-ASCII component to appear before it.\nAs this behaviour is broken, the incorrect BOM insertion code is being removed from Python 3.2.4 and later. However, it is not being replaced, and if you want to produce RFC 5424-compliant messages which include a BOM, an optional pure-ASCII sequence before it and arbitrary Unicode after it, encoded using UTF-8, then you need to do the following:\n\nAttach a Formatter instance to your SysLogHandler instance, with a format string such as:\n'ASCII section\\ufeffUnicode section'\n\nThe Unicode code point U+FEFF, when encoded using UTF-8, will be encoded as a UTF-8 BOM – the byte-string b'\\xef\\xbb\\xbf'.\nReplace the ASCII section with whatever placeholders you like, but make sure that the data that appears in there after substitution is always ASCII (that way, it will remain unchanged after UTF-8 encoding).\nReplace the Unicode section with whatever placeholders you like; if the data which appears there after substitution contains characters outside the ASCII range, that’s fine – it will be encoded using UTF-8.\n\nThe formatted message will be encoded using UTF-8 encoding by SysLogHandler. If you follow the above rules, you should be able to produce RFC 5424-compliant messages. If you don’t, logging may not complain, but your messages will not be RFC 5424-compliant, and your syslog daemon may complain.\n\n\n\nAlthough most logging messages are intended for reading by humans, and thus not readily machine-parseable, there might be circumstances where you want to output messages in a structured format which is capable of being parsed by a program (without needing complex regular expressions to parse the log message). This is straightforward to achieve using the logging package. There are a number of ways in which this could be achieved, but the following is a simple approach which uses JSON to serialise the event in a machine-parseable manner:\nimport json\nimport logging\n\nclass StructuredMessage:\n    def __init__(self, message, /, **kwargs):\n        self.message = message\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return '%s &gt;&gt;&gt; %s' % (self.message, json.dumps(self.kwargs))\n\n_ = StructuredMessage   # optional, to improve readability\n\nlogging.basicConfig(level=logging.INFO, format='%(message)s')\nlogging.info(_('message 1', foo='bar', bar='baz', num=123, fnum=123.456))\n\nIf the above script is run, it prints:\nmessage 1 &gt;&gt;&gt; {\"fnum\": 123.456, \"num\": 123, \"bar\": \"baz\", \"foo\": \"bar\"}\n\nNote that the order of items might be different according to the version of Python used.\nIf you need more specialised processing, you can use a custom JSON encoder, as in the following complete example:\nimport json\nimport logging\n\n\nclass Encoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, set):\n            return tuple(o)\n        elif isinstance(o, str):\n            return o.encode('unicode_escape').decode('ascii')\n        return super().default(o)\n\nclass StructuredMessage:\n    def __init__(self, message, /, **kwargs):\n        self.message = message\n        self.kwargs = kwargs\n\n    def __str__(self):\n        s = Encoder().encode(self.kwargs)\n        return '%s &gt;&gt;&gt; %s' % (self.message, s)\n\n_ = StructuredMessage   # optional, to improve readability\n\ndef main():\n    logging.basicConfig(level=logging.INFO, format='%(message)s')\n    logging.info(_('message 1', set_value={1, 2, 3}, snowman='\\u2603'))\n\nif __name__ == '__main__':\n    main()\n\nWhen the above script is run, it prints:\nmessage 1 &gt;&gt;&gt; {\"snowman\": \"\\u2603\", \"set_value\": [1, 2, 3]}\n\nNote that the order of items might be different according to the version of Python used.\n\n\n\nThere are times when you want to customize logging handlers in particular ways, and if you use dictConfig() you may be able to do this without subclassing. As an example, consider that you may want to set the ownership of a log file. On POSIX, this is easily done using shutil.chown(), but the file handlers in the stdlib don’t offer built-in support. You can customize handler creation using a plain function such as:\ndef owned_file_handler(filename, mode='a', encoding=None, owner=None):\n    if owner:\n        if not os.path.exists(filename):\n            open(filename, 'a').close()\n        shutil.chown(filename, *owner)\n    return logging.FileHandler(filename, mode, encoding)\n\nYou can then specify, in a logging configuration passed to dictConfig(), that a logging handler be created by calling this function:\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'default': {\n            'format': '%(asctime)s %(levelname)s %(name)s %(message)s'\n        },\n    },\n    'handlers': {\n        'file':{\n            # The values below are popped from this dictionary and\n            # used to create the handler, set the handler's level and\n            # its formatter.\n            '()': owned_file_handler,\n            'level':'DEBUG',\n            'formatter': 'default',\n            # The values below are passed to the handler creator callable\n            # as keyword arguments.\n            'owner': ['pulse', 'pulse'],\n            'filename': 'chowntest.log',\n            'mode': 'w',\n            'encoding': 'utf-8',\n        },\n    },\n    'root': {\n        'handlers': ['file'],\n        'level': 'DEBUG',\n    },\n}\n\nIn this example I am setting the ownership using the pulse user and group, just for the purposes of illustration. Putting it together into a working script, chowntest.py:\nimport logging, logging.config, os, shutil\n\ndef owned_file_handler(filename, mode='a', encoding=None, owner=None):\n    if owner:\n        if not os.path.exists(filename):\n            open(filename, 'a').close()\n        shutil.chown(filename, *owner)\n    return logging.FileHandler(filename, mode, encoding)\n\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'default': {\n            'format': '%(asctime)s %(levelname)s %(name)s %(message)s'\n        },\n    },\n    'handlers': {\n        'file':{\n            # The values below are popped from this dictionary and\n            # used to create the handler, set the handler's level and\n            # its formatter.\n            '()': owned_file_handler,\n            'level':'DEBUG',\n            'formatter': 'default',\n            # The values below are passed to the handler creator callable\n            # as keyword arguments.\n            'owner': ['pulse', 'pulse'],\n            'filename': 'chowntest.log',\n            'mode': 'w',\n            'encoding': 'utf-8',\n        },\n    },\n    'root': {\n        'handlers': ['file'],\n        'level': 'DEBUG',\n    },\n}\n\nlogging.config.dictConfig(LOGGING)\nlogger = logging.getLogger('mylogger')\nlogger.debug('A debug message')\n\nTo run this, you will probably need to run as root:\n$ sudo python3.3 chowntest.py\n$ cat chowntest.log\n2013-11-05 09:34:51,128 DEBUG mylogger A debug message\n$ ls -l chowntest.log\n-rw-r--r-- 1 pulse pulse 55 2013-11-05 09:34 chowntest.log\n\nNote that this example uses Python 3.3 because that’s where shutil.chown() makes an appearance. This approach should work with any Python version that supports dictConfig() - namely, Python 2.7, 3.2 or later. With pre-3.3 versions, you would need to implement the actual ownership change using e.g. os.chown().\nIn practice, the handler-creating function may be in a utility module somewhere in your project. Instead of the line in the configuration:\n'()': owned_file_handler,\n\nyou could use e.g.:\n'()': 'ext://project.util.owned_file_handler',\n\nwhere project.util can be replaced with the actual name of the package where the function resides. In the above working script, using 'ext://__main__.owned_file_handler' should work. Here, the actual callable is resolved by dictConfig() from the ext:// specification.\nThis example hopefully also points the way to how you could implement other types of file change - e.g. setting specific POSIX permission bits - in the same way, using os.chmod().\nOf course, the approach could also be extended to types of handler other than a FileHandler - for example, one of the rotating file handlers, or a different type of handler altogether.\n\n\n\nIn Python 3.2, the Formatter gained a style keyword parameter which, while defaulting to % for backward compatibility, allowed the specification of { or $ to support the formatting approaches supported by str.format() and string.Template. Note that this governs the formatting of logging messages for final output to logs, and is completely orthogonal to how an individual logging message is constructed.\nLogging calls (debug(), info() etc.) only take positional parameters for the actual logging message itself, with keyword parameters used only for determining options for how to handle the logging call (e.g. the exc_info keyword parameter to indicate that traceback information should be logged, or the extra keyword parameter to indicate additional contextual information to be added to the log). So you cannot directly make logging calls using str.format() or string.Template syntax, because internally the logging package uses %-formatting to merge the format string and the variable arguments. There would be no changing this while preserving backward compatibility, since all logging calls which are out there in existing code will be using %-format strings.\nThere have been suggestions to associate format styles with specific loggers, but that approach also runs into backward compatibility problems because any existing code could be using a given logger name and using %-formatting.\nFor logging to work interoperably between any third-party libraries and your code, decisions about formatting need to be made at the level of the individual logging call. This opens up a couple of ways in which alternative formatting styles can be accommodated.\n\n\nIn Python 3.2, along with the Formatter changes mentioned above, the logging package gained the ability to allow users to set their own LogRecord subclasses, using the setLogRecordFactory() function. You can use this to set your own subclass of LogRecord, which does the Right Thing by overriding the getMessage() method. The base class implementation of this method is where the msg % args formatting happens, and where you can substitute your alternate formatting; however, you should be careful to support all formatting styles and allow %-formatting as the default, to ensure interoperability with other code. Care should also be taken to call str(self.msg), just as the base implementation does.\nRefer to the reference documentation on setLogRecordFactory() and LogRecord for more information.\n\n\n\nThere is another, perhaps simpler way that you can use {}- and $- formatting to construct your individual log messages. You may recall (from Using arbitrary objects as messages) that when logging you can use an arbitrary object as a message format string, and that the logging package will call str() on that object to get the actual format string. Consider the following two classes:\nclass BraceMessage:\n    def __init__(self, fmt, /, *args, **kwargs):\n        self.fmt = fmt\n        self.args = args\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return self.fmt.format(*self.args, **self.kwargs)\n\nclass DollarMessage:\n    def __init__(self, fmt, /, **kwargs):\n        self.fmt = fmt\n        self.kwargs = kwargs\n\n    def __str__(self):\n        from string import Template\n        return Template(self.fmt).substitute(**self.kwargs)\n\nEither of these can be used in place of a format string, to allow {}- or \\(-formatting to be used to build the actual “message” part which appears in the formatted log output in place of “%(message)s” or “{message}” or “\\)message”. If you find it a little unwieldy to use the class names whenever you want to log something, you can make it more palatable if you use an alias such as M or _ for the message (or perhaps __, if you are using _ for localization).\nExamples of this approach are given below. Firstly, formatting with str.format():\n&gt;&gt;&gt;\n&gt;&gt;&gt; __ = BraceMessage\n&gt;&gt;&gt; print(__('Message with {0} {1}', 2, 'placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt; class Point: pass\n...\n&gt;&gt;&gt; p = Point()\n&gt;&gt;&gt; p.x = 0.5\n&gt;&gt;&gt; p.y = 0.5\n&gt;&gt;&gt; print(__('Message with coordinates: ({point.x:.2f}, {point.y:.2f})', point=p))\nMessage with coordinates: (0.50, 0.50)\n\nSecondly, formatting with string.Template:\n&gt;&gt;&gt;\n&gt;&gt;&gt; __ = DollarMessage\n&gt;&gt;&gt; print(__('Message with $num $what', num=2, what='placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt;\n\nOne thing to note is that you pay no significant performance penalty with this approach: the actual formatting happens not when you make the logging call, but when (and if) the logged message is actually about to be output to a log by a handler. So the only slightly unusual thing which might trip you up is that the parentheses go around the format string and the arguments, not just the format string. That’s because the __ notation is just syntax sugar for a constructor call to one of the _XXX_Message classes shown above.\n\n\n\n\nYou can configure filters using dictConfig(), though it might not be obvious at first glance how to do it (hence this recipe). Since Filter is the only filter class included in the standard library, and it is unlikely to cater to many requirements (it’s only there as a base class), you will typically need to define your own Filter subclass with an overridden filter() method. To do this, specify the () key in the configuration dictionary for the filter, specifying a callable which will be used to create the filter (a class is the most obvious, but you can provide any callable which returns a Filter instance). Here is a complete example:\nimport logging\nimport logging.config\nimport sys\n\nclass MyFilter(logging.Filter):\n    def __init__(self, param=None):\n        self.param = param\n\n    def filter(self, record):\n        if self.param is None:\n            allow = True\n        else:\n            allow = self.param not in record.msg\n        if allow:\n            record.msg = 'changed: ' + record.msg\n        return allow\n\nLOGGING = {\n    'version': 1,\n    'filters': {\n        'myfilter': {\n            '()': MyFilter,\n            'param': 'noshow',\n        }\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'filters': ['myfilter']\n        }\n    },\n    'root': {\n        'level': 'DEBUG',\n        'handlers': ['console']\n    },\n}\n\nif __name__ == '__main__':\n    logging.config.dictConfig(LOGGING)\n    logging.debug('hello')\n    logging.debug('hello - noshow')\n\nThis example shows how you can pass configuration data to the callable which constructs the instance, in the form of keyword parameters. When run, the above script will print:\nwhich shows that the filter is working as configured.\nA couple of extra points to note:\n\nIf you can’t refer to the callable directly in the configuration (e.g. if it lives in a different module, and you can’t import it directly where the configuration dictionary is), you can use the form ext://... as described in Access to external objects. For example, you could have used the text 'ext://__main__.MyFilter' instead of MyFilter in the above example.\nAs well as for filters, this technique can also be used to configure custom handlers and formatters. See User-defined objects for more information on how logging supports using user-defined objects in its configuration, and see the other cookbook recipe Customizing handlers with dictConfig() above.\n\n\n\n\nThere might be times when you want to do customized exception formatting - for argument’s sake, let’s say you want exactly one line per logged event, even when exception information is present. You can do this with a custom formatter class, as shown in the following example:\nimport logging\n\nclass OneLineExceptionFormatter(logging.Formatter):\n    def formatException(self, exc_info):\n        \"\"\"\n        Format an exception so that it prints on a single line.\n        \"\"\"\n        result = super().formatException(exc_info)\n        return repr(result)  # or format into one line however you want to\n\n    def format(self, record):\n        s = super().format(record)\n        if record.exc_text:\n            s = s.replace('\\n', '') + '|'\n        return s\n\ndef configure_logging():\n    fh = logging.FileHandler('output.txt', 'w')\n    f = OneLineExceptionFormatter('%(asctime)s|%(levelname)s|%(message)s|',\n                                  '%d/%m/%Y %H:%M:%S')\n    fh.setFormatter(f)\n    root = logging.getLogger()\n    root.setLevel(logging.DEBUG)\n    root.addHandler(fh)\n\ndef main():\n    configure_logging()\n    logging.info('Sample message')\n    try:\n        x = 1 / 0\n    except ZeroDivisionError as e:\n        logging.exception('ZeroDivisionError: %s', e)\n\nif __name__ == '__main__':\n    main()\n\nWhen run, this produces a file with exactly two lines:\n28/01/2015 07:21:23|INFO|Sample message|\n28/01/2015 07:21:23|ERROR|ZeroDivisionError: integer division or modulo by zero|'Traceback (most recent call last):\\n  File \"logtest7.py\", line 30, in main\\n    x = 1 / 0\\nZeroDivisionError: integer division or modulo by zero'|\n\nWhile the above treatment is simplistic, it points the way to how exception information can be formatted to your liking. The traceback module may be helpful for more specialized needs.\n\n\n\nThere might be situations when it is desirable to have logging messages rendered in an audible rather than a visible format. This is easy to do if you have text-to-speech (TTS) functionality available in your system, even if it doesn’t have a Python binding. Most TTS systems have a command line program you can run, and this can be invoked from a handler using subprocess. It’s assumed here that TTS command line programs won’t expect to interact with users or take a long time to complete, and that the frequency of logged messages will be not so high as to swamp the user with messages, and that it’s acceptable to have the messages spoken one at a time rather than concurrently, The example implementation below waits for one message to be spoken before the next is processed, and this might cause other handlers to be kept waiting. Here is a short example showing the approach, which assumes that the espeak TTS package is available:\nimport logging\nimport subprocess\nimport sys\n\nclass TTSHandler(logging.Handler):\n    def emit(self, record):\n        msg = self.format(record)\n        # Speak slowly in a female English voice\n        cmd = ['espeak', '-s150', '-ven+f3', msg]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                             stderr=subprocess.STDOUT)\n        # wait for the program to finish\n        p.communicate()\n\ndef configure_logging():\n    h = TTSHandler()\n    root = logging.getLogger()\n    root.addHandler(h)\n    # the default formatter just returns the message\n    root.setLevel(logging.DEBUG)\n\ndef main():\n    logging.info('Hello')\n    logging.debug('Goodbye')\n\nif __name__ == '__main__':\n    configure_logging()\n    sys.exit(main())\n\nWhen run, this script should say “Hello” and then “Goodbye” in a female voice.\nThe above approach can, of course, be adapted to other TTS systems and even other systems altogether which can process messages via external programs run from a command line.\n\n\n\nThere might be situations where you want to log messages in a temporary area and only output them if a certain condition occurs. For example, you may want to start logging debug events in a function, and if the function completes without errors, you don’t want to clutter the log with the collected debug information, but if there is an error, you want all the debug information to be output as well as the error.\nHere is an example which shows how you could do this using a decorator for your functions where you want logging to behave this way. It makes use of the logging.handlers.MemoryHandler, which allows buffering of logged events until some condition occurs, at which point the buffered events are flushed - passed to another handler (the target handler) for processing. By default, the MemoryHandler flushed when its buffer gets filled up or an event whose level is greater than or equal to a specified threshold is seen. You can use this recipe with a more specialised subclass of MemoryHandler if you want custom flushing behavior.\nThe example script has a simple function, foo, which just cycles through all the logging levels, writing to sys.stderr to say what level it’s about to log at, and then actually logging a message at that level. You can pass a parameter to foo which, if true, will log at ERROR and CRITICAL levels - otherwise, it only logs at DEBUG, INFO and WARNING levels.\nThe script just arranges to decorate foo with a decorator which will do the conditional logging that’s required. The decorator takes a logger as a parameter and attaches a memory handler for the duration of the call to the decorated function. The decorator can be additionally parameterised using a target handler, a level at which flushing should occur, and a capacity for the buffer (number of records buffered). These default to a StreamHandler which writes to sys.stderr, logging.ERROR and 100 respectively.\nHere’s the script:\nimport logging\nfrom logging.handlers import MemoryHandler\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\ndef log_if_errors(logger, target_handler=None, flush_level=None, capacity=None):\n    if target_handler is None:\n        target_handler = logging.StreamHandler()\n    if flush_level is None:\n        flush_level = logging.ERROR\n    if capacity is None:\n        capacity = 100\n    handler = MemoryHandler(capacity, flushLevel=flush_level, target=target_handler)\n\n    def decorator(fn):\n        def wrapper(*args, **kwargs):\n            logger.addHandler(handler)\n            try:\n                return fn(*args, **kwargs)\n            except Exception:\n                logger.exception('call failed')\n                raise\n            finally:\n                super(MemoryHandler, handler).flush()\n                logger.removeHandler(handler)\n        return wrapper\n\n    return decorator\n\ndef write_line(s):\n    sys.stderr.write('%s\\n' % s)\n\ndef foo(fail=False):\n    write_line('about to log at DEBUG ...')\n    logger.debug('Actually logged at DEBUG')\n    write_line('about to log at INFO ...')\n    logger.info('Actually logged at INFO')\n    write_line('about to log at WARNING ...')\n    logger.warning('Actually logged at WARNING')\n    if fail:\n        write_line('about to log at ERROR ...')\n        logger.error('Actually logged at ERROR')\n        write_line('about to log at CRITICAL ...')\n        logger.critical('Actually logged at CRITICAL')\n    return fail\n\ndecorated_foo = log_if_errors(logger)(foo)\n\nif __name__ == '__main__':\n    logger.setLevel(logging.DEBUG)\n    write_line('Calling undecorated foo with False')\n    assert not foo(False)\n    write_line('Calling undecorated foo with True')\n    assert foo(True)\n    write_line('Calling decorated foo with False')\n    assert not decorated_foo(False)\n    write_line('Calling decorated foo with True')\n    assert decorated_foo(True)\n\nWhen this script is run, the following output should be observed:\nCalling undecorated foo with False\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nCalling undecorated foo with True\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nabout to log at ERROR ...\nabout to log at CRITICAL ...\nCalling decorated foo with False\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nCalling decorated foo with True\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nabout to log at ERROR ...\nActually logged at DEBUG\nActually logged at INFO\nActually logged at WARNING\nActually logged at ERROR\nabout to log at CRITICAL ...\nActually logged at CRITICAL\n\nAs you can see, actual logging output only occurs when an event is logged whose severity is ERROR or greater, but in that case, any previous events at lower severities are also logged.\nYou can of course use the conventional means of decoration:\n@log_if_errors(logger)\ndef foo(fail=False):\n    ...\n\n\n\n\nTo illustrate how you can send log messages via email, so that a set number of messages are sent per email, you can subclass BufferingHandler. In the following example, which you can adapt to suit your specific needs, a simple test harness is provided which allows you to run the script with command line arguments specifying what you typically need to send things via SMTP. (Run the downloaded script with the -h argument to see the required and optional arguments.)\nimport logging\nimport logging.handlers\nimport smtplib\n\nclass BufferingSMTPHandler(logging.handlers.BufferingHandler):\n    def __init__(self, mailhost, port, username, password, fromaddr, toaddrs,\n                 subject, capacity):\n        logging.handlers.BufferingHandler.__init__(self, capacity)\n        self.mailhost = mailhost\n        self.mailport = port\n        self.username = username\n        self.password = password\n        self.fromaddr = fromaddr\n        if isinstance(toaddrs, str):\n            toaddrs = [toaddrs]\n        self.toaddrs = toaddrs\n        self.subject = subject\n        self.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)-5s %(message)s\"))\n\n    def flush(self):\n        if len(self.buffer) &gt; 0:\n            try:\n                smtp = smtplib.SMTP(self.mailhost, self.mailport)\n                smtp.starttls()\n                smtp.login(self.username, self.password)\n                msg = \"From: %s\\r\\nTo: %s\\r\\nSubject: %s\\r\\n\\r\\n\" % (self.fromaddr, ','.join(self.toaddrs), self.subject)\n                for record in self.buffer:\n                    s = self.format(record)\n                    msg = msg + s + \"\\r\\n\"\n                smtp.sendmail(self.fromaddr, self.toaddrs, msg)\n                smtp.quit()\n            except Exception:\n                if logging.raiseExceptions:\n                    raise\n            self.buffer = []\n\nif __name__ == '__main__':\n    import argparse\n\n    ap = argparse.ArgumentParser()\n    aa = ap.add_argument\n    aa('host', metavar='HOST', help='SMTP server')\n    aa('--port', '-p', type=int, default=587, help='SMTP port')\n    aa('user', metavar='USER', help='SMTP username')\n    aa('password', metavar='PASSWORD', help='SMTP password')\n    aa('to', metavar='TO', help='Addressee for emails')\n    aa('sender', metavar='SENDER', help='Sender email address')\n    aa('--subject', '-s',\n       default='Test Logging email from Python logging module (buffering)',\n       help='Subject of email')\n    options = ap.parse_args()\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    h = BufferingSMTPHandler(options.host, options.port, options.user,\n                             options.password, options.sender,\n                             options.to, options.subject, 10)\n    logger.addHandler(h)\n    for i in range(102):\n        logger.info(\"Info index = %d\", i)\n    h.flush()\n    h.close()\n\nIf you run this script and your SMTP server is correctly set up, you should find that it sends eleven emails to the addressee you specify. The first ten emails will each have ten log messages, and the eleventh will have two messages. That makes up 102 messages as specified in the script.\n\n\n\nSometimes you want to format times using UTC, which can be done using a class such as UTCFormatter, shown below:\nimport logging\nimport time\n\nclass UTCFormatter(logging.Formatter):\n    converter = time.gmtime\n\nand you can then use the UTCFormatter in your code instead of Formatter. If you want to do that via configuration, you can use the dictConfig() API with an approach illustrated by the following complete example:\nimport logging\nimport logging.config\nimport time\n\nclass UTCFormatter(logging.Formatter):\n    converter = time.gmtime\n\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'utc': {\n            '()': UTCFormatter,\n            'format': '%(asctime)s %(message)s',\n        },\n        'local': {\n            'format': '%(asctime)s %(message)s',\n        }\n    },\n    'handlers': {\n        'console1': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'utc',\n        },\n        'console2': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'local',\n        },\n    },\n    'root': {\n        'handlers': ['console1', 'console2'],\n   }\n}\n\nif __name__ == '__main__':\n    logging.config.dictConfig(LOGGING)\n    logging.warning('The local time is %s', time.asctime())\n\nWhen this script is run, it should print something like:\n2015-10-17 12:53:29,501 The local time is Sat Oct 17 13:53:29 2015\n2015-10-17 13:53:29,501 The local time is Sat Oct 17 13:53:29 2015\n\nshowing how the time is formatted both as local time and UTC, one for each handler.\n\n\n\nThere are times when it would be useful to temporarily change the logging configuration and revert it back after doing something. For this, a context manager is the most obvious way of saving and restoring the logging context. Here is a simple example of such a context manager, which allows you to optionally change the logging level and add a logging handler purely in the scope of the context manager:\nimport logging\nimport sys\n\nclass LoggingContext:\n    def __init__(self, logger, level=None, handler=None, close=True):\n        self.logger = logger\n        self.level = level\n        self.handler = handler\n        self.close = close\n\n    def __enter__(self):\n        if self.level is not None:\n            self.old_level = self.logger.level\n            self.logger.setLevel(self.level)\n        if self.handler:\n            self.logger.addHandler(self.handler)\n\n    def __exit__(self, et, ev, tb):\n        if self.level is not None:\n            self.logger.setLevel(self.old_level)\n        if self.handler:\n            self.logger.removeHandler(self.handler)\n        if self.handler and self.close:\n            self.handler.close()\n        # implicit return of None =&gt; don't swallow exceptions\n\nIf you specify a level value, the logger’s level is set to that value in the scope of the with block covered by the context manager. If you specify a handler, it is added to the logger on entry to the block and removed on exit from the block. You can also ask the manager to close the handler for you on block exit - you could do this if you don’t need the handler any more.\nTo illustrate how it works, we can add the following block of code to the above:\nif __name__ == '__main__':\n    logger = logging.getLogger('foo')\n    logger.addHandler(logging.StreamHandler())\n    logger.setLevel(logging.INFO)\n    logger.info('1. This should appear just once on stderr.')\n    logger.debug('2. This should not appear.')\n    with LoggingContext(logger, level=logging.DEBUG):\n        logger.debug('3. This should appear once on stderr.')\n    logger.debug('4. This should not appear.')\n    h = logging.StreamHandler(sys.stdout)\n    with LoggingContext(logger, level=logging.DEBUG, handler=h, close=True):\n        logger.debug('5. This should appear twice - once on stderr and once on stdout.')\n    logger.info('6. This should appear just once on stderr.')\n    logger.debug('7. This should not appear.')\n\nWe initially set the logger’s level to INFO, so message #1 appears and message #2 doesn’t. We then change the level to DEBUG temporarily in the following with block, and so message #3 appears. After the block exits, the logger’s level is restored to INFO and so message #4 doesn’t appear. In the next with block, we set the level to DEBUG again but also add a handler writing to sys.stdout. Thus, message #5 appears twice on the console (once via stderr and once via stdout). After the with statement’s completion, the status is as it was before so message #6 appears (like message #1) whereas message #7 doesn’t (just like message #2).\nIf we run the resulting script, the result is as follows:\n$ python logctx.py\n1. This should appear just once on stderr.\n3. This should appear once on stderr.\n5. This should appear twice - once on stderr and once on stdout.\n5. This should appear twice - once on stderr and once on stdout.\n6. This should appear just once on stderr.\n\nIf we run it again, but pipe stderr to /dev/null, we see the following, which is the only message written to stdout:\n$ python logctx.py 2&gt;/dev/null\n5. This should appear twice - once on stderr and once on stdout.\n\nOnce again, but piping stdout to /dev/null, we get:\n$ python logctx.py &gt;/dev/null\n1. This should appear just once on stderr.\n3. This should appear once on stderr.\n5. This should appear twice - once on stderr and once on stdout.\n6. This should appear just once on stderr.\n\nIn this case, the message #5 printed to stdout doesn’t appear, as expected.\nOf course, the approach described here can be generalised, for example to attach logging filters temporarily. Note that the above code works in Python 2 as well as Python 3.\n\n\n\nHere’s an example which shows how you can:\n\nUse a logging level based on command-line arguments\nDispatch to multiple subcommands in separate files, all logging at the same level in a consistent way\nMake use of simple, minimal configuration\n\nSuppose we have a command-line application whose job is to stop, start or restart some services. This could be organised for the purposes of illustration as a file app.py that is the main script for the application, with individual commands implemented in start.py, stop.py and restart.py. Suppose further that we want to control the verbosity of the application via a command-line argument, defaulting to logging.INFO. Here’s one way that app.py could be written:\nimport argparse\nimport importlib\nimport logging\nimport os\nimport sys\n\ndef main(args=None):\n    scriptname = os.path.basename(__file__)\n    parser = argparse.ArgumentParser(scriptname)\n    levels = ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')\n    parser.add_argument('--log-level', default='INFO', choices=levels)\n    subparsers = parser.add_subparsers(dest='command',\n                                       help='Available commands:')\n    start_cmd = subparsers.add_parser('start', help='Start a service')\n    start_cmd.add_argument('name', metavar='NAME',\n                           help='Name of service to start')\n    stop_cmd = subparsers.add_parser('stop',\n                                     help='Stop one or more services')\n    stop_cmd.add_argument('names', metavar='NAME', nargs='+',\n                          help='Name of service to stop')\n    restart_cmd = subparsers.add_parser('restart',\n                                        help='Restart one or more services')\n    restart_cmd.add_argument('names', metavar='NAME', nargs='+',\n                             help='Name of service to restart')\n    options = parser.parse_args()\n    # the code to dispatch commands could all be in this file. For the purposes\n    # of illustration only, we implement each command in a separate module.\n    try:\n        mod = importlib.import_module(options.command)\n        cmd = getattr(mod, 'command')\n    except (ImportError, AttributeError):\n        print('Unable to find the code for command \\'%s\\'' % options.command)\n        return 1\n    # Could get fancy here and load configuration from file or dictionary\n    logging.basicConfig(level=options.log_level,\n                        format='%(levelname)s %(name)s %(message)s')\n    cmd(options)\n\nif __name__ == '__main__':\n    sys.exit(main())\n\nAnd the start, stop and restart commands can be implemented in separate modules, like so for starting:\n# start.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef command(options):\n    logger.debug('About to start %s', options.name)\n    # actually do the command processing here ...\n    logger.info('Started the \\'%s\\' service.', options.name)\n\nand thus for stopping:\n# stop.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef command(options):\n    n = len(options.names)\n    if n == 1:\n        plural = ''\n        services = '\\'%s\\'' % options.names[0]\n    else:\n        plural = 's'\n        services = ', '.join('\\'%s\\'' % name for name in options.names)\n        i = services.rfind(', ')\n        services = services[:i] + ' and ' + services[i + 2:]\n    logger.debug('About to stop %s', services)\n    # actually do the command processing here ...\n    logger.info('Stopped the %s service%s.', services, plural)\n\nand similarly for restarting:\n# restart.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef command(options):\n    n = len(options.names)\n    if n == 1:\n        plural = ''\n        services = '\\'%s\\'' % options.names[0]\n    else:\n        plural = 's'\n        services = ', '.join('\\'%s\\'' % name for name in options.names)\n        i = services.rfind(', ')\n        services = services[:i] + ' and ' + services[i + 2:]\n    logger.debug('About to restart %s', services)\n    # actually do the command processing here ...\n    logger.info('Restarted the %s service%s.', services, plural)\n\nIf we run this application with the default log level, we get output like this:\n$ python app.py start foo\nINFO start Started the 'foo' service.\n\n$ python app.py stop foo bar\nINFO stop Stopped the 'foo' and 'bar' services.\n\n$ python app.py restart foo bar baz\nINFO restart Restarted the 'foo', 'bar' and 'baz' services.\n\nThe first word is the logging level, and the second word is the module or package name of the place where the event was logged.\nIf we change the logging level, then we can change the information sent to the log. For example, if we want more information:\n$ python app.py --log-level DEBUG start foo\nDEBUG start About to start foo\nINFO start Started the 'foo' service.\n\n$ python app.py --log-level DEBUG stop foo bar\nDEBUG stop About to stop 'foo' and 'bar'\nINFO stop Stopped the 'foo' and 'bar' services.\n\n$ python app.py --log-level DEBUG restart foo bar baz\nDEBUG restart About to restart 'foo', 'bar' and 'baz'\nINFO restart Restarted the 'foo', 'bar' and 'baz' services.\n\nAnd if we want less:\n$ python app.py --log-level WARNING start foo\n$ python app.py --log-level WARNING stop foo bar\n$ python app.py --log-level WARNING restart foo bar baz\n\nIn this case, the commands don’t print anything to the console, since nothing at WARNING level or above is logged by them.\n\n\n\nA question that comes up from time to time is about how to log to a GUI application. The Qt framework is a popular cross-platform UI framework with Python bindings using PySide2 or PyQt5 libraries.\nThe following example shows how to log to a Qt GUI. This introduces a simple QtHandler class which takes a callable, which should be a slot in the main thread that does GUI updates. A worker thread is also created to show how you can log to the GUI from both the UI itself (via a button for manual logging) as well as a worker thread doing work in the background (here, just logging messages at random levels with random short delays in between).\nThe worker thread is implemented using Qt’s QThread class rather than the threading module, as there are circumstances where one has to use QThread, which offers better integration with other Qt components.\nThe code should work with recent releases of any of PySide6, PyQt6, PySide2 or PyQt5. You should be able to adapt the approach to earlier versions of Qt. Please refer to the comments in the code snippet for more detailed information.\nimport datetime\nimport logging\nimport random\nimport sys\nimport time\n\n# Deal with minor differences between different Qt packages\ntry:\n    from PySide6 import QtCore, QtGui, QtWidgets\n    Signal = QtCore.Signal\n    Slot = QtCore.Slot\nexcept ImportError:\n    try:\n        from PyQt6 import QtCore, QtGui, QtWidgets\n        Signal = QtCore.pyqtSignal\n        Slot = QtCore.pyqtSlot\n    except ImportError:\n        try:\n            from PySide2 import QtCore, QtGui, QtWidgets\n            Signal = QtCore.Signal\n            Slot = QtCore.Slot\n        except ImportError:\n            from PyQt5 import QtCore, QtGui, QtWidgets\n            Signal = QtCore.pyqtSignal\n            Slot = QtCore.pyqtSlot\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Signals need to be contained in a QObject or subclass in order to be correctly\n# initialized.\n#\nclass Signaller(QtCore.QObject):\n    signal = Signal(str, logging.LogRecord)\n\n#\n# Output to a Qt GUI is only supposed to happen on the main thread. So, this\n# handler is designed to take a slot function which is set up to run in the main\n# thread. In this example, the function takes a string argument which is a\n# formatted log message, and the log record which generated it. The formatted\n# string is just a convenience - you could format a string for output any way\n# you like in the slot function itself.\n#\n# You specify the slot function to do whatever GUI updates you want. The handler\n# doesn't know or care about specific UI elements.\n#\nclass QtHandler(logging.Handler):\n    def __init__(self, slotfunc, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.signaller = Signaller()\n        self.signaller.signal.connect(slotfunc)\n\n    def emit(self, record):\n        s = self.format(record)\n        self.signaller.signal.emit(s, record)\n\n#\n# This example uses QThreads, which means that the threads at the Python level\n# are named something like \"Dummy-1\". The function below gets the Qt name of the\n# current thread.\n#\ndef ctname():\n    return QtCore.QThread.currentThread().objectName()\n\n\n#\n# Used to generate random levels for logging.\n#\nLEVELS = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n          logging.CRITICAL)\n\n#\n# This worker class represents work that is done in a thread separate to the\n# main thread. The way the thread is kicked off to do work is via a button press\n# that connects to a slot in the worker.\n#\n# Because the default threadName value in the LogRecord isn't much use, we add\n# a qThreadName which contains the QThread name as computed above, and pass that\n# value in an \"extra\" dictionary which is used to update the LogRecord with the\n# QThread name.\n#\n# This example worker just outputs messages sequentially, interspersed with\n# random delays of the order of a few seconds.\n#\nclass Worker(QtCore.QObject):\n    @Slot()\n    def start(self):\n        extra = {'qThreadName': ctname() }\n        logger.debug('Started work', extra=extra)\n        i = 1\n        # Let the thread run until interrupted. This allows reasonably clean\n        # thread termination.\n        while not QtCore.QThread.currentThread().isInterruptionRequested():\n            delay = 0.5 + random.random() * 2\n            time.sleep(delay)\n            try:\n                if random.random() &lt; 0.1:\n                    raise ValueError('Exception raised: %d' % i)\n                else:\n                    level = random.choice(LEVELS)\n                    logger.log(level, 'Message after delay of %3.1f: %d', delay, i, extra=extra)\n            except ValueError as e:\n                logger.exception('Failed: %s', e, extra=extra)\n            i += 1\n\n#\n# Implement a simple UI for this cookbook example. This contains:\n#\n# * A read-only text edit window which holds formatted log messages\n# * A button to start work and log stuff in a separate thread\n# * A button to log something from the main thread\n# * A button to clear the log window\n#\nclass Window(QtWidgets.QWidget):\n\n    COLORS = {\n        logging.DEBUG: 'black',\n        logging.INFO: 'blue',\n        logging.WARNING: 'orange',\n        logging.ERROR: 'red',\n        logging.CRITICAL: 'purple',\n    }\n\n    def __init__(self, app):\n        super().__init__()\n        self.app = app\n        self.textedit = te = QtWidgets.QPlainTextEdit(self)\n        # Set whatever the default monospace font is for the platform\n        f = QtGui.QFont('nosuchfont')\n        if hasattr(f, 'Monospace'):\n            f.setStyleHint(f.Monospace)\n        else:\n            f.setStyleHint(f.StyleHint.Monospace)  # for Qt6\n        te.setFont(f)\n        te.setReadOnly(True)\n        PB = QtWidgets.QPushButton\n        self.work_button = PB('Start background work', self)\n        self.log_button = PB('Log a message at a random level', self)\n        self.clear_button = PB('Clear log window', self)\n        self.handler = h = QtHandler(self.update_status)\n        # Remember to use qThreadName rather than threadName in the format string.\n        fs = '%(asctime)s %(qThreadName)-12s %(levelname)-8s %(message)s'\n        formatter = logging.Formatter(fs)\n        h.setFormatter(formatter)\n        logger.addHandler(h)\n        # Set up to terminate the QThread when we exit\n        app.aboutToQuit.connect(self.force_quit)\n\n        # Lay out all the widgets\n        layout = QtWidgets.QVBoxLayout(self)\n        layout.addWidget(te)\n        layout.addWidget(self.work_button)\n        layout.addWidget(self.log_button)\n        layout.addWidget(self.clear_button)\n        self.setFixedSize(900, 400)\n\n        # Connect the non-worker slots and signals\n        self.log_button.clicked.connect(self.manual_update)\n        self.clear_button.clicked.connect(self.clear_display)\n\n        # Start a new worker thread and connect the slots for the worker\n        self.start_thread()\n        self.work_button.clicked.connect(self.worker.start)\n        # Once started, the button should be disabled\n        self.work_button.clicked.connect(lambda : self.work_button.setEnabled(False))\n\n    def start_thread(self):\n        self.worker = Worker()\n        self.worker_thread = QtCore.QThread()\n        self.worker.setObjectName('Worker')\n        self.worker_thread.setObjectName('WorkerThread')  # for qThreadName\n        self.worker.moveToThread(self.worker_thread)\n        # This will start an event loop in the worker thread\n        self.worker_thread.start()\n\n    def kill_thread(self):\n        # Just tell the worker to stop, then tell it to quit and wait for that\n        # to happen\n        self.worker_thread.requestInterruption()\n        if self.worker_thread.isRunning():\n            self.worker_thread.quit()\n            self.worker_thread.wait()\n        else:\n            print('worker has already exited.')\n\n    def force_quit(self):\n        # For use when the window is closed\n        if self.worker_thread.isRunning():\n            self.kill_thread()\n\n    # The functions below update the UI and run in the main thread because\n    # that's where the slots are set up\n\n    @Slot(str, logging.LogRecord)\n    def update_status(self, status, record):\n        color = self.COLORS.get(record.levelno, 'black')\n        s = '&lt;pre&gt;&lt;font color=\"%s\"&gt;%s&lt;/font&gt;&lt;/pre&gt;' % (color, status)\n        self.textedit.appendHtml(s)\n\n    @Slot()\n    def manual_update(self):\n        # This function uses the formatted message passed in, but also uses\n        # information from the record to format the message in an appropriate\n        # color according to its severity (level).\n        level = random.choice(LEVELS)\n        extra = {'qThreadName': ctname() }\n        logger.log(level, 'Manually logged!', extra=extra)\n\n    @Slot()\n    def clear_display(self):\n        self.textedit.clear()\n\n\ndef main():\n    QtCore.QThread.currentThread().setObjectName('MainThread')\n    logging.getLogger().setLevel(logging.DEBUG)\n    app = QtWidgets.QApplication(sys.argv)\n    example = Window(app)\n    example.show()\n    if hasattr(app, 'exec'):\n        rc = app.exec()\n    else:\n        rc = app.exec_()\n    sys.exit(rc)\n\nif __name__=='__main__':\n    main()\n\n\n\n\nAlthough RFC 5424 dates from 2009, most syslog servers are configured by default to use the older RFC 3164, which hails from 2001. When logging was added to Python in 2003, it supported the earlier (and only existing) protocol at the time. Since RFC5424 came out, as there has not been widespread deployment of it in syslog servers, the SysLogHandler functionality has not been updated.\nRFC 5424 contains some useful features such as support for structured data, and if you need to be able to log to a syslog server with support for it, you can do so with a subclassed handler which looks something like this:\nimport datetime\nimport logging.handlers\nimport re\nimport socket\nimport time\n\nclass SysLogHandler5424(logging.handlers.SysLogHandler):\n\n    tz_offset = re.compile(r'([+-]\\d{2})(\\d{2})$')\n    escaped = re.compile(r'([\\]\"\\\\])')\n\n    def __init__(self, *args, **kwargs):\n        self.msgid = kwargs.pop('msgid', None)\n        self.appname = kwargs.pop('appname', None)\n        super().__init__(*args, **kwargs)\n\n    def format(self, record):\n        version = 1\n        asctime = datetime.datetime.fromtimestamp(record.created).isoformat()\n        m = self.tz_offset.match(time.strftime('%z'))\n        has_offset = False\n        if m and time.timezone:\n            hrs, mins = m.groups()\n            if int(hrs) or int(mins):\n                has_offset = True\n        if not has_offset:\n            asctime += 'Z'\n        else:\n            asctime += f'{hrs}:{mins}'\n        try:\n            hostname = socket.gethostname()\n        except Exception:\n            hostname = '-'\n        appname = self.appname or '-'\n        procid = record.process\n        msgid = '-'\n        msg = super().format(record)\n        sdata = '-'\n        if hasattr(record, 'structured_data'):\n            sd = record.structured_data\n            # This should be a dict where the keys are SD-ID and the value is a\n            # dict mapping PARAM-NAME to PARAM-VALUE (refer to the RFC for what these\n            # mean)\n            # There's no error checking here - it's purely for illustration, and you\n            # can adapt this code for use in production environments\n            parts = []\n\n            def replacer(m):\n                g = m.groups()\n                return '\\\\' + g[0]\n\n            for sdid, dv in sd.items():\n                part = f'[{sdid}'\n                for k, v in dv.items():\n                    s = str(v)\n                    s = self.escaped.sub(replacer, s)\n                    part += f' {k}=\"{s}\"'\n                part += ']'\n                parts.append(part)\n            sdata = ''.join(parts)\n        return f'{version} {asctime} {hostname} {appname} {procid} {msgid} {sdata} {msg}'\n\nYou’ll need to be familiar with RFC 5424 to fully understand the above code, and it may be that you have slightly different needs (e.g. for how you pass structural data to the log). Nevertheless, the above should be adaptable to your speciric needs. With the above handler, you’d pass structured data using something like this:\nsd = {\n    'foo@12345': {'bar': 'baz', 'baz': 'bozz', 'fizz': r'buzz'},\n    'foo@54321': {'rab': 'baz', 'zab': 'bozz', 'zzif': r'buzz'}\n}\nextra = {'structured_data': sd}\ni = 1\nlogger.debug('Message %d', i, extra=extra)\n\n\n\n\nSometimes, you need to interface to a third-party API which expects a file-like object to write to, but you want to direct the API’s output to a logger. You can do this using a class which wraps a logger with a file-like API. Here’s a short script illustrating such a class:\nimport logging\n\nclass LoggerWriter:\n    def __init__(self, logger, level):\n        self.logger = logger\n        self.level = level\n\n    def write(self, message):\n        if message != '\\n':  # avoid printing bare newlines, if you like\n            self.logger.log(self.level, message)\n\n    def flush(self):\n        # doesn't actually do anything, but might be expected of a file-like\n        # object - so optional depending on your situation\n        pass\n\n    def close(self):\n        # doesn't actually do anything, but might be expected of a file-like\n        # object - so optional depending on your situation. You might want\n        # to set a flag so that later calls to write raise an exception\n        pass\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG)\n    logger = logging.getLogger('demo')\n    info_fp = LoggerWriter(logger, logging.INFO)\n    debug_fp = LoggerWriter(logger, logging.DEBUG)\n    print('An INFO message', file=info_fp)\n    print('A DEBUG message', file=debug_fp)\n\nif __name__ == \"__main__\":\n    main()\n\nWhen this script is run, it prints\nINFO:demo:An INFO message\nDEBUG:demo:A DEBUG message\n\nYou could also use LoggerWriter to redirect sys.stdout and sys.stderr by doing something like this:\nimport sys\n\nsys.stdout = LoggerWriter(logger, logging.INFO)\nsys.stderr = LoggerWriter(logger, logging.WARNING)\n\nYou should do this after configuring logging for your needs. In the above example, the basicConfig() call does this (using the sys.stderr value before it is overwritten by a LoggerWriter instance). Then, you’d get this kind of result:\n&gt;&gt;&gt;\n&gt;&gt;&gt; print('Foo')\nINFO:demo:Foo\n&gt;&gt;&gt; print('Bar', file=sys.stderr)\nWARNING:demo:Bar\n&gt;&gt;&gt;\n\nOf course, the examples above show output according to the format used by basicConfig(), but you can use a different formatter when you configure logging.\nNote that with the above scheme, you are somewhat at the mercy of buffering and the sequence of write calls which you are intercepting. For example, with the definition of LoggerWriter above, if you have the snippet\nsys.stderr = LoggerWriter(logger, logging.WARNING)\n1 / 0\n\nthen running the script results in\nWARNING:demo:Traceback (most recent call last):\n\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/test.py\", line 53, in &lt;module&gt;\n\nWARNING:demo:\nWARNING:demo:main()\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/test.py\", line 49, in main\n\nWARNING:demo:\nWARNING:demo:1 / 0\nWARNING:demo:ZeroDivisionError\nWARNING:demo::\nWARNING:demo:division by zero\n\nAs you can see, this output isn’t ideal. That’s because the underlying code which writes to sys.stderr makes multiple writes, each of which results in a separate logged line (for example, the last three lines above). To get around this problem, you need to buffer things and only output log lines when newlines are seen. Let’s use a slightly better implementation of LoggerWriter:\nclass BufferingLoggerWriter(LoggerWriter):\n    def __init__(self, logger, level):\n        super().__init__(logger, level)\n        self.buffer = ''\n\n    def write(self, message):\n        if '\\n' not in message:\n            self.buffer += message\n        else:\n            parts = message.split('\\n')\n            if self.buffer:\n                s = self.buffer + parts.pop(0)\n                self.logger.log(self.level, s)\n            self.buffer = parts.pop()\n            for part in parts:\n                self.logger.log(self.level, part)\n\nThis just buffers up stuff until a newline is seen, and then logs complete lines. With this approach, you get better output:\nWARNING:demo:Traceback (most recent call last):\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/main.py\", line 55, in &lt;module&gt;\nWARNING:demo:    main()\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/main.py\", line 52, in main\nWARNING:demo:    1/0\nWARNING:demo:ZeroDivisionError: division by zero\n\n\n\n\nAlthough the preceding sections have described ways of doing things you might need to do or deal with, it is worth mentioning some usage patterns which are unhelpful, and which should therefore be avoided in most cases. The following sections are in no particular order.\n\n\nOn Windows, you will generally not be able to open the same file multiple times as this will lead to a “file is in use by another process” error. However, on POSIX platforms you’ll not get any errors if you open the same file multiple times. This could be done accidentally, for example by:\n\nAdding a file handler more than once which references the same file (e.g. by a copy/paste/forget-to-change error).\nOpening two files that look different, as they have different names, but are the same because one is a symbolic link to the other.\nForking a process, following which both parent and child have a reference to the same file. This might be through use of the multiprocessing module, for example.\n\nOpening a file multiple times might appear to work most of the time, but can lead to a number of problems in practice:\n\nLogging output can be garbled because multiple threads or processes try to write to the same file. Although logging guards against concurrent use of the same handler instance by multiple threads, there is no such protection if concurrent writes are attempted by two different threads using two different handler instances which happen to point to the same file.\nAn attempt to delete a file (e.g. during file rotation) silently fails, because there is another reference pointing to it. This can lead to confusion and wasted debugging time - log entries end up in unexpected places, or are lost altogether. Or a file that was supposed to be moved remains in place, and grows in size unexpectedly despite size-based rotation being supposedly in place.\n\nUse the techniques outlined in Logging to a single file from multiple processes to circumvent such issues.\n\n\n\nWhile there might be unusual cases where you’ll need to do this, in general there is no point because loggers are singletons. Code can always access a given logger instance by name using logging.getLogger(name), so passing instances around and holding them as instance attributes is pointless. Note that in other languages such as Java and C#, loggers are often static class attributes. However, this pattern doesn’t make sense in Python, where the module (and not the class) is the unit of software decomposition.\n\n\n\nConfiguring logging by adding handlers, formatters and filters is the responsibility of the application developer, not the library developer. If you are maintaining a library, ensure that you don’t add handlers to any of your loggers other than a NullHandler instance.\n\n\n\nLoggers are singletons that are never freed during a script execution, and so creating lots of loggers will use up memory which can’t then be freed. Rather than create a logger per e.g. file processed or network connection made, use the existing mechanisms for passing contextual information into your logs and restrict the loggers created to those describing areas within your application (generally modules, but occasionally slightly more fine-grained than that)."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#using-logging-in-multiple-modules",
    "href": "knowledgebase/python/python3_logging_cookbook.html#using-logging-in-multiple-modules",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Multiple calls to logging.getLogger('someLogger') return a reference to the same logger object. This is true not only within the same module, but also across modules as long as it is in the same Python interpreter process. It is true for references to the same object; additionally, application code can define and configure a parent logger in one module and create (but not configure) a child logger in a separate module, and all logger calls to the child will pass up to the parent. Here is a main module:\nimport logging\nimport auxiliary_module\n\n# create logger with 'spam_application'\nlogger = logging.getLogger('spam_application')\nlogger.setLevel(logging.DEBUG)\n# create file handler which logs even debug messages\nfh = logging.FileHandler('spam.log')\nfh.setLevel(logging.DEBUG)\n# create console handler with a higher log level\nch = logging.StreamHandler()\nch.setLevel(logging.ERROR)\n# create formatter and add it to the handlers\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfh.setFormatter(formatter)\nch.setFormatter(formatter)\n# add the handlers to the logger\nlogger.addHandler(fh)\nlogger.addHandler(ch)\n\nlogger.info('creating an instance of auxiliary_module.Auxiliary')\na = auxiliary_module.Auxiliary()\nlogger.info('created an instance of auxiliary_module.Auxiliary')\nlogger.info('calling auxiliary_module.Auxiliary.do_something')\na.do_something()\nlogger.info('finished auxiliary_module.Auxiliary.do_something')\nlogger.info('calling auxiliary_module.some_function()')\nauxiliary_module.some_function()\nlogger.info('done with auxiliary_module.some_function()')\n\nHere is the auxiliary module:\nimport logging\n\n# create logger\nmodule_logger = logging.getLogger('spam_application.auxiliary')\n\nclass Auxiliary:\n    def __init__(self):\n        self.logger = logging.getLogger('spam_application.auxiliary.Auxiliary')\n        self.logger.info('creating an instance of Auxiliary')\n\n    def do_something(self):\n        self.logger.info('doing something')\n        a = 1 + 1\n        self.logger.info('done doing something')\n\ndef some_function():\n    module_logger.info('received a call to \"some_function\"')\n\nThe output looks like this:\n2005-03-23 23:47:11,663 - spam_application - INFO -\n   creating an instance of auxiliary_module.Auxiliary\n2005-03-23 23:47:11,665 - spam_application.auxiliary.Auxiliary - INFO -\n   creating an instance of Auxiliary\n2005-03-23 23:47:11,665 - spam_application - INFO -\n   created an instance of auxiliary_module.Auxiliary\n2005-03-23 23:47:11,668 - spam_application - INFO -\n   calling auxiliary_module.Auxiliary.do_something\n2005-03-23 23:47:11,668 - spam_application.auxiliary.Auxiliary - INFO -\n   doing something\n2005-03-23 23:47:11,669 - spam_application.auxiliary.Auxiliary - INFO -\n   done doing something\n2005-03-23 23:47:11,670 - spam_application - INFO -\n   finished auxiliary_module.Auxiliary.do_something\n2005-03-23 23:47:11,671 - spam_application - INFO -\n   calling auxiliary_module.some_function()\n2005-03-23 23:47:11,672 - spam_application.auxiliary - INFO -\n   received a call to 'some_function'\n2005-03-23 23:47:11,673 - spam_application - INFO -\n   done with auxiliary_module.some_function()"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#logging-from-multiple-threads",
    "href": "knowledgebase/python/python3_logging_cookbook.html#logging-from-multiple-threads",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Logging from multiple threads requires no special effort. The following example shows logging from the main (initial) thread and another thread:\nimport logging\nimport threading\nimport time\n\ndef worker(arg):\n    while not arg['stop']:\n        logging.debug('Hi from myfunc')\n        time.sleep(0.5)\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG, format='%(relativeCreated)6d %(threadName)s %(message)s')\n    info = {'stop': False}\n    thread = threading.Thread(target=worker, args=(info,))\n    thread.start()\n    while True:\n        try:\n            logging.debug('Hello from main')\n            time.sleep(0.75)\n        except KeyboardInterrupt:\n            info['stop'] = True\n            break\n    thread.join()\n\nif __name__ == '__main__':\n    main()\n\nWhen run, the script should print something like the following:\n   0 Thread-1 Hi from myfunc\n   3 MainThread Hello from main\n 505 Thread-1 Hi from myfunc\n 755 MainThread Hello from main\n1007 Thread-1 Hi from myfunc\n1507 MainThread Hello from main\n1508 Thread-1 Hi from myfunc\n2010 Thread-1 Hi from myfunc\n2258 MainThread Hello from main\n2512 Thread-1 Hi from myfunc\n3009 MainThread Hello from main\n3013 Thread-1 Hi from myfunc\n3515 Thread-1 Hi from myfunc\n3761 MainThread Hello from main\n4017 Thread-1 Hi from myfunc\n4513 MainThread Hello from main\n4518 Thread-1 Hi from myfunc\n\nThis shows the logging output interspersed as one might expect. This approach works for more threads than shown here, of course."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#multiple-handlers-and-formatters",
    "href": "knowledgebase/python/python3_logging_cookbook.html#multiple-handlers-and-formatters",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Loggers are plain Python objects. The addHandler() method has no minimum or maximum quota for the number of handlers you may add. Sometimes it will be beneficial for an application to log all messages of all severities to a text file while simultaneously logging errors or above to the console. To set this up, simply configure the appropriate handlers. The logging calls in the application code will remain unchanged. Here is a slight modification to the previous simple module-based configuration example:\nimport logging\n\nlogger = logging.getLogger('simple_example')\nlogger.setLevel(logging.DEBUG)\n# create file handler which logs even debug messages\nfh = logging.FileHandler('spam.log')\nfh.setLevel(logging.DEBUG)\n# create console handler with a higher log level\nch = logging.StreamHandler()\nch.setLevel(logging.ERROR)\n# create formatter and add it to the handlers\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nch.setFormatter(formatter)\nfh.setFormatter(formatter)\n# add the handlers to logger\nlogger.addHandler(ch)\nlogger.addHandler(fh)\n\n# 'application' code\nlogger.debug('debug message')\nlogger.info('info message')\nlogger.warning('warn message')\nlogger.error('error message')\nlogger.critical('critical message')\n\nNotice that the ‘application’ code does not care about multiple handlers. All that changed was the addition and configuration of a new handler named fh.\nThe ability to create new handlers with higher- or lower-severity filters can be very helpful when writing and testing an application. Instead of using many print statements for debugging, use logger.debug: Unlike the print statements, which you will have to delete or comment out later, the logger.debug statements can remain intact in the source code and remain dormant until you need them again. At that time, the only change that needs to happen is to modify the severity level of the logger and/or handler to debug."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#logging-to-multiple-destinations",
    "href": "knowledgebase/python/python3_logging_cookbook.html#logging-to-multiple-destinations",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Let’s say you want to log to console and file with different message formats and in differing circumstances. Say you want to log messages with levels of DEBUG and higher to file, and those messages at level INFO and higher to the console. Let’s also assume that the file should contain timestamps, but the console messages should not. Here’s how you can achieve this:\nimport logging\n\n# set up logging to file - see previous section for more details\nlogging.basicConfig(level=logging.DEBUG,\n                    format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n                    datefmt='%m-%d %H:%M',\n                    filename='/tmp/myapp.log',\n                    filemode='w')\n# define a Handler which writes INFO messages or higher to the sys.stderr\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n# set a format which is simpler for console use\nformatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')\n# tell the handler to use this format\nconsole.setFormatter(formatter)\n# add the handler to the root logger\nlogging.getLogger('').addHandler(console)\n\n# Now, we can log to the root logger, or any other logger. First the root...\nlogging.info('Jackdaws love my big sphinx of quartz.')\n\n# Now, define a couple of other loggers which might represent areas in your\n# application:\n\nlogger1 = logging.getLogger('myapp.area1')\nlogger2 = logging.getLogger('myapp.area2')\n\nlogger1.debug('Quick zephyrs blow, vexing daft Jim.')\nlogger1.info('How quickly daft jumping zebras vex.')\nlogger2.warning('Jail zesty vixen who grabbed pay from quack.')\nlogger2.error('The five boxing wizards jump quickly.')\n\nWhen you run this, on the console you will see\nroot        : INFO     Jackdaws love my big sphinx of quartz.\nmyapp.area1 : INFO     How quickly daft jumping zebras vex.\nmyapp.area2 : WARNING  Jail zesty vixen who grabbed pay from quack.\nmyapp.area2 : ERROR    The five boxing wizards jump quickly.\n\nand in the file you will see something like\n10-22 22:19 root         INFO     Jackdaws love my big sphinx of quartz.\n10-22 22:19 myapp.area1  DEBUG    Quick zephyrs blow, vexing daft Jim.\n10-22 22:19 myapp.area1  INFO     How quickly daft jumping zebras vex.\n10-22 22:19 myapp.area2  WARNING  Jail zesty vixen who grabbed pay from quack.\n10-22 22:19 myapp.area2  ERROR    The five boxing wizards jump quickly.\n\nAs you can see, the DEBUG message only shows up in the file. The other messages are sent to both destinations.\nThis example uses console and file handlers, but you can use any number and combination of handlers you choose.\nNote that the above choice of log filename /tmp/myapp.log implies use of a standard location for temporary files on POSIX systems. On Windows, you may need to choose a different directory name for the log - just ensure that the directory exists and that you have the permissions to create and update files in it."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#custom-handling-of-levels",
    "href": "knowledgebase/python/python3_logging_cookbook.html#custom-handling-of-levels",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Sometimes, you might want to do something slightly different from the standard handling of levels in handlers, where all levels above a threshold get processed by a handler. To do this, you need to use filters. Let’s look at a scenario where you want to arrange things as follows:\n\nSend messages of severity INFO and WARNING to sys.stdout\nSend messages of severity ERROR and above to sys.stderr\nSend messages of severity DEBUG and above to file app.log\n\nSuppose you configure logging with the following JSON:\n{\n    \"version\": 1,\n    \"disable_existing_loggers\": false,\n    \"formatters\": {\n        \"simple\": {\n            \"format\": \"%(levelname)-8s - %(message)s\"\n        }\n    },\n    \"handlers\": {\n        \"stdout\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"INFO\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stdout\"\n        },\n        \"stderr\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"ERROR\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stderr\"\n        },\n        \"file\": {\n            \"class\": \"logging.FileHandler\",\n            \"formatter\": \"simple\",\n            \"filename\": \"app.log\",\n            \"mode\": \"w\"\n        }\n    },\n    \"root\": {\n        \"level\": \"DEBUG\",\n        \"handlers\": [\n            \"stderr\",\n            \"stdout\",\n            \"file\"\n        ]\n    }\n}\n\nThis configuration does almost what we want, except that sys.stdout would show messages of severity ERROR and only events of this severity and higher will be tracked as well as INFO and WARNING messages. To prevent this, we can set up a filter which excludes those messages and add it to the relevant handler. This can be configured by adding a filters section parallel to formatters and handlers:\n{\n    \"filters\": {\n        \"warnings_and_below\": {\n            \"()\" : \"__main__.filter_maker\",\n            \"level\": \"WARNING\"\n        }\n    }\n}\n\nand changing the section on the stdout handler to add it:\n{\n    \"stdout\": {\n        \"class\": \"logging.StreamHandler\",\n        \"level\": \"INFO\",\n        \"formatter\": \"simple\",\n        \"stream\": \"ext://sys.stdout\",\n        \"filters\": [\"warnings_and_below\"]\n    }\n}\n\nA filter is just a function, so we can define the filter_maker (a factory function) as follows:\ndef filter_maker(level):\n    level = getattr(logging, level)\n\n    def filter(record):\n        return record.levelno &lt;= level\n\n    return filter\n\nThis converts the string argument passed in to a numeric level, and returns a function which only returns True if the level of the passed in record is at or below the specified level. Note that in this example I have defined the filter_maker in a test script main.py that I run from the command line, so its module will be __main__ - hence the __main__.filter_maker in the filter configuration. You will need to change that if you define it in a different module.\nWith the filter added, we can run main.py, which in full is:\nimport json\nimport logging\nimport logging.config\n\nCONFIG = '''\n{\n    \"version\": 1,\n    \"disable_existing_loggers\": false,\n    \"formatters\": {\n        \"simple\": {\n            \"format\": \"%(levelname)-8s - %(message)s\"\n        }\n    },\n    \"filters\": {\n        \"warnings_and_below\": {\n            \"()\" : \"__main__.filter_maker\",\n            \"level\": \"WARNING\"\n        }\n    },\n    \"handlers\": {\n        \"stdout\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"INFO\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stdout\",\n            \"filters\": [\"warnings_and_below\"]\n        },\n        \"stderr\": {\n            \"class\": \"logging.StreamHandler\",\n            \"level\": \"ERROR\",\n            \"formatter\": \"simple\",\n            \"stream\": \"ext://sys.stderr\"\n        },\n        \"file\": {\n            \"class\": \"logging.FileHandler\",\n            \"formatter\": \"simple\",\n            \"filename\": \"app.log\",\n            \"mode\": \"w\"\n        }\n    },\n    \"root\": {\n        \"level\": \"DEBUG\",\n        \"handlers\": [\n            \"stderr\",\n            \"stdout\",\n            \"file\"\n        ]\n    }\n}\n'''\n\ndef filter_maker(level):\n    level = getattr(logging, level)\n\n    def filter(record):\n        return record.levelno &lt;= level\n\n    return filter\n\nlogging.config.dictConfig(json.loads(CONFIG))\nlogging.debug('A DEBUG message')\nlogging.info('An INFO message')\nlogging.warning('A WARNING message')\nlogging.error('An ERROR message')\nlogging.critical('A CRITICAL message')\n\nAnd after running it like this:\npython main.py 2&gt;stderr.log &gt;stdout.log\n\nWe can see the results are as expected:\n$ more *.log\n::::::::::::::\napp.log\n::::::::::::::\nDEBUG    - A DEBUG message\nINFO     - An INFO message\nWARNING  - A WARNING message\nERROR    - An ERROR message\nCRITICAL - A CRITICAL message\n::::::::::::::\nstderr.log\n::::::::::::::\nERROR    - An ERROR message\nCRITICAL - A CRITICAL message\n::::::::::::::\nstdout.log\n::::::::::::::\nINFO     - An INFO message\nWARNING  - A WARNING message"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#configuration-server-example",
    "href": "knowledgebase/python/python3_logging_cookbook.html#configuration-server-example",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Here is an example of a module using the logging configuration server:\nimport logging\nimport logging.config\nimport time\nimport os\n\n# read initial config file\nlogging.config.fileConfig('logging.conf')\n\n# create and start listener on port 9999\nt = logging.config.listen(9999)\nt.start()\n\nlogger = logging.getLogger('simpleExample')\n\ntry:\n    # loop through logging calls to see the difference\n    # new configurations make, until Ctrl+C is pressed\n    while True:\n        logger.debug('debug message')\n        logger.info('info message')\n        logger.warning('warn message')\n        logger.error('error message')\n        logger.critical('critical message')\n        time.sleep(5)\nexcept KeyboardInterrupt:\n    # cleanup\n    logging.config.stopListening()\n    t.join()\n\nAnd here is a script that takes a filename and sends that file to the server, properly preceded with the binary-encoded length, as the new logging configuration:\n#!/usr/bin/env python\nimport socket, sys, struct\n\nwith open(sys.argv[1], 'rb') as f:\n    data_to_send = f.read()\n\nHOST = 'localhost'\nPORT = 9999\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nprint('connecting...')\ns.connect((HOST, PORT))\nprint('sending config...')\ns.send(struct.pack('&gt;L', len(data_to_send)))\ns.send(data_to_send)\ns.close()\nprint('complete')"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#dealing-with-handlers-that-block",
    "href": "knowledgebase/python/python3_logging_cookbook.html#dealing-with-handlers-that-block",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Sometimes you have to get your logging handlers to do their work without blocking the thread you’re logging from. This is common in web applications, though of course it also occurs in other scenarios.\nA common culprit which demonstrates sluggish behaviour is the SMTPHandler: sending emails can take a long time, for a number of reasons outside the developer’s control (for example, a poorly performing mail or network infrastructure). But almost any network-based handler can block: Even a SocketHandler operation may do a DNS query under the hood which is too slow (and this query can be deep in the socket library code, below the Python layer, and outside your control).\nOne solution is to use a two-part approach. For the first part, attach only a QueueHandler to those loggers which are accessed from performance-critical threads. They simply write to their queue, which can be sized to a large enough capacity or initialized with no upper bound to their size. The write to the queue will typically be accepted quickly, though you will probably need to catch the queue.Full exception as a precaution in your code. If you are a library developer who has performance-critical threads in their code, be sure to document this (together with a suggestion to attach only QueueHandlers to your loggers) for the benefit of other developers who will use your code.\nThe second part of the solution is QueueListener, which has been designed as the counterpart to QueueHandler. A QueueListener is very simple: it’s passed a queue and some handlers, and it fires up an internal thread which listens to its queue for LogRecords sent from QueueHandlers (or any other source of LogRecords, for that matter). The LogRecords are removed from the queue and passed to the handlers for processing.\nThe advantage of having a separate QueueListener class is that you can use the same instance to service multiple QueueHandlers. This is more resource-friendly than, say, having threaded versions of the existing handler classes, which would eat up one thread per handler for no particular benefit.\nAn example of using these two classes follows (imports omitted):\nque = queue.Queue(-1)  # no limit on size\nqueue_handler = QueueHandler(que)\nhandler = logging.StreamHandler()\nlistener = QueueListener(que, handler)\nroot = logging.getLogger()\nroot.addHandler(queue_handler)\nformatter = logging.Formatter('%(threadName)s: %(message)s')\nhandler.setFormatter(formatter)\nlistener.start()\n# The log output will display the thread which generated\n# the event (the main thread) rather than the internal\n# thread which monitors the internal queue. This is what\n# you want to happen.\nroot.warning('Look out!')\nlistener.stop()\n\nwhich, when run, will produce:\nNote\nAlthough the earlier discussion wasn’t specifically talking about async code, but rather about slow logging handlers, it should be noted that when logging from async code, network and even file handlers could lead to problems (blocking the event loop) because some logging is done from asyncio internals. It might be best, if any async code is used in an application, to use the above approach for logging, so that any blocking code runs only in the QueueListener thread.\nChanged in version 3.5: Prior to Python 3.5, the QueueListener always passed every message received from the queue to every handler it was initialized with. (This was because it was assumed that level filtering was all done on the other side, where the queue is filled.) From 3.5 onwards, this behaviour can be changed by passing a keyword argument respect_handler_level=True to the listener’s constructor. When this is done, the listener compares the level of each message with the handler’s level, and only passes a message to a handler if it’s appropriate to do so."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#sending-and-receiving-logging-events-across-a-network",
    "href": "knowledgebase/python/python3_logging_cookbook.html#sending-and-receiving-logging-events-across-a-network",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Let’s say you want to send logging events across a network, and handle them at the receiving end. A simple way of doing this is attaching a SocketHandler instance to the root logger at the sending end:\nimport logging, logging.handlers\n\nrootLogger = logging.getLogger('')\nrootLogger.setLevel(logging.DEBUG)\nsocketHandler = logging.handlers.SocketHandler('localhost',\n                    logging.handlers.DEFAULT_TCP_LOGGING_PORT)\n# don't bother with a formatter, since a socket handler sends the event as\n# an unformatted pickle\nrootLogger.addHandler(socketHandler)\n\n# Now, we can log to the root logger, or any other logger. First the root...\nlogging.info('Jackdaws love my big sphinx of quartz.')\n\n# Now, define a couple of other loggers which might represent areas in your\n# application:\n\nlogger1 = logging.getLogger('myapp.area1')\nlogger2 = logging.getLogger('myapp.area2')\n\nlogger1.debug('Quick zephyrs blow, vexing daft Jim.')\nlogger1.info('How quickly daft jumping zebras vex.')\nlogger2.warning('Jail zesty vixen who grabbed pay from quack.')\nlogger2.error('The five boxing wizards jump quickly.')\n\nAt the receiving end, you can set up a receiver using the socketserver module. Here is a basic working example:\nimport pickle\nimport logging\nimport logging.handlers\nimport socketserver\nimport struct\n\n\nclass LogRecordStreamHandler(socketserver.StreamRequestHandler):\n    \"\"\"Handler for a streaming logging request.\n\n    This basically logs the record using whatever logging policy is\n    configured locally.\n    \"\"\"\n\n    def handle(self):\n        \"\"\"\n        Handle multiple requests - each expected to be a 4-byte length,\n        followed by the LogRecord in pickle format. Logs the record\n        according to whatever policy is configured locally.\n        \"\"\"\n        while True:\n            chunk = self.connection.recv(4)\n            if len(chunk) &lt; 4:\n                break\n            slen = struct.unpack('&gt;L', chunk)[0]\n            chunk = self.connection.recv(slen)\n            while len(chunk) &lt; slen:\n                chunk = chunk + self.connection.recv(slen - len(chunk))\n            obj = self.unPickle(chunk)\n            record = logging.makeLogRecord(obj)\n            self.handleLogRecord(record)\n\n    def unPickle(self, data):\n        return pickle.loads(data)\n\n    def handleLogRecord(self, record):\n        # if a name is specified, we use the named logger rather than the one\n        # implied by the record.\n        if self.server.logname is not None:\n            name = self.server.logname\n        else:\n            name = record.name\n        logger = logging.getLogger(name)\n        # N.B. EVERY record gets logged. This is because Logger.handle\n        # is normally called AFTER logger-level filtering. If you want\n        # to do filtering, do it at the client end to save wasting\n        # cycles and network bandwidth!\n        logger.handle(record)\n\nclass LogRecordSocketReceiver(socketserver.ThreadingTCPServer):\n    \"\"\"\n    Simple TCP socket-based logging receiver suitable for testing.\n    \"\"\"\n\n    allow_reuse_address = True\n\n    def __init__(self, host='localhost',\n                 port=logging.handlers.DEFAULT_TCP_LOGGING_PORT,\n                 handler=LogRecordStreamHandler):\n        socketserver.ThreadingTCPServer.__init__(self, (host, port), handler)\n        self.abort = 0\n        self.timeout = 1\n        self.logname = None\n\n    def serve_until_stopped(self):\n        import select\n        abort = 0\n        while not abort:\n            rd, wr, ex = select.select([self.socket.fileno()],\n                                       [], [],\n                                       self.timeout)\n            if rd:\n                self.handle_request()\n            abort = self.abort\n\ndef main():\n    logging.basicConfig(\n        format='%(relativeCreated)5d %(name)-15s %(levelname)-8s %(message)s')\n    tcpserver = LogRecordSocketReceiver()\n    print('About to start TCP server...')\n    tcpserver.serve_until_stopped()\n\nif __name__ == '__main__':\n    main()\n\nFirst run the server, and then the client. On the client side, nothing is printed on the console; on the server side, you should see something like:\nAbout to start TCP server...\n   59 root            INFO     Jackdaws love my big sphinx of quartz.\n   59 myapp.area1     DEBUG    Quick zephyrs blow, vexing daft Jim.\n   69 myapp.area1     INFO     How quickly daft jumping zebras vex.\n   69 myapp.area2     WARNING  Jail zesty vixen who grabbed pay from quack.\n   69 myapp.area2     ERROR    The five boxing wizards jump quickly.\n\nNote that there are some security issues with pickle in some scenarios. If these affect you, you can use an alternative serialization scheme by overriding the makePickle() method and implementing your alternative there, as well as adapting the above script to use your alternative serialization.\n\n\nTo run a logging listener in production, you may need to use a process-management tool such as Supervisor. Here is a Gist which provides the bare-bones files to run the above functionality using Supervisor. It consists of the following files:\n\nFile\n|\nPurpose\n\n\n\n\n\n\n\n\nprepare.sh\n|\nA Bash script to prepare the environment for testing\n| |\nsupervisor.conf\n|\nThe Supervisor configuration file, which has entries for the listener and a multi-process web application\n| |\nensure_app.sh\n|\nA Bash script to ensure that Supervisor is running with the above configuration\n| |\nlog_listener.py\n|\nThe socket listener program which receives log events and records them to a file\n| |\nmain.py\n|\nA simple web application which performs logging via a socket connected to the listener\n| |\nwebapp.json\n|\nA JSON configuration file for the web application\n| |\nclient.py\n|\nA Python script to exercise the web application\n|\nThe web application uses Gunicorn, which is a popular web application server that starts multiple worker processes to handle requests. This example setup shows how the workers can write to the same log file without conflicting with one another — they all go through the socket listener.\nTo test these files, do the following in a POSIX environment:\n\nDownload the Gist as a ZIP archive using the Download ZIP button.\nUnzip the above files from the archive into a scratch directory.\nIn the scratch directory, run bash prepare.sh to get things ready. This creates a run subdirectory to contain Supervisor-related and log files, and a venv subdirectory to contain a virtual environment into which bottle, gunicorn and supervisor are installed.\nRun bash ensure_app.sh to ensure that Supervisor is running with the above configuration.\nRun venv/bin/python client.py to exercise the web application, which will lead to records being written to the log.\nInspect the log files in the run subdirectory. You should see the most recent log lines in files matching the pattern app.log*. They won’t be in any particular order, since they have been handled concurrently by different worker processes in a non-deterministic way.\nYou can shut down the listener and the web application by running venv/bin/supervisorctl -c supervisor.conf shutdown.\n\nYou may need to tweak the configuration files in the unlikely event that the configured ports clash with something else in your test environment."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#adding-contextual-information-to-your-logging-output",
    "href": "knowledgebase/python/python3_logging_cookbook.html#adding-contextual-information-to-your-logging-output",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Sometimes you want logging output to contain contextual information in addition to the parameters passed to the logging call. For example, in a networked application, it may be desirable to log client-specific information in the log (e.g. remote client’s username, or IP address). Although you could use the extra parameter to achieve this, it’s not always convenient to pass the information in this way. While it might be tempting to create Logger instances on a per-connection basis, this is not a good idea because these instances are not garbage collected. While this is not a problem in practice, when the number of Logger instances is dependent on the level of granularity you want to use in logging an application, it could be hard to manage if the number of Logger instances becomes effectively unbounded.\n\n\nAn easy way in which you can pass contextual information to be output along with logging event information is to use the LoggerAdapter class. This class is designed to look like a Logger, so that you can call debug(), info(), warning(), error(), exception(), critical() and log(). These methods have the same signatures as their counterparts in Logger, so you can use the two types of instances interchangeably.\nWhen you create an instance of LoggerAdapter, you pass it a Logger instance and a dict-like object which contains your contextual information. When you call one of the logging methods on an instance of LoggerAdapter, it delegates the call to the underlying instance of Logger passed to its constructor, and arranges to pass the contextual information in the delegated call. Here’s a snippet from the code of LoggerAdapter:\ndef debug(self, msg, /, *args, **kwargs):\n    \"\"\"\n    Delegate a debug call to the underlying logger, after adding\n    contextual information from this adapter instance.\n    \"\"\"\n    msg, kwargs = self.process(msg, kwargs)\n    self.logger.debug(msg, *args, **kwargs)\n\nThe process() method of LoggerAdapter is where the contextual information is added to the logging output. It’s passed the message and keyword arguments of the logging call, and it passes back (potentially) modified versions of these to use in the call to the underlying logger. The default implementation of this method leaves the message alone, but inserts an ‘extra’ key in the keyword argument whose value is the dict-like object passed to the constructor. Of course, if you had passed an ‘extra’ keyword argument in the call to the adapter, it will be silently overwritten.\nThe advantage of using ‘extra’ is that the values in the dict-like object are merged into the LogRecord instance’s __dict__, allowing you to use customized strings with your Formatter instances which know about the keys of the dict-like object. If you need a different method, e.g. if you want to prepend or append the contextual information to the message string, you just need to subclass LoggerAdapter and override process() to do what you need. Here is a simple example:\nclass CustomAdapter(logging.LoggerAdapter):\n    \"\"\"\n    This example adapter expects the passed in dict-like object to have a\n    'connid' key, whose value in brackets is prepended to the log message.\n    \"\"\"\n    def process(self, msg, kwargs):\n        return '[%s] %s' % (self.extra['connid'], msg), kwargs\n\nwhich you can use like this:\nlogger = logging.getLogger(__name__)\nadapter = CustomAdapter(logger, {'connid': some_conn_id})\n\nThen any events that you log to the adapter will have the value of some_conn_id prepended to the log messages.\n\n\nYou don’t need to pass an actual dict to a LoggerAdapter - you could pass an instance of a class which implements __getitem__ and __iter__ so that it looks like a dict to logging. This would be useful if you want to generate values dynamically (whereas the values in a dict would be constant).\n\n\n\n\nYou can also add contextual information to log output using a user-defined Filter. Filter instances are allowed to modify the LogRecords passed to them, including adding additional attributes which can then be output using a suitable format string, or if needed a custom Formatter.\nFor example in a web application, the request being processed (or at least, the interesting parts of it) can be stored in a threadlocal (threading.local) variable, and then accessed from a Filter to add, say, information from the request - say, the remote IP address and remote user’s username - to the LogRecord, using the attribute names ‘ip’ and ‘user’ as in the LoggerAdapter example above. In that case, the same format string can be used to get similar output to that shown above. Here’s an example script:\nimport logging\nfrom random import choice\n\nclass ContextFilter(logging.Filter):\n    \"\"\"\n    This is a filter which injects contextual information into the log.\n\n    Rather than use actual contextual information, we just use random\n    data in this demo.\n    \"\"\"\n\n    USERS = ['jim', 'fred', 'sheila']\n    IPS = ['123.231.231.123', '127.0.0.1', '192.168.0.1']\n\n    def filter(self, record):\n\n        record.ip = choice(ContextFilter.IPS)\n        record.user = choice(ContextFilter.USERS)\n        return True\n\nif __name__ == '__main__':\n    levels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR, logging.CRITICAL)\n    logging.basicConfig(level=logging.DEBUG,\n                        format='%(asctime)-15s %(name)-5s %(levelname)-8s IP: %(ip)-15s User: %(user)-8s %(message)s')\n    a1 = logging.getLogger('a.b.c')\n    a2 = logging.getLogger('d.e.f')\n\n    f = ContextFilter()\n    a1.addFilter(f)\n    a2.addFilter(f)\n    a1.debug('A debug message')\n    a1.info('An info message with %s', 'some parameters')\n    for x in range(10):\n        lvl = choice(levels)\n        lvlname = logging.getLevelName(lvl)\n        a2.log(lvl, 'A message at %s level with %d %s', lvlname, 2, 'parameters')\n\nwhich, when run, produces something like:\n2010-09-06 22:38:15,292 a.b.c DEBUG    IP: 123.231.231.123 User: fred     A debug message\n2010-09-06 22:38:15,300 a.b.c INFO     IP: 192.168.0.1     User: sheila   An info message with some parameters\n2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f ERROR    IP: 127.0.0.1       User: jim      A message at ERROR level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 127.0.0.1       User: sheila   A message at DEBUG level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f ERROR    IP: 123.231.231.123 User: fred     A message at ERROR level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 192.168.0.1     User: jim      A message at CRITICAL level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f CRITICAL IP: 127.0.0.1       User: sheila   A message at CRITICAL level with 2 parameters\n2010-09-06 22:38:15,300 d.e.f DEBUG    IP: 192.168.0.1     User: jim      A message at DEBUG level with 2 parameters\n2010-09-06 22:38:15,301 d.e.f ERROR    IP: 127.0.0.1       User: sheila   A message at ERROR level with 2 parameters\n2010-09-06 22:38:15,301 d.e.f DEBUG    IP: 123.231.231.123 User: fred     A message at DEBUG level with 2 parameters\n2010-09-06 22:38:15,301 d.e.f INFO     IP: 123.231.231.123 User: fred     A message at INFO level with 2 parameters"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#use-of-contextvars",
    "href": "knowledgebase/python/python3_logging_cookbook.html#use-of-contextvars",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Since Python 3.7, the contextvars module has provided context-local storage which works for both threading and asyncio processing needs. This type of storage may thus be generally preferable to thread-locals. The following example shows how, in a multi-threaded environment, logs can populated with contextual information such as, for example, request attributes handled by web applications.\nFor the purposes of illustration, say that you have different web applications, each independent of the other but running in the same Python process and using a library common to them. How can each of these applications have their own log, where all logging messages from the library (and other request processing code) are directed to the appropriate application’s log file, while including in the log additional contextual information such as client IP, HTTP request method and client username?\nLet’s assume that the library can be simulated by the following code:\n# webapplib.py\nimport logging\nimport time\n\nlogger = logging.getLogger(__name__)\n\ndef useful():\n    # Just a representative event logged from the library\n    logger.debug('Hello from webapplib!')\n    # Just sleep for a bit so other threads get to run\n    time.sleep(0.01)\n\nWe can simulate the multiple web applications by means of two simple classes, Request and WebApp. These simulate how real threaded web applications work - each request is handled by a thread:\n# main.py\nimport argparse\nfrom contextvars import ContextVar\nimport logging\nimport os\nfrom random import choice\nimport threading\nimport webapplib\n\nlogger = logging.getLogger(__name__)\nroot = logging.getLogger()\nroot.setLevel(logging.DEBUG)\n\nclass Request:\n    \"\"\"\n    A simple dummy request class which just holds dummy HTTP request method,\n    client IP address and client username\n    \"\"\"\n    def __init__(self, method, ip, user):\n        self.method = method\n        self.ip = ip\n        self.user = user\n\n# A dummy set of requests which will be used in the simulation - we'll just pick\n# from this list randomly. Note that all GET requests are from 192.168.2.XXX\n# addresses, whereas POST requests are from 192.16.3.XXX addresses. Three users\n# are represented in the sample requests.\n\nREQUESTS = [\n    Request('GET', '192.168.2.20', 'jim'),\n    Request('POST', '192.168.3.20', 'fred'),\n    Request('GET', '192.168.2.21', 'sheila'),\n    Request('POST', '192.168.3.21', 'jim'),\n    Request('GET', '192.168.2.22', 'fred'),\n    Request('POST', '192.168.3.22', 'sheila'),\n]\n\n# Note that the format string includes references to request context information\n# such as HTTP method, client IP and username\n\nformatter = logging.Formatter('%(threadName)-11s %(appName)s %(name)-9s %(user)-6s %(ip)s %(method)-4s %(message)s')\n\n# Create our context variables. These will be filled at the start of request\n# processing, and used in the logging that happens during that processing\n\nctx_request = ContextVar('request')\nctx_appname = ContextVar('appname')\n\nclass InjectingFilter(logging.Filter):\n    \"\"\"\n    A filter which injects context-specific information into logs and ensures\n    that only information for a specific webapp is included in its log\n    \"\"\"\n    def __init__(self, app):\n        self.app = app\n\n    def filter(self, record):\n        request = ctx_request.get()\n        record.method = request.method\n        record.ip = request.ip\n        record.user = request.user\n        record.appName = appName = ctx_appname.get()\n        return appName == self.app.name\n\nclass WebApp:\n    \"\"\"\n    A dummy web application class which has its own handler and filter for a\n    webapp-specific log.\n    \"\"\"\n    def __init__(self, name):\n        self.name = name\n        handler = logging.FileHandler(name + '.log', 'w')\n        f = InjectingFilter(self)\n        handler.setFormatter(formatter)\n        handler.addFilter(f)\n        root.addHandler(handler)\n        self.num_requests = 0\n\n    def process_request(self, request):\n        \"\"\"\n        This is the dummy method for processing a request. It's called on a\n        different thread for every request. We store the context information into\n        the context vars before doing anything else.\n        \"\"\"\n        ctx_request.set(request)\n        ctx_appname.set(self.name)\n        self.num_requests += 1\n        logger.debug('Request processing started')\n        webapplib.useful()\n        logger.debug('Request processing finished')\n\ndef main():\n    fn = os.path.splitext(os.path.basename(__file__))[0]\n    adhf = argparse.ArgumentDefaultsHelpFormatter\n    ap = argparse.ArgumentParser(formatter_class=adhf, prog=fn,\n                                 description='Simulate a couple of web '\n                                             'applications handling some '\n                                             'requests, showing how request '\n                                             'context can be used to '\n                                             'populate logs')\n    aa = ap.add_argument\n    aa('--count', '-c', type=int, default=100, help='How many requests to simulate')\n    options = ap.parse_args()\n\n    # Create the dummy webapps and put them in a list which we can use to select\n    # from randomly\n    app1 = WebApp('app1')\n    app2 = WebApp('app2')\n    apps = [app1, app2]\n    threads = []\n    # Add a common handler which will capture all events\n    handler = logging.FileHandler('app.log', 'w')\n    handler.setFormatter(formatter)\n    root.addHandler(handler)\n\n    # Generate calls to process requests\n    for i in range(options.count):\n        try:\n            # Pick an app at random and a request for it to process\n            app = choice(apps)\n            request = choice(REQUESTS)\n            # Process the request in its own thread\n            t = threading.Thread(target=app.process_request, args=(request,))\n            threads.append(t)\n            t.start()\n        except KeyboardInterrupt:\n            break\n\n    # Wait for the threads to terminate\n    for t in threads:\n        t.join()\n\n    for app in apps:\n        print('%s processed %s requests' % (app.name, app.num_requests))\n\nif __name__ == '__main__':\n    main()\n\nIf you run the above, you should find that roughly half the requests go into app1.log and the rest into app2.log, and the all the requests are logged to app.log. Each webapp-specific log will contain only log entries for only that webapp, and the request information will be displayed consistently in the log (i.e. the information in each dummy request will always appear together in a log line). This is illustrated by the following shell output:\n~/logging-contextual-webapp$ python main.py\napp1 processed 51 requests\napp2 processed 49 requests\n~/logging-contextual-webapp$ wc -l *.log\n  153 app1.log\n  147 app2.log\n  300 app.log\n  600 total\n~/logging-contextual-webapp$ head -3 app1.log\nThread-3 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\nThread-3 (process_request) app1 webapplib jim    192.168.3.21 POST Hello from webapplib!\nThread-5 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\n~/logging-contextual-webapp$ head -3 app2.log\nThread-1 (process_request) app2 __main__  sheila 192.168.2.21 GET  Request processing started\nThread-1 (process_request) app2 webapplib sheila 192.168.2.21 GET  Hello from webapplib!\nThread-2 (process_request) app2 __main__  jim    192.168.2.20 GET  Request processing started\n~/logging-contextual-webapp$ head app.log\nThread-1 (process_request) app2 __main__  sheila 192.168.2.21 GET  Request processing started\nThread-1 (process_request) app2 webapplib sheila 192.168.2.21 GET  Hello from webapplib!\nThread-2 (process_request) app2 __main__  jim    192.168.2.20 GET  Request processing started\nThread-3 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\nThread-2 (process_request) app2 webapplib jim    192.168.2.20 GET  Hello from webapplib!\nThread-3 (process_request) app1 webapplib jim    192.168.3.21 POST Hello from webapplib!\nThread-4 (process_request) app2 __main__  fred   192.168.2.22 GET  Request processing started\nThread-5 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\nThread-4 (process_request) app2 webapplib fred   192.168.2.22 GET  Hello from webapplib!\nThread-6 (process_request) app1 __main__  jim    192.168.3.21 POST Request processing started\n~/logging-contextual-webapp$ grep app1 app1.log | wc -l\n153\n~/logging-contextual-webapp$ grep app2 app2.log | wc -l\n147\n~/logging-contextual-webapp$ grep app1 app.log | wc -l\n153\n~/logging-contextual-webapp$ grep app2 app.log | wc -l\n147"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#imparting-contextual-information-in-handlers",
    "href": "knowledgebase/python/python3_logging_cookbook.html#imparting-contextual-information-in-handlers",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Each Handler has its own chain of filters. If you want to add contextual information to a LogRecord without leaking it to other handlers, you can use a filter that returns a new LogRecord instead of modifying it in-place, as shown in the following script:\nimport copy\nimport logging\n\ndef filter(record: logging.LogRecord):\n    record = copy.copy(record)\n    record.user = 'jim'\n    return record\n\nif __name__ == '__main__':\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter('%(message)s from %(user)-8s')\n    handler.setFormatter(formatter)\n    handler.addFilter(filter)\n    logger.addHandler(handler)\n\n    logger.info('A log message')"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#logging-to-a-single-file-from-multiple-processes",
    "href": "knowledgebase/python/python3_logging_cookbook.html#logging-to-a-single-file-from-multiple-processes",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Although logging is thread-safe, and logging to a single file from multiple threads in a single process is supported, logging to a single file from multiple processes is not supported, because there is no standard way to serialize access to a single file across multiple processes in Python. If you need to log to a single file from multiple processes, one way of doing this is to have all the processes log to a SocketHandler, and have a separate process which implements a socket server which reads from the socket and logs to file. (If you prefer, you can dedicate one thread in one of the existing processes to perform this function.) This section documents this approach in more detail and includes a working socket receiver which can be used as a starting point for you to adapt in your own applications.\nYou could also write your own handler which uses the Lock class from the multiprocessing module to serialize access to the file from your processes. The stdlib FileHandler and subclasses do not make use of multiprocessing.\nAlternatively, you can use a Queue and a QueueHandler to send all logging events to one of the processes in your multi-process application. The following example script demonstrates how you can do this; in the example a separate listener process listens for events sent by other processes and logs them according to its own logging configuration. Although the example only demonstrates one way of doing it (for example, you may want to use a listener thread rather than a separate listener process – the implementation would be analogous) it does allow for completely different logging configurations for the listener and the other processes in your application, and can be used as the basis for code meeting your own specific requirements:\n# You'll need these imports in your own code\nimport logging\nimport logging.handlers\nimport multiprocessing\n\n# Next two import lines for this demo only\nfrom random import choice, random\nimport time\n\n#\n# Because you'll want to define the logging configurations for listener and workers, the\n# listener and worker process functions take a configurer parameter which is a callable\n# for configuring logging for that process. These functions are also passed the queue,\n# which they use for communication.\n#\n# In practice, you can configure the listener however you want, but note that in this\n# simple example, the listener does not apply level or filter logic to received records.\n# In practice, you would probably want to do this logic in the worker processes, to avoid\n# sending events which would be filtered out between processes.\n#\n# The size of the rotated files is made small so you can see the results easily.\ndef listener_configurer():\n    root = logging.getLogger()\n    h = logging.handlers.RotatingFileHandler('mptest.log', 'a', 300, 10)\n    f = logging.Formatter('%(asctime)s %(processName)-10s %(name)s %(levelname)-8s %(message)s')\n    h.setFormatter(f)\n    root.addHandler(h)\n\n# This is the listener process top-level loop: wait for logging events\n# (LogRecords)on the queue and handle them, quit when you get a None for a\n# LogRecord.\ndef listener_process(queue, configurer):\n    configurer()\n    while True:\n        try:\n            record = queue.get()\n            if record is None:  # We send this as a sentinel to tell the listener to quit.\n                break\n            logger = logging.getLogger(record.name)\n            logger.handle(record)  # No level or filter logic applied - just do it!\n        except Exception:\n            import sys, traceback\n            print('Whoops! Problem:', file=sys.stderr)\n            traceback.print_exc(file=sys.stderr)\n\n# Arrays used for random selections in this demo\n\nLEVELS = [logging.DEBUG, logging.INFO, logging.WARNING,\n          logging.ERROR, logging.CRITICAL]\n\nLOGGERS = ['a.b.c', 'd.e.f']\n\nMESSAGES = [\n    'Random message #1',\n    'Random message #2',\n    'Random message #3',\n]\n\n# The worker configuration is done at the start of the worker process run.\n# Note that on Windows you can't rely on fork semantics, so each process\n# will run the logging configuration code when it starts.\ndef worker_configurer(queue):\n    h = logging.handlers.QueueHandler(queue)  # Just the one handler needed\n    root = logging.getLogger()\n    root.addHandler(h)\n    # send all messages, for demo; no other level or filter logic applied.\n    root.setLevel(logging.DEBUG)\n\n# This is the worker process top-level loop, which just logs ten events with\n# random intervening delays before terminating.\n# The print messages are just so you know it's doing something!\ndef worker_process(queue, configurer):\n    configurer(queue)\n    name = multiprocessing.current_process().name\n    print('Worker started: %s' % name)\n    for i in range(10):\n        time.sleep(random())\n        logger = logging.getLogger(choice(LOGGERS))\n        level = choice(LEVELS)\n        message = choice(MESSAGES)\n        logger.log(level, message)\n    print('Worker finished: %s' % name)\n\n# Here's where the demo gets orchestrated. Create the queue, create and start\n# the listener, create ten workers and start them, wait for them to finish,\n# then send a None to the queue to tell the listener to finish.\ndef main():\n    queue = multiprocessing.Queue(-1)\n    listener = multiprocessing.Process(target=listener_process,\n                                       args=(queue, listener_configurer))\n    listener.start()\n    workers = []\n    for i in range(10):\n        worker = multiprocessing.Process(target=worker_process,\n                                         args=(queue, worker_configurer))\n        workers.append(worker)\n        worker.start()\n    for w in workers:\n        w.join()\n    queue.put_nowait(None)\n    listener.join()\n\nif __name__ == '__main__':\n    main()\n\nA variant of the above script keeps the logging in the main process, in a separate thread:\nimport logging\nimport logging.config\nimport logging.handlers\nfrom multiprocessing import Process, Queue\nimport random\nimport threading\nimport time\n\ndef logger_thread(q):\n    while True:\n        record = q.get()\n        if record is None:\n            break\n        logger = logging.getLogger(record.name)\n        logger.handle(record)\n\n\ndef worker_process(q):\n    qh = logging.handlers.QueueHandler(q)\n    root = logging.getLogger()\n    root.setLevel(logging.DEBUG)\n    root.addHandler(qh)\n    levels = [logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n              logging.CRITICAL]\n    loggers = ['foo', 'foo.bar', 'foo.bar.baz',\n               'spam', 'spam.ham', 'spam.ham.eggs']\n    for i in range(100):\n        lvl = random.choice(levels)\n        logger = logging.getLogger(random.choice(loggers))\n        logger.log(lvl, 'Message no. %d', i)\n\nif __name__ == '__main__':\n    q = Queue()\n    d = {\n        'version': 1,\n        'formatters': {\n            'detailed': {\n                'class': 'logging.Formatter',\n                'format': '%(asctime)s %(name)-15s %(levelname)-8s %(processName)-10s %(message)s'\n            }\n        },\n        'handlers': {\n            'console': {\n                'class': 'logging.StreamHandler',\n                'level': 'INFO',\n            },\n            'file': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog.log',\n                'mode': 'w',\n                'formatter': 'detailed',\n            },\n            'foofile': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-foo.log',\n                'mode': 'w',\n                'formatter': 'detailed',\n            },\n            'errors': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-errors.log',\n                'mode': 'w',\n                'level': 'ERROR',\n                'formatter': 'detailed',\n            },\n        },\n        'loggers': {\n            'foo': {\n                'handlers': ['foofile']\n            }\n        },\n        'root': {\n            'level': 'DEBUG',\n            'handlers': ['console', 'file', 'errors']\n        },\n    }\n    workers = []\n    for i in range(5):\n        wp = Process(target=worker_process, name='worker %d' % (i + 1), args=(q,))\n        workers.append(wp)\n        wp.start()\n    logging.config.dictConfig(d)\n    lp = threading.Thread(target=logger_thread, args=(q,))\n    lp.start()\n    # At this point, the main process could do some useful work of its own\n    # Once it's done that, it can wait for the workers to terminate...\n    for wp in workers:\n        wp.join()\n    # And now tell the logging thread to finish up, too\n    q.put(None)\n    lp.join()\n\nThis variant shows how you can e.g. apply configuration for particular loggers - e.g. the foo logger has a special handler which stores all events in the foo subsystem in a file mplog-foo.log. This will be used by the logging machinery in the main process (even though the logging events are generated in the worker processes) to direct the messages to the appropriate destinations.\n\n\nIf you want to use concurrent.futures.ProcessPoolExecutor to start your worker processes, you need to create the queue slightly differently. Instead of\nqueue = multiprocessing.Queue(-1)\n\nyou should use\nqueue = multiprocessing.Manager().Queue(-1)  # also works with the examples above\n\nand you can then replace the worker creation from this:\nworkers = []\nfor i in range(10):\n    worker = multiprocessing.Process(target=worker_process,\n                                     args=(queue, worker_configurer))\n    workers.append(worker)\n    worker.start()\nfor w in workers:\n    w.join()\n\nto this (remembering to first import concurrent.futures):\nwith concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:\n    for i in range(10):\n        executor.submit(worker_process, queue, worker_configurer)\n\n\n\n\nWhen deploying Web applications using Gunicorn or uWSGI (or similar), multiple worker processes are created to handle client requests. In such environments, avoid creating file-based handlers directly in your web application. Instead, use a SocketHandler to log from the web application to a listener in a separate process. This can be set up using a process management tool such as Supervisor - see Running a logging socket listener in production for more details."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#using-file-rotation",
    "href": "knowledgebase/python/python3_logging_cookbook.html#using-file-rotation",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Sometimes you want to let a log file grow to a certain size, then open a new file and log to that. You may want to keep a certain number of these files, and when that many files have been created, rotate the files so that the number of files and the size of the files both remain bounded. For this usage pattern, the logging package provides a RotatingFileHandler:\nimport glob\nimport logging\nimport logging.handlers\n\nLOG_FILENAME = 'logging_rotatingfile_example.out'\n\n# Set up a specific logger with our desired output level\nmy_logger = logging.getLogger('MyLogger')\nmy_logger.setLevel(logging.DEBUG)\n\n# Add the log message handler to the logger\nhandler = logging.handlers.RotatingFileHandler(\n              LOG_FILENAME, maxBytes=20, backupCount=5)\n\nmy_logger.addHandler(handler)\n\n# Log some messages\nfor i in range(20):\n    my_logger.debug('i = %d' % i)\n\n# See what files are created\nlogfiles = glob.glob('%s*' % LOG_FILENAME)\n\nfor filename in logfiles:\n    print(filename)\n\nThe result should be 6 separate files, each with part of the log history for the application:\nlogging_rotatingfile_example.out\nlogging_rotatingfile_example.out.1\nlogging_rotatingfile_example.out.2\nlogging_rotatingfile_example.out.3\nlogging_rotatingfile_example.out.4\nlogging_rotatingfile_example.out.5\n\nThe most current file is always logging_rotatingfile_example.out, and each time it reaches the size limit it is renamed with the suffix .1. Each of the existing backup files is renamed to increment the suffix (.1 becomes .2, etc.) and the .6 file is erased.\nObviously this example sets the log length much too small as an extreme example. You would want to set maxBytes to an appropriate value."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#use-of-alternative-formatting-styles",
    "href": "knowledgebase/python/python3_logging_cookbook.html#use-of-alternative-formatting-styles",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "When logging was added to the Python standard library, the only way of formatting messages with variable content was to use the %-formatting method. Since then, Python has gained two new formatting approaches: string.Template (added in Python 2.4) and str.format() (added in Python 2.6).\nLogging (as of 3.2) provides improved support for these two additional formatting styles. The Formatter class been enhanced to take an additional, optional keyword parameter named style. This defaults to '%', but other possible values are '{' and '$', which correspond to the other two formatting styles. Backwards compatibility is maintained by default (as you would expect), but by explicitly specifying a style parameter, you get the ability to specify format strings which work with str.format() or string.Template. Here’s an example console session to show the possibilities:\n&gt;&gt;&gt;\n&gt;&gt;&gt; import logging\n&gt;&gt;&gt; root = logging.getLogger()\n&gt;&gt;&gt; root.setLevel(logging.DEBUG)\n&gt;&gt;&gt; handler = logging.StreamHandler()\n&gt;&gt;&gt; bf = logging.Formatter('{asctime} {name} {levelname:8s} {message}',\n...                        style='{')\n&gt;&gt;&gt; handler.setFormatter(bf)\n&gt;&gt;&gt; root.addHandler(handler)\n&gt;&gt;&gt; logger = logging.getLogger('foo.bar')\n&gt;&gt;&gt; logger.debug('This is a DEBUG message')\n2010-10-28 15:11:55,341 foo.bar DEBUG    This is a DEBUG message\n&gt;&gt;&gt; logger.critical('This is a CRITICAL message')\n2010-10-28 15:12:11,526 foo.bar CRITICAL This is a CRITICAL message\n&gt;&gt;&gt; df = logging.Formatter('$asctime $name ${levelname} $message',\n...                        style='$')\n&gt;&gt;&gt; handler.setFormatter(df)\n&gt;&gt;&gt; logger.debug('This is a DEBUG message')\n2010-10-28 15:13:06,924 foo.bar DEBUG This is a DEBUG message\n&gt;&gt;&gt; logger.critical('This is a CRITICAL message')\n2010-10-28 15:13:11,494 foo.bar CRITICAL This is a CRITICAL message\n&gt;&gt;&gt;\n\nNote that the formatting of logging messages for final output to logs is completely independent of how an individual logging message is constructed. That can still use %-formatting, as shown here:\n&gt;&gt;&gt;\n&gt;&gt;&gt; logger.error('This is an%s %s %s', 'other,', 'ERROR,', 'message')\n2010-10-28 15:19:29,833 foo.bar ERROR This is another, ERROR, message\n&gt;&gt;&gt;\n\nLogging calls (logger.debug(), logger.info() etc.) only take positional parameters for the actual logging message itself, with keyword parameters used only for determining options for how to handle the actual logging call (e.g. the exc_info keyword parameter to indicate that traceback information should be logged, or the extra keyword parameter to indicate additional contextual information to be added to the log). So you cannot directly make logging calls using str.format() or string.Template syntax, because internally the logging package uses %-formatting to merge the format string and the variable arguments. There would be no changing this while preserving backward compatibility, since all logging calls which are out there in existing code will be using %-format strings.\nThere is, however, a way that you can use {}- and $- formatting to construct your individual log messages. Recall that for a message you can use an arbitrary object as a message format string, and that the logging package will call str() on that object to get the actual format string. Consider the following two classes:\nclass BraceMessage:\n    def __init__(self, fmt, /, *args, **kwargs):\n        self.fmt = fmt\n        self.args = args\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return self.fmt.format(*self.args, **self.kwargs)\n\nclass DollarMessage:\n    def __init__(self, fmt, /, **kwargs):\n        self.fmt = fmt\n        self.kwargs = kwargs\n\n    def __str__(self):\n        from string import Template\n        return Template(self.fmt).substitute(**self.kwargs)\n\nEither of these can be used in place of a format string, to allow {}- or \\(-formatting to be used to build the actual “message” part which appears in the formatted log output in place of “%(message)s” or “{message}” or “\\)message”. It’s a little unwieldy to use the class names whenever you want to log something, but it’s quite palatable if you use an alias such as __ (double underscore — not to be confused with _, the single underscore used as a synonym/alias for gettext.gettext() or its brethren).\nThe above classes are not included in Python, though they’re easy enough to copy and paste into your own code. They can be used as follows (assuming that they’re declared in a module called wherever):\n&gt;&gt;&gt;\n&gt;&gt;&gt; from wherever import BraceMessage as __\n&gt;&gt;&gt; print(__('Message with {0} {name}', 2, name='placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt; class Point: pass\n...\n&gt;&gt;&gt; p = Point()\n&gt;&gt;&gt; p.x = 0.5\n&gt;&gt;&gt; p.y = 0.5\n&gt;&gt;&gt; print(__('Message with coordinates: ({point.x:.2f}, {point.y:.2f})',\n...       point=p))\nMessage with coordinates: (0.50, 0.50)\n&gt;&gt;&gt; from wherever import DollarMessage as __\n&gt;&gt;&gt; print(__('Message with $num $what', num=2, what='placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt;\n\nWhile the above examples use print() to show how the formatting works, you would of course use logger.debug() or similar to actually log using this approach.\nOne thing to note is that you pay no significant performance penalty with this approach: the actual formatting happens not when you make the logging call, but when (and if) the logged message is actually about to be output to a log by a handler. So the only slightly unusual thing which might trip you up is that the parentheses go around the format string and the arguments, not just the format string. That’s because the __ notation is just syntax sugar for a constructor call to one of the _XXX_Message classes.\nIf you prefer, you can use a LoggerAdapter to achieve a similar effect to the above, as in the following example:\nimport logging\n\nclass Message:\n    def __init__(self, fmt, args):\n        self.fmt = fmt\n        self.args = args\n\n    def __str__(self):\n        return self.fmt.format(*self.args)\n\nclass StyleAdapter(logging.LoggerAdapter):\n    def log(self, level, msg, /, *args, stacklevel=1, **kwargs):\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, Message(msg, args), **kwargs,\n                            stacklevel=stacklevel+1)\n\nlogger = StyleAdapter(logging.getLogger(__name__))\n\ndef main():\n    logger.debug('Hello, {}', 'world!')\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.DEBUG)\n    main()\n\nThe above script should log the message Hello, world! when run with Python 3.8 or later."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#customizing-logrecord",
    "href": "knowledgebase/python/python3_logging_cookbook.html#customizing-logrecord",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Every logging event is represented by a LogRecord instance. When an event is logged and not filtered out by a logger’s level, a LogRecord is created, populated with information about the event and then passed to the handlers for that logger (and its ancestors, up to and including the logger where further propagation up the hierarchy is disabled). Before Python 3.2, there were only two places where this creation was done:\n\nLogger.makeRecord(), which is called in the normal process of logging an event. This invoked LogRecord directly to create an instance.\nmakeLogRecord(), which is called with a dictionary containing attributes to be added to the LogRecord. This is typically invoked when a suitable dictionary has been received over the network (e.g. in pickle form via a SocketHandler, or in JSON form via an HTTPHandler).\n\nThis has usually meant that if you need to do anything special with a LogRecord, you’ve had to do one of the following.\n\nCreate your own Logger subclass, which overrides Logger.makeRecord(), and set it using setLoggerClass() before any loggers that you care about are instantiated.\nAdd a Filter to a logger or handler, which does the necessary special manipulation you need when its filter() method is called.\n\nThe first approach would be a little unwieldy in the scenario where (say) several different libraries wanted to do different things. Each would attempt to set its own Logger subclass, and the one which did this last would win.\nThe second approach works reasonably well for many cases, but does not allow you to e.g. use a specialized subclass of LogRecord. Library developers can set a suitable filter on their loggers, but they would have to remember to do this every time they introduced a new logger (which they would do simply by adding new packages or modules and doing\nlogger = logging.getLogger(__name__)\n\nat module level). It’s probably one too many things to think about. Developers could also add the filter to a NullHandler attached to their top-level logger, but this would not be invoked if an application developer attached a handler to a lower-level library logger — so output from that handler would not reflect the intentions of the library developer.\nIn Python 3.2 and later, LogRecord creation is done through a factory, which you can specify. The factory is just a callable you can set with setLogRecordFactory(), and interrogate with getLogRecordFactory(). The factory is invoked with the same signature as the LogRecord constructor, as LogRecord is the default setting for the factory.\nThis approach allows a custom factory to control all aspects of LogRecord creation. For example, you could return a subclass, or just add some additional attributes to the record once created, using a pattern similar to this:\nold_factory = logging.getLogRecordFactory()\n\ndef record_factory(*args, **kwargs):\n    record = old_factory(*args, **kwargs)\n    record.custom_attribute = 0xdecafbad\n    return record\n\nlogging.setLogRecordFactory(record_factory)\n\nThis pattern allows different libraries to chain factories together, and as long as they don’t overwrite each other’s attributes or unintentionally overwrite the attributes provided as standard, there should be no surprises. However, it should be borne in mind that each link in the chain adds run-time overhead to all logging operations, and the technique should only be used when the use of a Filter does not provide the desired result."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#subclassing-queuehandler-and-queuelistener--a-zeromq-example",
    "href": "knowledgebase/python/python3_logging_cookbook.html#subclassing-queuehandler-and-queuelistener--a-zeromq-example",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "You can use a QueueHandler subclass to send messages to other kinds of queues, for example a ZeroMQ ‘publish’ socket. In the example below,the socket is created separately and passed to the handler (as its ‘queue’):\nimport zmq   # using pyzmq, the Python binding for ZeroMQ\nimport json  # for serializing records portably\n\nctx = zmq.Context()\nsock = zmq.Socket(ctx, zmq.PUB)  # or zmq.PUSH, or other suitable value\nsock.bind('tcp://*:5556')        # or wherever\n\nclass ZeroMQSocketHandler(QueueHandler):\n    def enqueue(self, record):\n        self.queue.send_json(record.__dict__)\n\n\nhandler = ZeroMQSocketHandler(sock)\n\nOf course there are other ways of organizing this, for example passing in the data needed by the handler to create the socket:\nclass ZeroMQSocketHandler(QueueHandler):\n    def __init__(self, uri, socktype=zmq.PUB, ctx=None):\n        self.ctx = ctx or zmq.Context()\n        socket = zmq.Socket(self.ctx, socktype)\n        socket.bind(uri)\n        super().__init__(socket)\n\n    def enqueue(self, record):\n        self.queue.send_json(record.__dict__)\n\n    def close(self):\n        self.queue.close()\n\n\n\n\nYou can also subclass QueueListener to get messages from other kinds of queues, for example a ZeroMQ ‘subscribe’ socket. Here’s an example:\nclass ZeroMQSocketListener(QueueListener):\n    def __init__(self, uri, /, *handlers, **kwargs):\n        self.ctx = kwargs.get('ctx') or zmq.Context()\n        socket = zmq.Socket(self.ctx, zmq.SUB)\n        socket.setsockopt_string(zmq.SUBSCRIBE, '')  # subscribe to everything\n        socket.connect(uri)\n        super().__init__(socket, *handlers, **kwargs)\n\n    def dequeue(self):\n        msg = self.queue.recv_json()\n        return logging.makeLogRecord(msg)"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#subclassing-queuehandler-and-queuelistener--a-pynng-example",
    "href": "knowledgebase/python/python3_logging_cookbook.html#subclassing-queuehandler-and-queuelistener--a-pynng-example",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "In a similar way to the above section, we can implement a listener and handler using pynng, which is a Python binding to NNG, billed as a spiritual successor to ZeroMQ. The following snippets illustrate – you can test them in an environment which has pynng installed. Just for variety, we present the listener first.\n\n\n# listener.py\nimport json\nimport logging\nimport logging.handlers\n\nimport pynng\n\nDEFAULT_ADDR = \"tcp://localhost:13232\"\n\ninterrupted = False\n\nclass NNGSocketListener(logging.handlers.QueueListener):\n\n    def __init__(self, uri, /, *handlers, **kwargs):\n        # Have a timeout for interruptability, and open a\n        # subscriber socket\n        socket = pynng.Sub0(listen=uri, recv_timeout=500)\n        # The b'' subscription matches all topics\n        topics = kwargs.pop('topics', None) or b''\n        socket.subscribe(topics)\n        # We treat the socket as a queue\n        super().__init__(socket, *handlers, **kwargs)\n\n    def dequeue(self, block):\n        data = None\n        # Keep looping while not interrupted and no data received over the\n        # socket\n        while not interrupted:\n            try:\n                data = self.queue.recv(block=block)\n                break\n            except pynng.Timeout:\n                pass\n            except pynng.Closed:  # sometimes happens when you hit Ctrl-C\n                break\n        if data is None:\n            return None\n        # Get the logging event sent from a publisher\n        event = json.loads(data.decode('utf-8'))\n        return logging.makeLogRecord(event)\n\n    def enqueue_sentinel(self):\n        # Not used in this implementation, as the socket isn't really a\n        # queue\n        pass\n\nlogging.getLogger('pynng').propagate = False\nlistener = NNGSocketListener(DEFAULT_ADDR, logging.StreamHandler(), topics=b'')\nlistener.start()\nprint('Press Ctrl-C to stop.')\ntry:\n    while True:\n        pass\nexcept KeyboardInterrupt:\n    interrupted = True\nfinally:\n    listener.stop()\n\n\n\n\n# sender.py\nimport json\nimport logging\nimport logging.handlers\nimport time\nimport random\n\nimport pynng\n\nDEFAULT_ADDR = \"tcp://localhost:13232\"\n\nclass NNGSocketHandler(logging.handlers.QueueHandler):\n\n    def __init__(self, uri):\n        socket = pynng.Pub0(dial=uri, send_timeout=500)\n        super().__init__(socket)\n\n    def enqueue(self, record):\n        # Send the record as UTF-8 encoded JSON\n        d = dict(record.__dict__)\n        data = json.dumps(d)\n        self.queue.send(data.encode('utf-8'))\n\n    def close(self):\n        self.queue.close()\n\nlogging.getLogger('pynng').propagate = False\nhandler = NNGSocketHandler(DEFAULT_ADDR)\n# Make sure the process ID is in the output\nlogging.basicConfig(level=logging.DEBUG,\n                    handlers=[logging.StreamHandler(), handler],\n                    format='%(levelname)-8s %(name)10s %(process)6s %(message)s')\nlevels = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n          logging.CRITICAL)\nlogger_names = ('myapp', 'myapp.lib1', 'myapp.lib2')\nmsgno = 1\nwhile True:\n    # Just randomly select some loggers and levels and log away\n    level = random.choice(levels)\n    logger = logging.getLogger(random.choice(logger_names))\n    logger.log(level, 'Message no. %5d' % msgno)\n    msgno += 1\n    delay = random.random() * 2 + 0.5\n    time.sleep(delay)\n\nYou can run the above two snippets in separate command shells. If we run the listener in one shell and run the sender in two separate shells, we should see something like the following. In the first sender shell:\n$ python sender.py\nDEBUG         myapp    613 Message no.     1\nWARNING  myapp.lib2    613 Message no.     2\nCRITICAL myapp.lib2    613 Message no.     3\nWARNING  myapp.lib2    613 Message no.     4\nCRITICAL myapp.lib1    613 Message no.     5\nDEBUG         myapp    613 Message no.     6\nCRITICAL myapp.lib1    613 Message no.     7\nINFO     myapp.lib1    613 Message no.     8\n(and so on)\n\nIn the second sender shell:\n$ python sender.py\nINFO     myapp.lib2    657 Message no.     1\nCRITICAL myapp.lib2    657 Message no.     2\nCRITICAL      myapp    657 Message no.     3\nCRITICAL myapp.lib1    657 Message no.     4\nINFO     myapp.lib1    657 Message no.     5\nWARNING  myapp.lib2    657 Message no.     6\nCRITICAL      myapp    657 Message no.     7\nDEBUG    myapp.lib1    657 Message no.     8\n(and so on)\n\nIn the listener shell:\n$ python listener.py\nPress Ctrl-C to stop.\nDEBUG         myapp    613 Message no.     1\nWARNING  myapp.lib2    613 Message no.     2\nINFO     myapp.lib2    657 Message no.     1\nCRITICAL myapp.lib2    613 Message no.     3\nCRITICAL myapp.lib2    657 Message no.     2\nCRITICAL      myapp    657 Message no.     3\nWARNING  myapp.lib2    613 Message no.     4\nCRITICAL myapp.lib1    613 Message no.     5\nCRITICAL myapp.lib1    657 Message no.     4\nINFO     myapp.lib1    657 Message no.     5\nDEBUG         myapp    613 Message no.     6\nWARNING  myapp.lib2    657 Message no.     6\nCRITICAL      myapp    657 Message no.     7\nCRITICAL myapp.lib1    613 Message no.     7\nINFO     myapp.lib1    613 Message no.     8\nDEBUG    myapp.lib1    657 Message no.     8\n(and so on)\n\nAs you can see, the logging from the two sender processes is interleaved in the listener’s output."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#an-example-dictionary-based-configuration",
    "href": "knowledgebase/python/python3_logging_cookbook.html#an-example-dictionary-based-configuration",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Below is an example of a logging configuration dictionary - it’s taken from the documentation on the Django project. This dictionary is passed to dictConfig() to put the configuration into effect:\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'verbose': {\n            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',\n            'style': '{',\n        },\n        'simple': {\n            'format': '{levelname} {message}',\n            'style': '{',\n        },\n    },\n    'filters': {\n        'special': {\n            '()': 'project.logging.SpecialFilter',\n            'foo': 'bar',\n        },\n    },\n    'handlers': {\n        'console': {\n            'level': 'INFO',\n            'class': 'logging.StreamHandler',\n            'formatter': 'simple',\n        },\n        'mail_admins': {\n            'level': 'ERROR',\n            'class': 'django.utils.log.AdminEmailHandler',\n            'filters': ['special']\n        }\n    },\n    'loggers': {\n        'django': {\n            'handlers': ['console'],\n            'propagate': True,\n        },\n        'django.request': {\n            'handlers': ['mail_admins'],\n            'level': 'ERROR',\n            'propagate': False,\n        },\n        'myproject.custom': {\n            'handlers': ['console', 'mail_admins'],\n            'level': 'INFO',\n            'filters': ['special']\n        }\n    }\n}\n\nFor more information about this configuration, you can see the relevant section of the Django documentation."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#using-a-rotator-and-namer-to-customize-log-rotation-processing",
    "href": "knowledgebase/python/python3_logging_cookbook.html#using-a-rotator-and-namer-to-customize-log-rotation-processing",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "An example of how you can define a namer and rotator is given in the following runnable script, which shows gzip compression of the log file:\nimport gzip\nimport logging\nimport logging.handlers\nimport os\nimport shutil\n\ndef namer(name):\n    return name + \".gz\"\n\ndef rotator(source, dest):\n    with open(source, 'rb') as f_in:\n        with gzip.open(dest, 'wb') as f_out:\n            shutil.copyfileobj(f_in, f_out)\n    os.remove(source)\n\n\nrh = logging.handlers.RotatingFileHandler('rotated.log', maxBytes=128, backupCount=5)\nrh.rotator = rotator\nrh.namer = namer\n\nroot = logging.getLogger()\nroot.setLevel(logging.INFO)\nroot.addHandler(rh)\nf = logging.Formatter('%(asctime)s %(message)s')\nrh.setFormatter(f)\nfor i in range(1000):\n    root.info(f'Message no. {i + 1}')\n\nAfter running this, you will see six new files, five of which are compressed:\n$ ls rotated.log*\nrotated.log       rotated.log.2.gz  rotated.log.4.gz\nrotated.log.1.gz  rotated.log.3.gz  rotated.log.5.gz\n$ zcat rotated.log.1.gz\n2023-01-20 02:28:17,767 Message no. 996\n2023-01-20 02:28:17,767 Message no. 997\n2023-01-20 02:28:17,767 Message no. 998"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#a-more-elaborate-multiprocessing-example",
    "href": "knowledgebase/python/python3_logging_cookbook.html#a-more-elaborate-multiprocessing-example",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "The following working example shows how logging can be used with multiprocessing using configuration files. The configurations are fairly simple, but serve to illustrate how more complex ones could be implemented in a real multiprocessing scenario.\nIn the example, the main process spawns a listener process and some worker processes. Each of the main process, the listener and the workers have three separate configurations (the workers all share the same configuration). We can see logging in the main process, how the workers log to a QueueHandler and how the listener implements a QueueListener and a more complex logging configuration, and arranges to dispatch events received via the queue to the handlers specified in the configuration. Note that these configurations are purely illustrative, but you should be able to adapt this example to your own scenario.\nHere’s the script - the docstrings and the comments hopefully explain how it works:\nimport logging\nimport logging.config\nimport logging.handlers\nfrom multiprocessing import Process, Queue, Event, current_process\nimport os\nimport random\nimport time\n\nclass MyHandler:\n    \"\"\"\n    A simple handler for logging events. It runs in the listener process and\n    dispatches events to loggers based on the name in the received record,\n    which then get dispatched, by the logging system, to the handlers\n    configured for those loggers.\n    \"\"\"\n\n    def handle(self, record):\n        if record.name == \"root\":\n            logger = logging.getLogger()\n        else:\n            logger = logging.getLogger(record.name)\n\n        if logger.isEnabledFor(record.levelno):\n            # The process name is transformed just to show that it's the listener\n            # doing the logging to files and console\n            record.processName = '%s (for %s)' % (current_process().name, record.processName)\n            logger.handle(record)\n\ndef listener_process(q, stop_event, config):\n    \"\"\"\n    This could be done in the main process, but is just done in a separate\n    process for illustrative purposes.\n\n    This initialises logging according to the specified configuration,\n    starts the listener and waits for the main process to signal completion\n    via the event. The listener is then stopped, and the process exits.\n    \"\"\"\n    logging.config.dictConfig(config)\n    listener = logging.handlers.QueueListener(q, MyHandler())\n    listener.start()\n    if os.name == 'posix':\n        # On POSIX, the setup logger will have been configured in the\n        # parent process, but should have been disabled following the\n        # dictConfig call.\n        # On Windows, since fork isn't used, the setup logger won't\n        # exist in the child, so it would be created and the message\n        # would appear - hence the \"if posix\" clause.\n        logger = logging.getLogger('setup')\n        logger.critical('Should not appear, because of disabled logger ...')\n    stop_event.wait()\n    listener.stop()\n\ndef worker_process(config):\n    \"\"\"\n    A number of these are spawned for the purpose of illustration. In\n    practice, they could be a heterogeneous bunch of processes rather than\n    ones which are identical to each other.\n\n    This initialises logging according to the specified configuration,\n    and logs a hundred messages with random levels to randomly selected\n    loggers.\n\n    A small sleep is added to allow other processes a chance to run. This\n    is not strictly needed, but it mixes the output from the different\n    processes a bit more than if it's left out.\n    \"\"\"\n    logging.config.dictConfig(config)\n    levels = [logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n              logging.CRITICAL]\n    loggers = ['foo', 'foo.bar', 'foo.bar.baz',\n               'spam', 'spam.ham', 'spam.ham.eggs']\n    if os.name == 'posix':\n        # On POSIX, the setup logger will have been configured in the\n        # parent process, but should have been disabled following the\n        # dictConfig call.\n        # On Windows, since fork isn't used, the setup logger won't\n        # exist in the child, so it would be created and the message\n        # would appear - hence the \"if posix\" clause.\n        logger = logging.getLogger('setup')\n        logger.critical('Should not appear, because of disabled logger ...')\n    for i in range(100):\n        lvl = random.choice(levels)\n        logger = logging.getLogger(random.choice(loggers))\n        logger.log(lvl, 'Message no. %d', i)\n        time.sleep(0.01)\n\ndef main():\n    q = Queue()\n    # The main process gets a simple configuration which prints to the console.\n    config_initial = {\n        'version': 1,\n        'handlers': {\n            'console': {\n                'class': 'logging.StreamHandler',\n                'level': 'INFO'\n            }\n        },\n        'root': {\n            'handlers': ['console'],\n            'level': 'DEBUG'\n        }\n    }\n    # The worker process configuration is just a QueueHandler attached to the\n    # root logger, which allows all messages to be sent to the queue.\n    # We disable existing loggers to disable the \"setup\" logger used in the\n    # parent process. This is needed on POSIX because the logger will\n    # be there in the child following a fork().\n    config_worker = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'handlers': {\n            'queue': {\n                'class': 'logging.handlers.QueueHandler',\n                'queue': q\n            }\n        },\n        'root': {\n            'handlers': ['queue'],\n            'level': 'DEBUG'\n        }\n    }\n    # The listener process configuration shows that the full flexibility of\n    # logging configuration is available to dispatch events to handlers however\n    # you want.\n    # We disable existing loggers to disable the \"setup\" logger used in the\n    # parent process. This is needed on POSIX because the logger will\n    # be there in the child following a fork().\n    config_listener = {\n        'version': 1,\n        'disable_existing_loggers': True,\n        'formatters': {\n            'detailed': {\n                'class': 'logging.Formatter',\n                'format': '%(asctime)s %(name)-15s %(levelname)-8s %(processName)-10s %(message)s'\n            },\n            'simple': {\n                'class': 'logging.Formatter',\n                'format': '%(name)-15s %(levelname)-8s %(processName)-10s %(message)s'\n            }\n        },\n        'handlers': {\n            'console': {\n                'class': 'logging.StreamHandler',\n                'formatter': 'simple',\n                'level': 'INFO'\n            },\n            'file': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog.log',\n                'mode': 'w',\n                'formatter': 'detailed'\n            },\n            'foofile': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-foo.log',\n                'mode': 'w',\n                'formatter': 'detailed'\n            },\n            'errors': {\n                'class': 'logging.FileHandler',\n                'filename': 'mplog-errors.log',\n                'mode': 'w',\n                'formatter': 'detailed',\n                'level': 'ERROR'\n            }\n        },\n        'loggers': {\n            'foo': {\n                'handlers': ['foofile']\n            }\n        },\n        'root': {\n            'handlers': ['console', 'file', 'errors'],\n            'level': 'DEBUG'\n        }\n    }\n    # Log some initial events, just to show that logging in the parent works\n    # normally.\n    logging.config.dictConfig(config_initial)\n    logger = logging.getLogger('setup')\n    logger.info('About to create workers ...')\n    workers = []\n    for i in range(5):\n        wp = Process(target=worker_process, name='worker %d' % (i + 1),\n                     args=(config_worker,))\n        workers.append(wp)\n        wp.start()\n        logger.info('Started worker: %s', wp.name)\n    logger.info('About to create listener ...')\n    stop_event = Event()\n    lp = Process(target=listener_process, name='listener',\n                 args=(q, stop_event, config_listener))\n    lp.start()\n    logger.info('Started listener')\n    # We now hang around for the workers to finish their work.\n    for wp in workers:\n        wp.join()\n    # Workers all done, listening can now stop.\n    # Logging in the parent still works normally.\n    logger.info('Telling listener to stop ...')\n    stop_event.set()\n    lp.join()\n    logger.info('All done.')\n\nif __name__ == '__main__':\n    main()"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#inserting-a-bom-into-messages-sent-to-a-sysloghandler",
    "href": "knowledgebase/python/python3_logging_cookbook.html#inserting-a-bom-into-messages-sent-to-a-sysloghandler",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "RFC 5424 requires that a Unicode message be sent to a syslog daemon as a set of bytes which have the following structure: an optional pure-ASCII component, followed by a UTF-8 Byte Order Mark (BOM), followed by Unicode encoded using UTF-8. (See the relevant section of the specification.)\nIn Python 3.1, code was added to SysLogHandler to insert a BOM into the message, but unfortunately, it was implemented incorrectly, with the BOM appearing at the beginning of the message and hence not allowing any pure-ASCII component to appear before it.\nAs this behaviour is broken, the incorrect BOM insertion code is being removed from Python 3.2.4 and later. However, it is not being replaced, and if you want to produce RFC 5424-compliant messages which include a BOM, an optional pure-ASCII sequence before it and arbitrary Unicode after it, encoded using UTF-8, then you need to do the following:\n\nAttach a Formatter instance to your SysLogHandler instance, with a format string such as:\n'ASCII section\\ufeffUnicode section'\n\nThe Unicode code point U+FEFF, when encoded using UTF-8, will be encoded as a UTF-8 BOM – the byte-string b'\\xef\\xbb\\xbf'.\nReplace the ASCII section with whatever placeholders you like, but make sure that the data that appears in there after substitution is always ASCII (that way, it will remain unchanged after UTF-8 encoding).\nReplace the Unicode section with whatever placeholders you like; if the data which appears there after substitution contains characters outside the ASCII range, that’s fine – it will be encoded using UTF-8.\n\nThe formatted message will be encoded using UTF-8 encoding by SysLogHandler. If you follow the above rules, you should be able to produce RFC 5424-compliant messages. If you don’t, logging may not complain, but your messages will not be RFC 5424-compliant, and your syslog daemon may complain."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#implementing-structured-logging",
    "href": "knowledgebase/python/python3_logging_cookbook.html#implementing-structured-logging",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Although most logging messages are intended for reading by humans, and thus not readily machine-parseable, there might be circumstances where you want to output messages in a structured format which is capable of being parsed by a program (without needing complex regular expressions to parse the log message). This is straightforward to achieve using the logging package. There are a number of ways in which this could be achieved, but the following is a simple approach which uses JSON to serialise the event in a machine-parseable manner:\nimport json\nimport logging\n\nclass StructuredMessage:\n    def __init__(self, message, /, **kwargs):\n        self.message = message\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return '%s &gt;&gt;&gt; %s' % (self.message, json.dumps(self.kwargs))\n\n_ = StructuredMessage   # optional, to improve readability\n\nlogging.basicConfig(level=logging.INFO, format='%(message)s')\nlogging.info(_('message 1', foo='bar', bar='baz', num=123, fnum=123.456))\n\nIf the above script is run, it prints:\nmessage 1 &gt;&gt;&gt; {\"fnum\": 123.456, \"num\": 123, \"bar\": \"baz\", \"foo\": \"bar\"}\n\nNote that the order of items might be different according to the version of Python used.\nIf you need more specialised processing, you can use a custom JSON encoder, as in the following complete example:\nimport json\nimport logging\n\n\nclass Encoder(json.JSONEncoder):\n    def default(self, o):\n        if isinstance(o, set):\n            return tuple(o)\n        elif isinstance(o, str):\n            return o.encode('unicode_escape').decode('ascii')\n        return super().default(o)\n\nclass StructuredMessage:\n    def __init__(self, message, /, **kwargs):\n        self.message = message\n        self.kwargs = kwargs\n\n    def __str__(self):\n        s = Encoder().encode(self.kwargs)\n        return '%s &gt;&gt;&gt; %s' % (self.message, s)\n\n_ = StructuredMessage   # optional, to improve readability\n\ndef main():\n    logging.basicConfig(level=logging.INFO, format='%(message)s')\n    logging.info(_('message 1', set_value={1, 2, 3}, snowman='\\u2603'))\n\nif __name__ == '__main__':\n    main()\n\nWhen the above script is run, it prints:\nmessage 1 &gt;&gt;&gt; {\"snowman\": \"\\u2603\", \"set_value\": [1, 2, 3]}\n\nNote that the order of items might be different according to the version of Python used."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#customizing-handlers-with-dictconfig",
    "href": "knowledgebase/python/python3_logging_cookbook.html#customizing-handlers-with-dictconfig",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "There are times when you want to customize logging handlers in particular ways, and if you use dictConfig() you may be able to do this without subclassing. As an example, consider that you may want to set the ownership of a log file. On POSIX, this is easily done using shutil.chown(), but the file handlers in the stdlib don’t offer built-in support. You can customize handler creation using a plain function such as:\ndef owned_file_handler(filename, mode='a', encoding=None, owner=None):\n    if owner:\n        if not os.path.exists(filename):\n            open(filename, 'a').close()\n        shutil.chown(filename, *owner)\n    return logging.FileHandler(filename, mode, encoding)\n\nYou can then specify, in a logging configuration passed to dictConfig(), that a logging handler be created by calling this function:\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'default': {\n            'format': '%(asctime)s %(levelname)s %(name)s %(message)s'\n        },\n    },\n    'handlers': {\n        'file':{\n            # The values below are popped from this dictionary and\n            # used to create the handler, set the handler's level and\n            # its formatter.\n            '()': owned_file_handler,\n            'level':'DEBUG',\n            'formatter': 'default',\n            # The values below are passed to the handler creator callable\n            # as keyword arguments.\n            'owner': ['pulse', 'pulse'],\n            'filename': 'chowntest.log',\n            'mode': 'w',\n            'encoding': 'utf-8',\n        },\n    },\n    'root': {\n        'handlers': ['file'],\n        'level': 'DEBUG',\n    },\n}\n\nIn this example I am setting the ownership using the pulse user and group, just for the purposes of illustration. Putting it together into a working script, chowntest.py:\nimport logging, logging.config, os, shutil\n\ndef owned_file_handler(filename, mode='a', encoding=None, owner=None):\n    if owner:\n        if not os.path.exists(filename):\n            open(filename, 'a').close()\n        shutil.chown(filename, *owner)\n    return logging.FileHandler(filename, mode, encoding)\n\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'default': {\n            'format': '%(asctime)s %(levelname)s %(name)s %(message)s'\n        },\n    },\n    'handlers': {\n        'file':{\n            # The values below are popped from this dictionary and\n            # used to create the handler, set the handler's level and\n            # its formatter.\n            '()': owned_file_handler,\n            'level':'DEBUG',\n            'formatter': 'default',\n            # The values below are passed to the handler creator callable\n            # as keyword arguments.\n            'owner': ['pulse', 'pulse'],\n            'filename': 'chowntest.log',\n            'mode': 'w',\n            'encoding': 'utf-8',\n        },\n    },\n    'root': {\n        'handlers': ['file'],\n        'level': 'DEBUG',\n    },\n}\n\nlogging.config.dictConfig(LOGGING)\nlogger = logging.getLogger('mylogger')\nlogger.debug('A debug message')\n\nTo run this, you will probably need to run as root:\n$ sudo python3.3 chowntest.py\n$ cat chowntest.log\n2013-11-05 09:34:51,128 DEBUG mylogger A debug message\n$ ls -l chowntest.log\n-rw-r--r-- 1 pulse pulse 55 2013-11-05 09:34 chowntest.log\n\nNote that this example uses Python 3.3 because that’s where shutil.chown() makes an appearance. This approach should work with any Python version that supports dictConfig() - namely, Python 2.7, 3.2 or later. With pre-3.3 versions, you would need to implement the actual ownership change using e.g. os.chown().\nIn practice, the handler-creating function may be in a utility module somewhere in your project. Instead of the line in the configuration:\n'()': owned_file_handler,\n\nyou could use e.g.:\n'()': 'ext://project.util.owned_file_handler',\n\nwhere project.util can be replaced with the actual name of the package where the function resides. In the above working script, using 'ext://__main__.owned_file_handler' should work. Here, the actual callable is resolved by dictConfig() from the ext:// specification.\nThis example hopefully also points the way to how you could implement other types of file change - e.g. setting specific POSIX permission bits - in the same way, using os.chmod().\nOf course, the approach could also be extended to types of handler other than a FileHandler - for example, one of the rotating file handlers, or a different type of handler altogether."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#using-particular-formatting-styles-throughout-your-application",
    "href": "knowledgebase/python/python3_logging_cookbook.html#using-particular-formatting-styles-throughout-your-application",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "In Python 3.2, the Formatter gained a style keyword parameter which, while defaulting to % for backward compatibility, allowed the specification of { or $ to support the formatting approaches supported by str.format() and string.Template. Note that this governs the formatting of logging messages for final output to logs, and is completely orthogonal to how an individual logging message is constructed.\nLogging calls (debug(), info() etc.) only take positional parameters for the actual logging message itself, with keyword parameters used only for determining options for how to handle the logging call (e.g. the exc_info keyword parameter to indicate that traceback information should be logged, or the extra keyword parameter to indicate additional contextual information to be added to the log). So you cannot directly make logging calls using str.format() or string.Template syntax, because internally the logging package uses %-formatting to merge the format string and the variable arguments. There would be no changing this while preserving backward compatibility, since all logging calls which are out there in existing code will be using %-format strings.\nThere have been suggestions to associate format styles with specific loggers, but that approach also runs into backward compatibility problems because any existing code could be using a given logger name and using %-formatting.\nFor logging to work interoperably between any third-party libraries and your code, decisions about formatting need to be made at the level of the individual logging call. This opens up a couple of ways in which alternative formatting styles can be accommodated.\n\n\nIn Python 3.2, along with the Formatter changes mentioned above, the logging package gained the ability to allow users to set their own LogRecord subclasses, using the setLogRecordFactory() function. You can use this to set your own subclass of LogRecord, which does the Right Thing by overriding the getMessage() method. The base class implementation of this method is where the msg % args formatting happens, and where you can substitute your alternate formatting; however, you should be careful to support all formatting styles and allow %-formatting as the default, to ensure interoperability with other code. Care should also be taken to call str(self.msg), just as the base implementation does.\nRefer to the reference documentation on setLogRecordFactory() and LogRecord for more information.\n\n\n\nThere is another, perhaps simpler way that you can use {}- and $- formatting to construct your individual log messages. You may recall (from Using arbitrary objects as messages) that when logging you can use an arbitrary object as a message format string, and that the logging package will call str() on that object to get the actual format string. Consider the following two classes:\nclass BraceMessage:\n    def __init__(self, fmt, /, *args, **kwargs):\n        self.fmt = fmt\n        self.args = args\n        self.kwargs = kwargs\n\n    def __str__(self):\n        return self.fmt.format(*self.args, **self.kwargs)\n\nclass DollarMessage:\n    def __init__(self, fmt, /, **kwargs):\n        self.fmt = fmt\n        self.kwargs = kwargs\n\n    def __str__(self):\n        from string import Template\n        return Template(self.fmt).substitute(**self.kwargs)\n\nEither of these can be used in place of a format string, to allow {}- or \\(-formatting to be used to build the actual “message” part which appears in the formatted log output in place of “%(message)s” or “{message}” or “\\)message”. If you find it a little unwieldy to use the class names whenever you want to log something, you can make it more palatable if you use an alias such as M or _ for the message (or perhaps __, if you are using _ for localization).\nExamples of this approach are given below. Firstly, formatting with str.format():\n&gt;&gt;&gt;\n&gt;&gt;&gt; __ = BraceMessage\n&gt;&gt;&gt; print(__('Message with {0} {1}', 2, 'placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt; class Point: pass\n...\n&gt;&gt;&gt; p = Point()\n&gt;&gt;&gt; p.x = 0.5\n&gt;&gt;&gt; p.y = 0.5\n&gt;&gt;&gt; print(__('Message with coordinates: ({point.x:.2f}, {point.y:.2f})', point=p))\nMessage with coordinates: (0.50, 0.50)\n\nSecondly, formatting with string.Template:\n&gt;&gt;&gt;\n&gt;&gt;&gt; __ = DollarMessage\n&gt;&gt;&gt; print(__('Message with $num $what', num=2, what='placeholders'))\nMessage with 2 placeholders\n&gt;&gt;&gt;\n\nOne thing to note is that you pay no significant performance penalty with this approach: the actual formatting happens not when you make the logging call, but when (and if) the logged message is actually about to be output to a log by a handler. So the only slightly unusual thing which might trip you up is that the parentheses go around the format string and the arguments, not just the format string. That’s because the __ notation is just syntax sugar for a constructor call to one of the _XXX_Message classes shown above."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#configuring-filters-with-dictconfig",
    "href": "knowledgebase/python/python3_logging_cookbook.html#configuring-filters-with-dictconfig",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "You can configure filters using dictConfig(), though it might not be obvious at first glance how to do it (hence this recipe). Since Filter is the only filter class included in the standard library, and it is unlikely to cater to many requirements (it’s only there as a base class), you will typically need to define your own Filter subclass with an overridden filter() method. To do this, specify the () key in the configuration dictionary for the filter, specifying a callable which will be used to create the filter (a class is the most obvious, but you can provide any callable which returns a Filter instance). Here is a complete example:\nimport logging\nimport logging.config\nimport sys\n\nclass MyFilter(logging.Filter):\n    def __init__(self, param=None):\n        self.param = param\n\n    def filter(self, record):\n        if self.param is None:\n            allow = True\n        else:\n            allow = self.param not in record.msg\n        if allow:\n            record.msg = 'changed: ' + record.msg\n        return allow\n\nLOGGING = {\n    'version': 1,\n    'filters': {\n        'myfilter': {\n            '()': MyFilter,\n            'param': 'noshow',\n        }\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'filters': ['myfilter']\n        }\n    },\n    'root': {\n        'level': 'DEBUG',\n        'handlers': ['console']\n    },\n}\n\nif __name__ == '__main__':\n    logging.config.dictConfig(LOGGING)\n    logging.debug('hello')\n    logging.debug('hello - noshow')\n\nThis example shows how you can pass configuration data to the callable which constructs the instance, in the form of keyword parameters. When run, the above script will print:\nwhich shows that the filter is working as configured.\nA couple of extra points to note:\n\nIf you can’t refer to the callable directly in the configuration (e.g. if it lives in a different module, and you can’t import it directly where the configuration dictionary is), you can use the form ext://... as described in Access to external objects. For example, you could have used the text 'ext://__main__.MyFilter' instead of MyFilter in the above example.\nAs well as for filters, this technique can also be used to configure custom handlers and formatters. See User-defined objects for more information on how logging supports using user-defined objects in its configuration, and see the other cookbook recipe Customizing handlers with dictConfig() above."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#customized-exception-formatting",
    "href": "knowledgebase/python/python3_logging_cookbook.html#customized-exception-formatting",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "There might be times when you want to do customized exception formatting - for argument’s sake, let’s say you want exactly one line per logged event, even when exception information is present. You can do this with a custom formatter class, as shown in the following example:\nimport logging\n\nclass OneLineExceptionFormatter(logging.Formatter):\n    def formatException(self, exc_info):\n        \"\"\"\n        Format an exception so that it prints on a single line.\n        \"\"\"\n        result = super().formatException(exc_info)\n        return repr(result)  # or format into one line however you want to\n\n    def format(self, record):\n        s = super().format(record)\n        if record.exc_text:\n            s = s.replace('\\n', '') + '|'\n        return s\n\ndef configure_logging():\n    fh = logging.FileHandler('output.txt', 'w')\n    f = OneLineExceptionFormatter('%(asctime)s|%(levelname)s|%(message)s|',\n                                  '%d/%m/%Y %H:%M:%S')\n    fh.setFormatter(f)\n    root = logging.getLogger()\n    root.setLevel(logging.DEBUG)\n    root.addHandler(fh)\n\ndef main():\n    configure_logging()\n    logging.info('Sample message')\n    try:\n        x = 1 / 0\n    except ZeroDivisionError as e:\n        logging.exception('ZeroDivisionError: %s', e)\n\nif __name__ == '__main__':\n    main()\n\nWhen run, this produces a file with exactly two lines:\n28/01/2015 07:21:23|INFO|Sample message|\n28/01/2015 07:21:23|ERROR|ZeroDivisionError: integer division or modulo by zero|'Traceback (most recent call last):\\n  File \"logtest7.py\", line 30, in main\\n    x = 1 / 0\\nZeroDivisionError: integer division or modulo by zero'|\n\nWhile the above treatment is simplistic, it points the way to how exception information can be formatted to your liking. The traceback module may be helpful for more specialized needs."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#speaking-logging-messages",
    "href": "knowledgebase/python/python3_logging_cookbook.html#speaking-logging-messages",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "There might be situations when it is desirable to have logging messages rendered in an audible rather than a visible format. This is easy to do if you have text-to-speech (TTS) functionality available in your system, even if it doesn’t have a Python binding. Most TTS systems have a command line program you can run, and this can be invoked from a handler using subprocess. It’s assumed here that TTS command line programs won’t expect to interact with users or take a long time to complete, and that the frequency of logged messages will be not so high as to swamp the user with messages, and that it’s acceptable to have the messages spoken one at a time rather than concurrently, The example implementation below waits for one message to be spoken before the next is processed, and this might cause other handlers to be kept waiting. Here is a short example showing the approach, which assumes that the espeak TTS package is available:\nimport logging\nimport subprocess\nimport sys\n\nclass TTSHandler(logging.Handler):\n    def emit(self, record):\n        msg = self.format(record)\n        # Speak slowly in a female English voice\n        cmd = ['espeak', '-s150', '-ven+f3', msg]\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                             stderr=subprocess.STDOUT)\n        # wait for the program to finish\n        p.communicate()\n\ndef configure_logging():\n    h = TTSHandler()\n    root = logging.getLogger()\n    root.addHandler(h)\n    # the default formatter just returns the message\n    root.setLevel(logging.DEBUG)\n\ndef main():\n    logging.info('Hello')\n    logging.debug('Goodbye')\n\nif __name__ == '__main__':\n    configure_logging()\n    sys.exit(main())\n\nWhen run, this script should say “Hello” and then “Goodbye” in a female voice.\nThe above approach can, of course, be adapted to other TTS systems and even other systems altogether which can process messages via external programs run from a command line."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#buffering-logging-messages-and-outputting-them-conditionally",
    "href": "knowledgebase/python/python3_logging_cookbook.html#buffering-logging-messages-and-outputting-them-conditionally",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "There might be situations where you want to log messages in a temporary area and only output them if a certain condition occurs. For example, you may want to start logging debug events in a function, and if the function completes without errors, you don’t want to clutter the log with the collected debug information, but if there is an error, you want all the debug information to be output as well as the error.\nHere is an example which shows how you could do this using a decorator for your functions where you want logging to behave this way. It makes use of the logging.handlers.MemoryHandler, which allows buffering of logged events until some condition occurs, at which point the buffered events are flushed - passed to another handler (the target handler) for processing. By default, the MemoryHandler flushed when its buffer gets filled up or an event whose level is greater than or equal to a specified threshold is seen. You can use this recipe with a more specialised subclass of MemoryHandler if you want custom flushing behavior.\nThe example script has a simple function, foo, which just cycles through all the logging levels, writing to sys.stderr to say what level it’s about to log at, and then actually logging a message at that level. You can pass a parameter to foo which, if true, will log at ERROR and CRITICAL levels - otherwise, it only logs at DEBUG, INFO and WARNING levels.\nThe script just arranges to decorate foo with a decorator which will do the conditional logging that’s required. The decorator takes a logger as a parameter and attaches a memory handler for the duration of the call to the decorated function. The decorator can be additionally parameterised using a target handler, a level at which flushing should occur, and a capacity for the buffer (number of records buffered). These default to a StreamHandler which writes to sys.stderr, logging.ERROR and 100 respectively.\nHere’s the script:\nimport logging\nfrom logging.handlers import MemoryHandler\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(logging.NullHandler())\n\ndef log_if_errors(logger, target_handler=None, flush_level=None, capacity=None):\n    if target_handler is None:\n        target_handler = logging.StreamHandler()\n    if flush_level is None:\n        flush_level = logging.ERROR\n    if capacity is None:\n        capacity = 100\n    handler = MemoryHandler(capacity, flushLevel=flush_level, target=target_handler)\n\n    def decorator(fn):\n        def wrapper(*args, **kwargs):\n            logger.addHandler(handler)\n            try:\n                return fn(*args, **kwargs)\n            except Exception:\n                logger.exception('call failed')\n                raise\n            finally:\n                super(MemoryHandler, handler).flush()\n                logger.removeHandler(handler)\n        return wrapper\n\n    return decorator\n\ndef write_line(s):\n    sys.stderr.write('%s\\n' % s)\n\ndef foo(fail=False):\n    write_line('about to log at DEBUG ...')\n    logger.debug('Actually logged at DEBUG')\n    write_line('about to log at INFO ...')\n    logger.info('Actually logged at INFO')\n    write_line('about to log at WARNING ...')\n    logger.warning('Actually logged at WARNING')\n    if fail:\n        write_line('about to log at ERROR ...')\n        logger.error('Actually logged at ERROR')\n        write_line('about to log at CRITICAL ...')\n        logger.critical('Actually logged at CRITICAL')\n    return fail\n\ndecorated_foo = log_if_errors(logger)(foo)\n\nif __name__ == '__main__':\n    logger.setLevel(logging.DEBUG)\n    write_line('Calling undecorated foo with False')\n    assert not foo(False)\n    write_line('Calling undecorated foo with True')\n    assert foo(True)\n    write_line('Calling decorated foo with False')\n    assert not decorated_foo(False)\n    write_line('Calling decorated foo with True')\n    assert decorated_foo(True)\n\nWhen this script is run, the following output should be observed:\nCalling undecorated foo with False\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nCalling undecorated foo with True\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nabout to log at ERROR ...\nabout to log at CRITICAL ...\nCalling decorated foo with False\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nCalling decorated foo with True\nabout to log at DEBUG ...\nabout to log at INFO ...\nabout to log at WARNING ...\nabout to log at ERROR ...\nActually logged at DEBUG\nActually logged at INFO\nActually logged at WARNING\nActually logged at ERROR\nabout to log at CRITICAL ...\nActually logged at CRITICAL\n\nAs you can see, actual logging output only occurs when an event is logged whose severity is ERROR or greater, but in that case, any previous events at lower severities are also logged.\nYou can of course use the conventional means of decoration:\n@log_if_errors(logger)\ndef foo(fail=False):\n    ..."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#sending-logging-messages-to-email-with-buffering",
    "href": "knowledgebase/python/python3_logging_cookbook.html#sending-logging-messages-to-email-with-buffering",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "To illustrate how you can send log messages via email, so that a set number of messages are sent per email, you can subclass BufferingHandler. In the following example, which you can adapt to suit your specific needs, a simple test harness is provided which allows you to run the script with command line arguments specifying what you typically need to send things via SMTP. (Run the downloaded script with the -h argument to see the required and optional arguments.)\nimport logging\nimport logging.handlers\nimport smtplib\n\nclass BufferingSMTPHandler(logging.handlers.BufferingHandler):\n    def __init__(self, mailhost, port, username, password, fromaddr, toaddrs,\n                 subject, capacity):\n        logging.handlers.BufferingHandler.__init__(self, capacity)\n        self.mailhost = mailhost\n        self.mailport = port\n        self.username = username\n        self.password = password\n        self.fromaddr = fromaddr\n        if isinstance(toaddrs, str):\n            toaddrs = [toaddrs]\n        self.toaddrs = toaddrs\n        self.subject = subject\n        self.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)-5s %(message)s\"))\n\n    def flush(self):\n        if len(self.buffer) &gt; 0:\n            try:\n                smtp = smtplib.SMTP(self.mailhost, self.mailport)\n                smtp.starttls()\n                smtp.login(self.username, self.password)\n                msg = \"From: %s\\r\\nTo: %s\\r\\nSubject: %s\\r\\n\\r\\n\" % (self.fromaddr, ','.join(self.toaddrs), self.subject)\n                for record in self.buffer:\n                    s = self.format(record)\n                    msg = msg + s + \"\\r\\n\"\n                smtp.sendmail(self.fromaddr, self.toaddrs, msg)\n                smtp.quit()\n            except Exception:\n                if logging.raiseExceptions:\n                    raise\n            self.buffer = []\n\nif __name__ == '__main__':\n    import argparse\n\n    ap = argparse.ArgumentParser()\n    aa = ap.add_argument\n    aa('host', metavar='HOST', help='SMTP server')\n    aa('--port', '-p', type=int, default=587, help='SMTP port')\n    aa('user', metavar='USER', help='SMTP username')\n    aa('password', metavar='PASSWORD', help='SMTP password')\n    aa('to', metavar='TO', help='Addressee for emails')\n    aa('sender', metavar='SENDER', help='Sender email address')\n    aa('--subject', '-s',\n       default='Test Logging email from Python logging module (buffering)',\n       help='Subject of email')\n    options = ap.parse_args()\n    logger = logging.getLogger()\n    logger.setLevel(logging.DEBUG)\n    h = BufferingSMTPHandler(options.host, options.port, options.user,\n                             options.password, options.sender,\n                             options.to, options.subject, 10)\n    logger.addHandler(h)\n    for i in range(102):\n        logger.info(\"Info index = %d\", i)\n    h.flush()\n    h.close()\n\nIf you run this script and your SMTP server is correctly set up, you should find that it sends eleven emails to the addressee you specify. The first ten emails will each have ten log messages, and the eleventh will have two messages. That makes up 102 messages as specified in the script."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#formatting-times-using-utc-gmt-via-configuration",
    "href": "knowledgebase/python/python3_logging_cookbook.html#formatting-times-using-utc-gmt-via-configuration",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Sometimes you want to format times using UTC, which can be done using a class such as UTCFormatter, shown below:\nimport logging\nimport time\n\nclass UTCFormatter(logging.Formatter):\n    converter = time.gmtime\n\nand you can then use the UTCFormatter in your code instead of Formatter. If you want to do that via configuration, you can use the dictConfig() API with an approach illustrated by the following complete example:\nimport logging\nimport logging.config\nimport time\n\nclass UTCFormatter(logging.Formatter):\n    converter = time.gmtime\n\nLOGGING = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'utc': {\n            '()': UTCFormatter,\n            'format': '%(asctime)s %(message)s',\n        },\n        'local': {\n            'format': '%(asctime)s %(message)s',\n        }\n    },\n    'handlers': {\n        'console1': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'utc',\n        },\n        'console2': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'local',\n        },\n    },\n    'root': {\n        'handlers': ['console1', 'console2'],\n   }\n}\n\nif __name__ == '__main__':\n    logging.config.dictConfig(LOGGING)\n    logging.warning('The local time is %s', time.asctime())\n\nWhen this script is run, it should print something like:\n2015-10-17 12:53:29,501 The local time is Sat Oct 17 13:53:29 2015\n2015-10-17 13:53:29,501 The local time is Sat Oct 17 13:53:29 2015\n\nshowing how the time is formatted both as local time and UTC, one for each handler."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#using-a-context-manager-for-selective-logging",
    "href": "knowledgebase/python/python3_logging_cookbook.html#using-a-context-manager-for-selective-logging",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "There are times when it would be useful to temporarily change the logging configuration and revert it back after doing something. For this, a context manager is the most obvious way of saving and restoring the logging context. Here is a simple example of such a context manager, which allows you to optionally change the logging level and add a logging handler purely in the scope of the context manager:\nimport logging\nimport sys\n\nclass LoggingContext:\n    def __init__(self, logger, level=None, handler=None, close=True):\n        self.logger = logger\n        self.level = level\n        self.handler = handler\n        self.close = close\n\n    def __enter__(self):\n        if self.level is not None:\n            self.old_level = self.logger.level\n            self.logger.setLevel(self.level)\n        if self.handler:\n            self.logger.addHandler(self.handler)\n\n    def __exit__(self, et, ev, tb):\n        if self.level is not None:\n            self.logger.setLevel(self.old_level)\n        if self.handler:\n            self.logger.removeHandler(self.handler)\n        if self.handler and self.close:\n            self.handler.close()\n        # implicit return of None =&gt; don't swallow exceptions\n\nIf you specify a level value, the logger’s level is set to that value in the scope of the with block covered by the context manager. If you specify a handler, it is added to the logger on entry to the block and removed on exit from the block. You can also ask the manager to close the handler for you on block exit - you could do this if you don’t need the handler any more.\nTo illustrate how it works, we can add the following block of code to the above:\nif __name__ == '__main__':\n    logger = logging.getLogger('foo')\n    logger.addHandler(logging.StreamHandler())\n    logger.setLevel(logging.INFO)\n    logger.info('1. This should appear just once on stderr.')\n    logger.debug('2. This should not appear.')\n    with LoggingContext(logger, level=logging.DEBUG):\n        logger.debug('3. This should appear once on stderr.')\n    logger.debug('4. This should not appear.')\n    h = logging.StreamHandler(sys.stdout)\n    with LoggingContext(logger, level=logging.DEBUG, handler=h, close=True):\n        logger.debug('5. This should appear twice - once on stderr and once on stdout.')\n    logger.info('6. This should appear just once on stderr.')\n    logger.debug('7. This should not appear.')\n\nWe initially set the logger’s level to INFO, so message #1 appears and message #2 doesn’t. We then change the level to DEBUG temporarily in the following with block, and so message #3 appears. After the block exits, the logger’s level is restored to INFO and so message #4 doesn’t appear. In the next with block, we set the level to DEBUG again but also add a handler writing to sys.stdout. Thus, message #5 appears twice on the console (once via stderr and once via stdout). After the with statement’s completion, the status is as it was before so message #6 appears (like message #1) whereas message #7 doesn’t (just like message #2).\nIf we run the resulting script, the result is as follows:\n$ python logctx.py\n1. This should appear just once on stderr.\n3. This should appear once on stderr.\n5. This should appear twice - once on stderr and once on stdout.\n5. This should appear twice - once on stderr and once on stdout.\n6. This should appear just once on stderr.\n\nIf we run it again, but pipe stderr to /dev/null, we see the following, which is the only message written to stdout:\n$ python logctx.py 2&gt;/dev/null\n5. This should appear twice - once on stderr and once on stdout.\n\nOnce again, but piping stdout to /dev/null, we get:\n$ python logctx.py &gt;/dev/null\n1. This should appear just once on stderr.\n3. This should appear once on stderr.\n5. This should appear twice - once on stderr and once on stdout.\n6. This should appear just once on stderr.\n\nIn this case, the message #5 printed to stdout doesn’t appear, as expected.\nOf course, the approach described here can be generalised, for example to attach logging filters temporarily. Note that the above code works in Python 2 as well as Python 3."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#a-cli-application-starter-template",
    "href": "knowledgebase/python/python3_logging_cookbook.html#a-cli-application-starter-template",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Here’s an example which shows how you can:\n\nUse a logging level based on command-line arguments\nDispatch to multiple subcommands in separate files, all logging at the same level in a consistent way\nMake use of simple, minimal configuration\n\nSuppose we have a command-line application whose job is to stop, start or restart some services. This could be organised for the purposes of illustration as a file app.py that is the main script for the application, with individual commands implemented in start.py, stop.py and restart.py. Suppose further that we want to control the verbosity of the application via a command-line argument, defaulting to logging.INFO. Here’s one way that app.py could be written:\nimport argparse\nimport importlib\nimport logging\nimport os\nimport sys\n\ndef main(args=None):\n    scriptname = os.path.basename(__file__)\n    parser = argparse.ArgumentParser(scriptname)\n    levels = ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL')\n    parser.add_argument('--log-level', default='INFO', choices=levels)\n    subparsers = parser.add_subparsers(dest='command',\n                                       help='Available commands:')\n    start_cmd = subparsers.add_parser('start', help='Start a service')\n    start_cmd.add_argument('name', metavar='NAME',\n                           help='Name of service to start')\n    stop_cmd = subparsers.add_parser('stop',\n                                     help='Stop one or more services')\n    stop_cmd.add_argument('names', metavar='NAME', nargs='+',\n                          help='Name of service to stop')\n    restart_cmd = subparsers.add_parser('restart',\n                                        help='Restart one or more services')\n    restart_cmd.add_argument('names', metavar='NAME', nargs='+',\n                             help='Name of service to restart')\n    options = parser.parse_args()\n    # the code to dispatch commands could all be in this file. For the purposes\n    # of illustration only, we implement each command in a separate module.\n    try:\n        mod = importlib.import_module(options.command)\n        cmd = getattr(mod, 'command')\n    except (ImportError, AttributeError):\n        print('Unable to find the code for command \\'%s\\'' % options.command)\n        return 1\n    # Could get fancy here and load configuration from file or dictionary\n    logging.basicConfig(level=options.log_level,\n                        format='%(levelname)s %(name)s %(message)s')\n    cmd(options)\n\nif __name__ == '__main__':\n    sys.exit(main())\n\nAnd the start, stop and restart commands can be implemented in separate modules, like so for starting:\n# start.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef command(options):\n    logger.debug('About to start %s', options.name)\n    # actually do the command processing here ...\n    logger.info('Started the \\'%s\\' service.', options.name)\n\nand thus for stopping:\n# stop.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef command(options):\n    n = len(options.names)\n    if n == 1:\n        plural = ''\n        services = '\\'%s\\'' % options.names[0]\n    else:\n        plural = 's'\n        services = ', '.join('\\'%s\\'' % name for name in options.names)\n        i = services.rfind(', ')\n        services = services[:i] + ' and ' + services[i + 2:]\n    logger.debug('About to stop %s', services)\n    # actually do the command processing here ...\n    logger.info('Stopped the %s service%s.', services, plural)\n\nand similarly for restarting:\n# restart.py\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef command(options):\n    n = len(options.names)\n    if n == 1:\n        plural = ''\n        services = '\\'%s\\'' % options.names[0]\n    else:\n        plural = 's'\n        services = ', '.join('\\'%s\\'' % name for name in options.names)\n        i = services.rfind(', ')\n        services = services[:i] + ' and ' + services[i + 2:]\n    logger.debug('About to restart %s', services)\n    # actually do the command processing here ...\n    logger.info('Restarted the %s service%s.', services, plural)\n\nIf we run this application with the default log level, we get output like this:\n$ python app.py start foo\nINFO start Started the 'foo' service.\n\n$ python app.py stop foo bar\nINFO stop Stopped the 'foo' and 'bar' services.\n\n$ python app.py restart foo bar baz\nINFO restart Restarted the 'foo', 'bar' and 'baz' services.\n\nThe first word is the logging level, and the second word is the module or package name of the place where the event was logged.\nIf we change the logging level, then we can change the information sent to the log. For example, if we want more information:\n$ python app.py --log-level DEBUG start foo\nDEBUG start About to start foo\nINFO start Started the 'foo' service.\n\n$ python app.py --log-level DEBUG stop foo bar\nDEBUG stop About to stop 'foo' and 'bar'\nINFO stop Stopped the 'foo' and 'bar' services.\n\n$ python app.py --log-level DEBUG restart foo bar baz\nDEBUG restart About to restart 'foo', 'bar' and 'baz'\nINFO restart Restarted the 'foo', 'bar' and 'baz' services.\n\nAnd if we want less:\n$ python app.py --log-level WARNING start foo\n$ python app.py --log-level WARNING stop foo bar\n$ python app.py --log-level WARNING restart foo bar baz\n\nIn this case, the commands don’t print anything to the console, since nothing at WARNING level or above is logged by them."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#a-qt-gui-for-logging",
    "href": "knowledgebase/python/python3_logging_cookbook.html#a-qt-gui-for-logging",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "A question that comes up from time to time is about how to log to a GUI application. The Qt framework is a popular cross-platform UI framework with Python bindings using PySide2 or PyQt5 libraries.\nThe following example shows how to log to a Qt GUI. This introduces a simple QtHandler class which takes a callable, which should be a slot in the main thread that does GUI updates. A worker thread is also created to show how you can log to the GUI from both the UI itself (via a button for manual logging) as well as a worker thread doing work in the background (here, just logging messages at random levels with random short delays in between).\nThe worker thread is implemented using Qt’s QThread class rather than the threading module, as there are circumstances where one has to use QThread, which offers better integration with other Qt components.\nThe code should work with recent releases of any of PySide6, PyQt6, PySide2 or PyQt5. You should be able to adapt the approach to earlier versions of Qt. Please refer to the comments in the code snippet for more detailed information.\nimport datetime\nimport logging\nimport random\nimport sys\nimport time\n\n# Deal with minor differences between different Qt packages\ntry:\n    from PySide6 import QtCore, QtGui, QtWidgets\n    Signal = QtCore.Signal\n    Slot = QtCore.Slot\nexcept ImportError:\n    try:\n        from PyQt6 import QtCore, QtGui, QtWidgets\n        Signal = QtCore.pyqtSignal\n        Slot = QtCore.pyqtSlot\n    except ImportError:\n        try:\n            from PySide2 import QtCore, QtGui, QtWidgets\n            Signal = QtCore.Signal\n            Slot = QtCore.Slot\n        except ImportError:\n            from PyQt5 import QtCore, QtGui, QtWidgets\n            Signal = QtCore.pyqtSignal\n            Slot = QtCore.pyqtSlot\n\nlogger = logging.getLogger(__name__)\n\n\n#\n# Signals need to be contained in a QObject or subclass in order to be correctly\n# initialized.\n#\nclass Signaller(QtCore.QObject):\n    signal = Signal(str, logging.LogRecord)\n\n#\n# Output to a Qt GUI is only supposed to happen on the main thread. So, this\n# handler is designed to take a slot function which is set up to run in the main\n# thread. In this example, the function takes a string argument which is a\n# formatted log message, and the log record which generated it. The formatted\n# string is just a convenience - you could format a string for output any way\n# you like in the slot function itself.\n#\n# You specify the slot function to do whatever GUI updates you want. The handler\n# doesn't know or care about specific UI elements.\n#\nclass QtHandler(logging.Handler):\n    def __init__(self, slotfunc, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.signaller = Signaller()\n        self.signaller.signal.connect(slotfunc)\n\n    def emit(self, record):\n        s = self.format(record)\n        self.signaller.signal.emit(s, record)\n\n#\n# This example uses QThreads, which means that the threads at the Python level\n# are named something like \"Dummy-1\". The function below gets the Qt name of the\n# current thread.\n#\ndef ctname():\n    return QtCore.QThread.currentThread().objectName()\n\n\n#\n# Used to generate random levels for logging.\n#\nLEVELS = (logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR,\n          logging.CRITICAL)\n\n#\n# This worker class represents work that is done in a thread separate to the\n# main thread. The way the thread is kicked off to do work is via a button press\n# that connects to a slot in the worker.\n#\n# Because the default threadName value in the LogRecord isn't much use, we add\n# a qThreadName which contains the QThread name as computed above, and pass that\n# value in an \"extra\" dictionary which is used to update the LogRecord with the\n# QThread name.\n#\n# This example worker just outputs messages sequentially, interspersed with\n# random delays of the order of a few seconds.\n#\nclass Worker(QtCore.QObject):\n    @Slot()\n    def start(self):\n        extra = {'qThreadName': ctname() }\n        logger.debug('Started work', extra=extra)\n        i = 1\n        # Let the thread run until interrupted. This allows reasonably clean\n        # thread termination.\n        while not QtCore.QThread.currentThread().isInterruptionRequested():\n            delay = 0.5 + random.random() * 2\n            time.sleep(delay)\n            try:\n                if random.random() &lt; 0.1:\n                    raise ValueError('Exception raised: %d' % i)\n                else:\n                    level = random.choice(LEVELS)\n                    logger.log(level, 'Message after delay of %3.1f: %d', delay, i, extra=extra)\n            except ValueError as e:\n                logger.exception('Failed: %s', e, extra=extra)\n            i += 1\n\n#\n# Implement a simple UI for this cookbook example. This contains:\n#\n# * A read-only text edit window which holds formatted log messages\n# * A button to start work and log stuff in a separate thread\n# * A button to log something from the main thread\n# * A button to clear the log window\n#\nclass Window(QtWidgets.QWidget):\n\n    COLORS = {\n        logging.DEBUG: 'black',\n        logging.INFO: 'blue',\n        logging.WARNING: 'orange',\n        logging.ERROR: 'red',\n        logging.CRITICAL: 'purple',\n    }\n\n    def __init__(self, app):\n        super().__init__()\n        self.app = app\n        self.textedit = te = QtWidgets.QPlainTextEdit(self)\n        # Set whatever the default monospace font is for the platform\n        f = QtGui.QFont('nosuchfont')\n        if hasattr(f, 'Monospace'):\n            f.setStyleHint(f.Monospace)\n        else:\n            f.setStyleHint(f.StyleHint.Monospace)  # for Qt6\n        te.setFont(f)\n        te.setReadOnly(True)\n        PB = QtWidgets.QPushButton\n        self.work_button = PB('Start background work', self)\n        self.log_button = PB('Log a message at a random level', self)\n        self.clear_button = PB('Clear log window', self)\n        self.handler = h = QtHandler(self.update_status)\n        # Remember to use qThreadName rather than threadName in the format string.\n        fs = '%(asctime)s %(qThreadName)-12s %(levelname)-8s %(message)s'\n        formatter = logging.Formatter(fs)\n        h.setFormatter(formatter)\n        logger.addHandler(h)\n        # Set up to terminate the QThread when we exit\n        app.aboutToQuit.connect(self.force_quit)\n\n        # Lay out all the widgets\n        layout = QtWidgets.QVBoxLayout(self)\n        layout.addWidget(te)\n        layout.addWidget(self.work_button)\n        layout.addWidget(self.log_button)\n        layout.addWidget(self.clear_button)\n        self.setFixedSize(900, 400)\n\n        # Connect the non-worker slots and signals\n        self.log_button.clicked.connect(self.manual_update)\n        self.clear_button.clicked.connect(self.clear_display)\n\n        # Start a new worker thread and connect the slots for the worker\n        self.start_thread()\n        self.work_button.clicked.connect(self.worker.start)\n        # Once started, the button should be disabled\n        self.work_button.clicked.connect(lambda : self.work_button.setEnabled(False))\n\n    def start_thread(self):\n        self.worker = Worker()\n        self.worker_thread = QtCore.QThread()\n        self.worker.setObjectName('Worker')\n        self.worker_thread.setObjectName('WorkerThread')  # for qThreadName\n        self.worker.moveToThread(self.worker_thread)\n        # This will start an event loop in the worker thread\n        self.worker_thread.start()\n\n    def kill_thread(self):\n        # Just tell the worker to stop, then tell it to quit and wait for that\n        # to happen\n        self.worker_thread.requestInterruption()\n        if self.worker_thread.isRunning():\n            self.worker_thread.quit()\n            self.worker_thread.wait()\n        else:\n            print('worker has already exited.')\n\n    def force_quit(self):\n        # For use when the window is closed\n        if self.worker_thread.isRunning():\n            self.kill_thread()\n\n    # The functions below update the UI and run in the main thread because\n    # that's where the slots are set up\n\n    @Slot(str, logging.LogRecord)\n    def update_status(self, status, record):\n        color = self.COLORS.get(record.levelno, 'black')\n        s = '&lt;pre&gt;&lt;font color=\"%s\"&gt;%s&lt;/font&gt;&lt;/pre&gt;' % (color, status)\n        self.textedit.appendHtml(s)\n\n    @Slot()\n    def manual_update(self):\n        # This function uses the formatted message passed in, but also uses\n        # information from the record to format the message in an appropriate\n        # color according to its severity (level).\n        level = random.choice(LEVELS)\n        extra = {'qThreadName': ctname() }\n        logger.log(level, 'Manually logged!', extra=extra)\n\n    @Slot()\n    def clear_display(self):\n        self.textedit.clear()\n\n\ndef main():\n    QtCore.QThread.currentThread().setObjectName('MainThread')\n    logging.getLogger().setLevel(logging.DEBUG)\n    app = QtWidgets.QApplication(sys.argv)\n    example = Window(app)\n    example.show()\n    if hasattr(app, 'exec'):\n        rc = app.exec()\n    else:\n        rc = app.exec_()\n    sys.exit(rc)\n\nif __name__=='__main__':\n    main()"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#logging-to-syslog-with-rfc5424-support",
    "href": "knowledgebase/python/python3_logging_cookbook.html#logging-to-syslog-with-rfc5424-support",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Although RFC 5424 dates from 2009, most syslog servers are configured by default to use the older RFC 3164, which hails from 2001. When logging was added to Python in 2003, it supported the earlier (and only existing) protocol at the time. Since RFC5424 came out, as there has not been widespread deployment of it in syslog servers, the SysLogHandler functionality has not been updated.\nRFC 5424 contains some useful features such as support for structured data, and if you need to be able to log to a syslog server with support for it, you can do so with a subclassed handler which looks something like this:\nimport datetime\nimport logging.handlers\nimport re\nimport socket\nimport time\n\nclass SysLogHandler5424(logging.handlers.SysLogHandler):\n\n    tz_offset = re.compile(r'([+-]\\d{2})(\\d{2})$')\n    escaped = re.compile(r'([\\]\"\\\\])')\n\n    def __init__(self, *args, **kwargs):\n        self.msgid = kwargs.pop('msgid', None)\n        self.appname = kwargs.pop('appname', None)\n        super().__init__(*args, **kwargs)\n\n    def format(self, record):\n        version = 1\n        asctime = datetime.datetime.fromtimestamp(record.created).isoformat()\n        m = self.tz_offset.match(time.strftime('%z'))\n        has_offset = False\n        if m and time.timezone:\n            hrs, mins = m.groups()\n            if int(hrs) or int(mins):\n                has_offset = True\n        if not has_offset:\n            asctime += 'Z'\n        else:\n            asctime += f'{hrs}:{mins}'\n        try:\n            hostname = socket.gethostname()\n        except Exception:\n            hostname = '-'\n        appname = self.appname or '-'\n        procid = record.process\n        msgid = '-'\n        msg = super().format(record)\n        sdata = '-'\n        if hasattr(record, 'structured_data'):\n            sd = record.structured_data\n            # This should be a dict where the keys are SD-ID and the value is a\n            # dict mapping PARAM-NAME to PARAM-VALUE (refer to the RFC for what these\n            # mean)\n            # There's no error checking here - it's purely for illustration, and you\n            # can adapt this code for use in production environments\n            parts = []\n\n            def replacer(m):\n                g = m.groups()\n                return '\\\\' + g[0]\n\n            for sdid, dv in sd.items():\n                part = f'[{sdid}'\n                for k, v in dv.items():\n                    s = str(v)\n                    s = self.escaped.sub(replacer, s)\n                    part += f' {k}=\"{s}\"'\n                part += ']'\n                parts.append(part)\n            sdata = ''.join(parts)\n        return f'{version} {asctime} {hostname} {appname} {procid} {msgid} {sdata} {msg}'\n\nYou’ll need to be familiar with RFC 5424 to fully understand the above code, and it may be that you have slightly different needs (e.g. for how you pass structural data to the log). Nevertheless, the above should be adaptable to your speciric needs. With the above handler, you’d pass structured data using something like this:\nsd = {\n    'foo@12345': {'bar': 'baz', 'baz': 'bozz', 'fizz': r'buzz'},\n    'foo@54321': {'rab': 'baz', 'zab': 'bozz', 'zzif': r'buzz'}\n}\nextra = {'structured_data': sd}\ni = 1\nlogger.debug('Message %d', i, extra=extra)"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#how-to-treat-a-logger-like-an-output-stream",
    "href": "knowledgebase/python/python3_logging_cookbook.html#how-to-treat-a-logger-like-an-output-stream",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Sometimes, you need to interface to a third-party API which expects a file-like object to write to, but you want to direct the API’s output to a logger. You can do this using a class which wraps a logger with a file-like API. Here’s a short script illustrating such a class:\nimport logging\n\nclass LoggerWriter:\n    def __init__(self, logger, level):\n        self.logger = logger\n        self.level = level\n\n    def write(self, message):\n        if message != '\\n':  # avoid printing bare newlines, if you like\n            self.logger.log(self.level, message)\n\n    def flush(self):\n        # doesn't actually do anything, but might be expected of a file-like\n        # object - so optional depending on your situation\n        pass\n\n    def close(self):\n        # doesn't actually do anything, but might be expected of a file-like\n        # object - so optional depending on your situation. You might want\n        # to set a flag so that later calls to write raise an exception\n        pass\n\ndef main():\n    logging.basicConfig(level=logging.DEBUG)\n    logger = logging.getLogger('demo')\n    info_fp = LoggerWriter(logger, logging.INFO)\n    debug_fp = LoggerWriter(logger, logging.DEBUG)\n    print('An INFO message', file=info_fp)\n    print('A DEBUG message', file=debug_fp)\n\nif __name__ == \"__main__\":\n    main()\n\nWhen this script is run, it prints\nINFO:demo:An INFO message\nDEBUG:demo:A DEBUG message\n\nYou could also use LoggerWriter to redirect sys.stdout and sys.stderr by doing something like this:\nimport sys\n\nsys.stdout = LoggerWriter(logger, logging.INFO)\nsys.stderr = LoggerWriter(logger, logging.WARNING)\n\nYou should do this after configuring logging for your needs. In the above example, the basicConfig() call does this (using the sys.stderr value before it is overwritten by a LoggerWriter instance). Then, you’d get this kind of result:\n&gt;&gt;&gt;\n&gt;&gt;&gt; print('Foo')\nINFO:demo:Foo\n&gt;&gt;&gt; print('Bar', file=sys.stderr)\nWARNING:demo:Bar\n&gt;&gt;&gt;\n\nOf course, the examples above show output according to the format used by basicConfig(), but you can use a different formatter when you configure logging.\nNote that with the above scheme, you are somewhat at the mercy of buffering and the sequence of write calls which you are intercepting. For example, with the definition of LoggerWriter above, if you have the snippet\nsys.stderr = LoggerWriter(logger, logging.WARNING)\n1 / 0\n\nthen running the script results in\nWARNING:demo:Traceback (most recent call last):\n\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/test.py\", line 53, in &lt;module&gt;\n\nWARNING:demo:\nWARNING:demo:main()\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/test.py\", line 49, in main\n\nWARNING:demo:\nWARNING:demo:1 / 0\nWARNING:demo:ZeroDivisionError\nWARNING:demo::\nWARNING:demo:division by zero\n\nAs you can see, this output isn’t ideal. That’s because the underlying code which writes to sys.stderr makes multiple writes, each of which results in a separate logged line (for example, the last three lines above). To get around this problem, you need to buffer things and only output log lines when newlines are seen. Let’s use a slightly better implementation of LoggerWriter:\nclass BufferingLoggerWriter(LoggerWriter):\n    def __init__(self, logger, level):\n        super().__init__(logger, level)\n        self.buffer = ''\n\n    def write(self, message):\n        if '\\n' not in message:\n            self.buffer += message\n        else:\n            parts = message.split('\\n')\n            if self.buffer:\n                s = self.buffer + parts.pop(0)\n                self.logger.log(self.level, s)\n            self.buffer = parts.pop()\n            for part in parts:\n                self.logger.log(self.level, part)\n\nThis just buffers up stuff until a newline is seen, and then logs complete lines. With this approach, you get better output:\nWARNING:demo:Traceback (most recent call last):\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/main.py\", line 55, in &lt;module&gt;\nWARNING:demo:    main()\nWARNING:demo:  File \"/home/runner/cookbook-loggerwriter/main.py\", line 52, in main\nWARNING:demo:    1/0\nWARNING:demo:ZeroDivisionError: division by zero"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_cookbook.html#patterns-to-avoid",
    "href": "knowledgebase/python/python3_logging_cookbook.html#patterns-to-avoid",
    "title": "Logging Cookbook — Python 3.13.1 documentation",
    "section": "",
    "text": "Although the preceding sections have described ways of doing things you might need to do or deal with, it is worth mentioning some usage patterns which are unhelpful, and which should therefore be avoided in most cases. The following sections are in no particular order.\n\n\nOn Windows, you will generally not be able to open the same file multiple times as this will lead to a “file is in use by another process” error. However, on POSIX platforms you’ll not get any errors if you open the same file multiple times. This could be done accidentally, for example by:\n\nAdding a file handler more than once which references the same file (e.g. by a copy/paste/forget-to-change error).\nOpening two files that look different, as they have different names, but are the same because one is a symbolic link to the other.\nForking a process, following which both parent and child have a reference to the same file. This might be through use of the multiprocessing module, for example.\n\nOpening a file multiple times might appear to work most of the time, but can lead to a number of problems in practice:\n\nLogging output can be garbled because multiple threads or processes try to write to the same file. Although logging guards against concurrent use of the same handler instance by multiple threads, there is no such protection if concurrent writes are attempted by two different threads using two different handler instances which happen to point to the same file.\nAn attempt to delete a file (e.g. during file rotation) silently fails, because there is another reference pointing to it. This can lead to confusion and wasted debugging time - log entries end up in unexpected places, or are lost altogether. Or a file that was supposed to be moved remains in place, and grows in size unexpectedly despite size-based rotation being supposedly in place.\n\nUse the techniques outlined in Logging to a single file from multiple processes to circumvent such issues.\n\n\n\nWhile there might be unusual cases where you’ll need to do this, in general there is no point because loggers are singletons. Code can always access a given logger instance by name using logging.getLogger(name), so passing instances around and holding them as instance attributes is pointless. Note that in other languages such as Java and C#, loggers are often static class attributes. However, this pattern doesn’t make sense in Python, where the module (and not the class) is the unit of software decomposition.\n\n\n\nConfiguring logging by adding handlers, formatters and filters is the responsibility of the application developer, not the library developer. If you are maintaining a library, ensure that you don’t add handlers to any of your loggers other than a NullHandler instance.\n\n\n\nLoggers are singletons that are never freed during a script execution, and so creating lots of loggers will use up memory which can’t then be freed. Rather than create a logger per e.g. file processed or network connection made, use the existing mechanisms for passing contextual information into your logs and restrict the loggers created to those describing areas within your application (generally modules, but occasionally slightly more fine-grained than that)."
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html",
    "href": "knowledgebase/cheatsheets/tmux.html",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "A comprehensive guide to tmux, a terminal multiplexer that allows you to manage multiple terminal sessions within a single window. Ideal for managing long-running processes, organizing workspaces, and enhancing productivity.\n\n\n\n\nInstallation\nStarting tmux\nBasic Commands\nSession Management\nWindow Management\nPane Management\nCustomization\nUseful Tips & Tricks\nCommon Use Cases\n\n\n\n\n\nEnsure tmux is installed on your Ubuntu WSL environment.\nsudo apt update\nsudo apt install tmux\nVerify installation:\ntmux -V\n# Example Output: tmux 3.2a\n\n\n\n\n\n\ntmux\n\n\n\ntmux new -s mysession\n\n\n\ntmux attach -t mysession\n# or\ntmux a -t mysession\n\n\n\ntmux ls\n\n\n\n\nKeyboard Shortcut: Ctrl + b then d\n\n\n\n\n\n\ntmux uses a prefix key (Ctrl + b by default) followed by a command key.\n\nPrefix Key: Ctrl + b\n\n\n\n\n\n\n\n\n\n\nAction\nCommand\n\n\n\n\nSend prefix key to application\nCtrl + b then Ctrl + b\n\n\nList key bindings\nCtrl + b then ?\n\n\nReload configuration\nCtrl + b then : then type source-file ~/.tmux.conf\n\n\n\n\n\n\n\n\n\n\ntmux new -s mysession\n\n\n\ntmux ls\n\n\n\ntmux attach -t mysession\n\n\n\n\nKeyboard Shortcut: Ctrl + b then d\n\n\n\n\ntmux kill-session -t mysession\n\n\n\n\nCommand: tmux rename-session -t oldname newname\n\n\n\n\n\n\nEach session can have multiple windows, akin to tabs.\n\n\n\nKeyboard Shortcut: Ctrl + b then c\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then n\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then p\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then &lt;window-number&gt;\nExample: Ctrl + b then 2 to switch to window 2.\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then ,\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then &\n\n\n\n\n\n\nSplit windows into multiple panes for parallel tasks.\n\n\n\nKeyboard Shortcut: Ctrl + b then \"\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then %\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then arrow keys (←, →, ↑, ↓)\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then : and type resize-pane -D / -U / -L / -R\nAlternatively, use Ctrl + b then Ctrl + arrow keys (if configured).\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then x\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then o\n\n\n\n\n\n\nCustomize tmux behavior via the ~/.tmux.conf file.\n\n\n# Set prefix to Ctrl + a (optional)\n# set-option -g prefix C-a\n# unbind default prefix\n# unbind C-b\n# bind new prefix\n# bind C-a send-prefix\n\n# Enable mouse support\nset -g mouse on\n\n# Set window splitting shortcuts\nbind | split-window -h\nbind - split-window -v\n\n# Reload config with prefix + r\nbind r source-file ~/.tmux.conf \\; display-message \"Config Reloaded!\"\n\n# Enable vim-style pane switching\nbind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\n# Set status bar\nset -g status-bg colour235\nset -g status-fg colour136\nset -g status-left '[: #S | #I:#W]'\n\n\n\nAfter editing ~/.tmux.conf, reload it:\ntmux source-file ~/.tmux.conf\n\n\n\n\n\n\nCopy Mode: Enter copy mode to navigate and copy text.\n\nKeyboard Shortcut: Ctrl + b then [\nExit Copy Mode: q or Esc\n\nSearch in Copy Mode:\n\nPress / and type the search query.\n\nScroll Up/Down:\n\nUse arrow keys or PgUp/PgDn in copy mode.\n\nSynchronize Panes:\n\nExecute the same command in all panes.\nCommand: Ctrl + b then :setw synchronize-panes on\n\nLock tmux Session:\n\nCommand: Ctrl + b then :lock-server\n\nDisplay Current Key Bindings:\n\nKeyboard Shortcut: Ctrl + b then ?\n\n\n\n\n\n\n\n\nOrganize different processes in separate panes or windows.\n# Window 1: Development server\ntmux new -s dev\n# Inside tmux\n# Pane 1: Run server\npython manage.py runserver\n\n# Pane 2: Monitor logs\ntail -f logs/server.log\n\n\n\nMaintain persistent sessions across SSH connections.\n# On remote server\ntmux new -s remote\n# Start long-running task\npython train_model.py\n\n# Detach and disconnect\nCtrl + b then d\n\n# Reattach later\ntmux attach -t remote\n\n\n\nUse separate panes for editing, committing, and viewing status.\n# Split window vertically\nCtrl + b then %\n\n# Pane 1: Edit code\nvim main.py\n\n# Pane 2: Git status and commits\ngit status\ngit commit -m \"Update main.py\"\n\n\n\nRun monitoring tools alongside development.\n# Split window horizontally\nCtrl + b then \"\n\n# Pane 1: Develop\nvim script.py\n\n# Pane 2: Monitor\nhtop\n\n\n\n\n\n\nStart a New Session:\ntmux new -s ai_project\nCreate Windows:\n\nWindow 1: Code Editor\n\nKeyboard Shortcut: Ctrl + b then c\nCommands: bash       vim main.py\n\nWindow 2: Terminal\n\nKeyboard Shortcut: Ctrl + b then c\nCommands: bash       python main.py\n\nWindow 3: Logs\n\nKeyboard Shortcut: Ctrl + b then c\nCommands: bash       tail -f logs/output.log\n\n\nNavigate Between Windows:\n\nNext Window: Ctrl + b then n\nPrevious Window: Ctrl + b then p\nSelect Window by Number: Ctrl + b then 1, 2, 3, etc.\n\nDetach and Reattach:\n\nDetach: Ctrl + b then d\nReattach: tmux attach -t ai_project\n\n\n\n\n\n\n\ntmux Official Documentation\ntmux Cheat Sheet & Quick Reference\nMastering tmux"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#table-of-contents",
    "href": "knowledgebase/cheatsheets/tmux.html#table-of-contents",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Installation\nStarting tmux\nBasic Commands\nSession Management\nWindow Management\nPane Management\nCustomization\nUseful Tips & Tricks\nCommon Use Cases"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#installation",
    "href": "knowledgebase/cheatsheets/tmux.html#installation",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Ensure tmux is installed on your Ubuntu WSL environment.\nsudo apt update\nsudo apt install tmux\nVerify installation:\ntmux -V\n# Example Output: tmux 3.2a"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#starting-tmux",
    "href": "knowledgebase/cheatsheets/tmux.html#starting-tmux",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "tmux\n\n\n\ntmux new -s mysession\n\n\n\ntmux attach -t mysession\n# or\ntmux a -t mysession\n\n\n\ntmux ls\n\n\n\n\nKeyboard Shortcut: Ctrl + b then d"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#basic-commands",
    "href": "knowledgebase/cheatsheets/tmux.html#basic-commands",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "tmux uses a prefix key (Ctrl + b by default) followed by a command key.\n\nPrefix Key: Ctrl + b\n\n\n\n\n\n\n\n\n\n\nAction\nCommand\n\n\n\n\nSend prefix key to application\nCtrl + b then Ctrl + b\n\n\nList key bindings\nCtrl + b then ?\n\n\nReload configuration\nCtrl + b then : then type source-file ~/.tmux.conf"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#session-management",
    "href": "knowledgebase/cheatsheets/tmux.html#session-management",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "tmux new -s mysession\n\n\n\ntmux ls\n\n\n\ntmux attach -t mysession\n\n\n\n\nKeyboard Shortcut: Ctrl + b then d\n\n\n\n\ntmux kill-session -t mysession\n\n\n\n\nCommand: tmux rename-session -t oldname newname"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#window-management",
    "href": "knowledgebase/cheatsheets/tmux.html#window-management",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Each session can have multiple windows, akin to tabs.\n\n\n\nKeyboard Shortcut: Ctrl + b then c\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then n\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then p\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then &lt;window-number&gt;\nExample: Ctrl + b then 2 to switch to window 2.\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then ,\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then &"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#pane-management",
    "href": "knowledgebase/cheatsheets/tmux.html#pane-management",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Split windows into multiple panes for parallel tasks.\n\n\n\nKeyboard Shortcut: Ctrl + b then \"\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then %\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then arrow keys (←, →, ↑, ↓)\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then : and type resize-pane -D / -U / -L / -R\nAlternatively, use Ctrl + b then Ctrl + arrow keys (if configured).\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then x\n\n\n\n\n\nKeyboard Shortcut: Ctrl + b then o"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#customization",
    "href": "knowledgebase/cheatsheets/tmux.html#customization",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Customize tmux behavior via the ~/.tmux.conf file.\n\n\n# Set prefix to Ctrl + a (optional)\n# set-option -g prefix C-a\n# unbind default prefix\n# unbind C-b\n# bind new prefix\n# bind C-a send-prefix\n\n# Enable mouse support\nset -g mouse on\n\n# Set window splitting shortcuts\nbind | split-window -h\nbind - split-window -v\n\n# Reload config with prefix + r\nbind r source-file ~/.tmux.conf \\; display-message \"Config Reloaded!\"\n\n# Enable vim-style pane switching\nbind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\n# Set status bar\nset -g status-bg colour235\nset -g status-fg colour136\nset -g status-left '[: #S | #I:#W]'\n\n\n\nAfter editing ~/.tmux.conf, reload it:\ntmux source-file ~/.tmux.conf"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#useful-tips-tricks",
    "href": "knowledgebase/cheatsheets/tmux.html#useful-tips-tricks",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Copy Mode: Enter copy mode to navigate and copy text.\n\nKeyboard Shortcut: Ctrl + b then [\nExit Copy Mode: q or Esc\n\nSearch in Copy Mode:\n\nPress / and type the search query.\n\nScroll Up/Down:\n\nUse arrow keys or PgUp/PgDn in copy mode.\n\nSynchronize Panes:\n\nExecute the same command in all panes.\nCommand: Ctrl + b then :setw synchronize-panes on\n\nLock tmux Session:\n\nCommand: Ctrl + b then :lock-server\n\nDisplay Current Key Bindings:\n\nKeyboard Shortcut: Ctrl + b then ?"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#common-use-cases",
    "href": "knowledgebase/cheatsheets/tmux.html#common-use-cases",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Organize different processes in separate panes or windows.\n# Window 1: Development server\ntmux new -s dev\n# Inside tmux\n# Pane 1: Run server\npython manage.py runserver\n\n# Pane 2: Monitor logs\ntail -f logs/server.log\n\n\n\nMaintain persistent sessions across SSH connections.\n# On remote server\ntmux new -s remote\n# Start long-running task\npython train_model.py\n\n# Detach and disconnect\nCtrl + b then d\n\n# Reattach later\ntmux attach -t remote\n\n\n\nUse separate panes for editing, committing, and viewing status.\n# Split window vertically\nCtrl + b then %\n\n# Pane 1: Edit code\nvim main.py\n\n# Pane 2: Git status and commits\ngit status\ngit commit -m \"Update main.py\"\n\n\n\nRun monitoring tools alongside development.\n# Split window horizontally\nCtrl + b then \"\n\n# Pane 1: Develop\nvim script.py\n\n# Pane 2: Monitor\nhtop"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#example-workflow",
    "href": "knowledgebase/cheatsheets/tmux.html#example-workflow",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "Start a New Session:\ntmux new -s ai_project\nCreate Windows:\n\nWindow 1: Code Editor\n\nKeyboard Shortcut: Ctrl + b then c\nCommands: bash       vim main.py\n\nWindow 2: Terminal\n\nKeyboard Shortcut: Ctrl + b then c\nCommands: bash       python main.py\n\nWindow 3: Logs\n\nKeyboard Shortcut: Ctrl + b then c\nCommands: bash       tail -f logs/output.log\n\n\nNavigate Between Windows:\n\nNext Window: Ctrl + b then n\nPrevious Window: Ctrl + b then p\nSelect Window by Number: Ctrl + b then 1, 2, 3, etc.\n\nDetach and Reattach:\n\nDetach: Ctrl + b then d\nReattach: tmux attach -t ai_project"
  },
  {
    "objectID": "knowledgebase/cheatsheets/tmux.html#references",
    "href": "knowledgebase/cheatsheets/tmux.html#references",
    "title": "tmux Cheat Sheet",
    "section": "",
    "text": "tmux Official Documentation\ntmux Cheat Sheet & Quick Reference\nMastering tmux"
  },
  {
    "objectID": "index.html#featured-work",
    "href": "index.html#featured-work",
    "title": "Glenn Matlin",
    "section": "Featured Work",
    "text": "Featured Work\n\n\n🚀 Latest Project\n\nFLAME - Financial Language Model Evaluation\nA comprehensive framework for evaluating large language models on financial domain knowledge, reasoning, and compliance tasks.\n\n\n\n📝 Recent Post\n\nThe Challenges of Evaluating Large Language Models\nAn exploration of current challenges in LLM evaluation and promising approaches for more comprehensive assessment frameworks.\n\n\n\n📚 Latest Publication\n\nUnfoldML at NeurIPS 2022\nCost-aware and uncertainty-based framework for dynamic 2D prediction in multi-stage classification systems."
  },
  {
    "objectID": "index.html#areas-of-expertise",
    "href": "index.html#areas-of-expertise",
    "title": "Glenn Matlin",
    "section": "Areas of Expertise",
    "text": "Areas of Expertise\n\n\nMachine Learning & AI\nBuilding and evaluating ML models with focus on Large Language Models and their real-world applications.\n\n\nSoftware Engineering\nFull-stack development with modern frameworks and industry best practices for scalable solutions.\n\n\nHealthcare Technology\nLeveraging technology and data science to improve healthcare outcomes and operational efficiency.\n\n\nFinancial Technology\nBuilding secure and scalable financial services with focus on compliance and user experience."
  },
  {
    "objectID": "index.html#recent-experience",
    "href": "index.html#recent-experience",
    "title": "Glenn Matlin",
    "section": "Recent Experience",
    "text": "Recent Experience\n\nI’ve had the privilege of working with leading organizations including Change Healthcare, Komodo Health, LendUp, and RichRelevance, where I’ve contributed to innovative solutions in healthcare technology, financial services, and personalization systems.\n\n\n\nLet’s Connect\nI’m always interested in discussing new opportunities, collaborations, and interesting technical challenges.\nGet in Touch Download Resume"
  },
  {
    "objectID": "project/flame/index.html",
    "href": "project/flame/index.html",
    "title": "FLAME - Financial Language Model Evaluation",
    "section": "",
    "text": "FLAME is a comprehensive evaluation framework specifically designed to assess large language models (LLMs) in financial contexts. As LLMs continue to gain adoption in financial services, there is an increasing need for specialized benchmarks that can measure their capabilities and limitations within this domain.\n\n\nFLAME provides:\n\nDomain-Specific Benchmarking: A benchmark suite that specifically targets financial knowledge, reasoning, and compliance capabilities.\nMulti-Dimensional Evaluation: Assessment across various financial sub-domains including:\n\nBanking regulations and compliance\nInvestment analysis and portfolio management\nFinancial reporting and accounting standards\nRisk assessment and management\nMarket analysis and economic forecasting\n\nStandardized Methodology for:\n\nCreating consistent evaluation scenarios\nEstablishing clear metrics for comparison\nControlling for evaluation biases\nEnsuring reproducible results\n\nModular Architecture supporting:\n\nExtensibility to new models and tasks\nCustomization for specific research questions\nIntegration with existing benchmarking tools\nAutomated evaluation pipelines\n\n\n\n\n\n\n\nThe framework includes specialized task suites for comprehensive evaluation in the financial domain:\n\nKnowledge Suite: Factual recall across financial concepts, regulations, and historical market events.\nReasoning Suite: Evaluates the model’s ability to perform financial calculations, analyze market trends, and make sound investment recommendations.\nCompliance Suite: Measures the model’s understanding of financial regulations and ability to identify potential compliance issues.\nEthics Suite: Assesses how models handle ethically complex financial scenarios and whether they promote responsible financial practices.\nRobustness Suite: Tests for maintaining performance under input perturbations, context length variations, and adversarial examples designed to test reliability in high-stakes financial decisions.\n\n\n\n\nFLAME implements a range of metrics beyond simple accuracy:\n\nCalibration Score: Measures alignment between model confidence and correctness\nConsistency Index: Evaluates reliability of responses across similar financial queries\nError Analysis: Categorizes and quantifies different types of model failures in financial contexts\nBias Detection: Identifies systematic patterns of errors across demographic or financial topic dimensions\nHallucination Rate: Measures frequency and severity of factual inaccuracies in financial information\n\n\n\n\n\nThis evaluation framework supports ongoing research in:\n\nModel development - Identifying weaknesses in financial understanding to guide architecture improvements\nFine-tuning optimization - Benchmarking different approaches to model adaptation for financial applications\nPrompt engineering - Testing effectiveness of different prompting strategies for financial tasks\nSafety research - Assessing vulnerabilities to misuse and harmful outputs in high-stakes financial contexts\nDomain adaptation - Measuring performance gaps in specialized financial fields\n\n\n\n\nThe FLAME framework is currently in development phase, with the initial benchmark suite being created and validated by financial domain experts. We are actively seeking collaboration with financial institutions and research organizations interested in contributing to this important evaluation framework. Initial benchmarks have been established for several popular open-source and commercial LLMs, with ongoing work focused on expanding the evaluation suites and developing standardized reporting formats."
  },
  {
    "objectID": "project/flame/index.html#flame-financial-language-model-evaluation-framework",
    "href": "project/flame/index.html#flame-financial-language-model-evaluation-framework",
    "title": "FLAME - Financial Language Model Evaluation",
    "section": "",
    "text": "FLAME is a comprehensive evaluation framework specifically designed to assess large language models (LLMs) in financial contexts. As LLMs continue to gain adoption in financial services, there is an increasing need for specialized benchmarks that can measure their capabilities and limitations within this domain.\n\n\nFLAME provides:\n\nDomain-Specific Benchmarking: A benchmark suite that specifically targets financial knowledge, reasoning, and compliance capabilities.\nMulti-Dimensional Evaluation: Assessment across various financial sub-domains including:\n\nBanking regulations and compliance\nInvestment analysis and portfolio management\nFinancial reporting and accounting standards\nRisk assessment and management\nMarket analysis and economic forecasting\n\nStandardized Methodology for:\n\nCreating consistent evaluation scenarios\nEstablishing clear metrics for comparison\nControlling for evaluation biases\nEnsuring reproducible results\n\nModular Architecture supporting:\n\nExtensibility to new models and tasks\nCustomization for specific research questions\nIntegration with existing benchmarking tools\nAutomated evaluation pipelines\n\n\n\n\n\n\n\nThe framework includes specialized task suites for comprehensive evaluation in the financial domain:\n\nKnowledge Suite: Factual recall across financial concepts, regulations, and historical market events.\nReasoning Suite: Evaluates the model’s ability to perform financial calculations, analyze market trends, and make sound investment recommendations.\nCompliance Suite: Measures the model’s understanding of financial regulations and ability to identify potential compliance issues.\nEthics Suite: Assesses how models handle ethically complex financial scenarios and whether they promote responsible financial practices.\nRobustness Suite: Tests for maintaining performance under input perturbations, context length variations, and adversarial examples designed to test reliability in high-stakes financial decisions.\n\n\n\n\nFLAME implements a range of metrics beyond simple accuracy:\n\nCalibration Score: Measures alignment between model confidence and correctness\nConsistency Index: Evaluates reliability of responses across similar financial queries\nError Analysis: Categorizes and quantifies different types of model failures in financial contexts\nBias Detection: Identifies systematic patterns of errors across demographic or financial topic dimensions\nHallucination Rate: Measures frequency and severity of factual inaccuracies in financial information\n\n\n\n\n\nThis evaluation framework supports ongoing research in:\n\nModel development - Identifying weaknesses in financial understanding to guide architecture improvements\nFine-tuning optimization - Benchmarking different approaches to model adaptation for financial applications\nPrompt engineering - Testing effectiveness of different prompting strategies for financial tasks\nSafety research - Assessing vulnerabilities to misuse and harmful outputs in high-stakes financial contexts\nDomain adaptation - Measuring performance gaps in specialized financial fields\n\n\n\n\nThe FLAME framework is currently in development phase, with the initial benchmark suite being created and validated by financial domain experts. We are actively seeking collaboration with financial institutions and research organizations interested in contributing to this important evaluation framework. Initial benchmarks have been established for several popular open-source and commercial LLMs, with ongoing work focused on expanding the evaluation suites and developing standardized reporting formats."
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html",
    "href": "knowledgebase/cheatsheets/marp.html",
    "title": "Directives",
    "section": "",
    "text": "Marpit Markdown uses extended syntax called “Directives” to support writing awesome slides. It can control your slide-deck theme, page number, header, footer, style, and so on.\n\n\nThe written directives would parse as YAML.\nWhen the value includes YAML special chars, you should wrap with quotes to be recognized correctly. You may enable a loose parsing by looseYAML Marpit constructor option if you want.\n\n\n&lt;!--\ntheme: default\npaginate: true\n--&gt;\n?&gt; The HTML comment is also used for presenter notes. When it is parsed as a directive, it would not be collected in the comments result of Marpit.render().\n\n\n\nMarpit also supports YAML front-matter, that is a syntax often used for keeping metadata of Markdown. It must be the first thing of Markdown, and between the dash rulers.\n---\ntheme: default\npaginate: true\n---\nPlease not confuse to the ruler for paging slides. The actual slide contents would start after the ending ruler of front-matter.\n\n\n\n\n\n\nGlobal directives are the setting value of the whole slide deck such as theme. Marpit recognizes only the last value if you wrote a same global directives many times.\n!&gt; $ prefix for global directives has removed in v1.4.0. Developer may re-define dollar-prefixed custom directives as an alias to built-in directive if necessary.\n\n\n\nLocal directives are the setting value per slide pages. These would apply to defined page and following pages.\n&lt;!-- backgroundColor: aqua --&gt;\n\nThis page has aqua background.\n\n---\n\nThe second page also has same color.\n\n\nIf you want to apply local directives only to the current page, you have to add the prefix _ to the name of directives.\n&lt;!-- _backgroundColor: aqua --&gt;\n\nAdd underscore prefix `_` to the name of local directives.\n\n---\n\nThe second page would not apply setting of directives.\n\n\n\n\n\n\n\nDirectives Diagram\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nheadingDivider\nSpecify heading divider option.\n\n\nlang\nSet the value of lang attribute for each slide\n\n\nstyle\nSpecify CSS for tweaking theme.\n\n\ntheme\nSpecify theme of the slide deck.\n\n\n\n\n\nChoose a theme with theme global directive.\n&lt;!-- theme: registered-theme-name --&gt;\nIt recognizes the name of theme added to themeSet of Marpit instance.\n\n\nNormally you may tweak theme by &lt;style&gt; element, but it might break a style for documentation when opening in another Markdown editor. Thus you can use style global directive instead of &lt;style&gt;.\n---\ntheme: base-theme\nstyle: |\n  section {\n    background-color: #ccc;\n  }\n---\n\n\n\n\nYou may instruct to divide slide pages automatically at before of headings by using headingDivider global directive. This feature is similar to Pandoc’s --slide-level option and Deckset 2’s “Slide Dividers” option.\nIt have to specify heading level from 1 to 6, or array of them. This feature is enabled at headings whose the level larger than or equal to the specified value if in a number, and it is enabled at only specified levels if in array.\nFor example, the below two Markdowns have the same output.\n\n\n# 1st page\n\nThe content of 1st page\n\n---\n\n## 2nd page\n\n### The content of 2nd page\n\nHello, world!\n\n---\n\n# 3rd page\n\n😃\n\n\n\n&lt;!-- headingDivider: 2 --&gt;\n\n# 1st page\n\nThe content of 1st page\n\n## 2nd page\n\n### The content of 2nd page\n\nHello, world!\n\n# 3rd page\n\n😃\nIt is useful when you want to create a slide deck from a plain Markdown. Even if you opened Markdown that is using headingDivider in general editor, it keeps a beautiful rendering with no unsightly rulers.\n?&gt; Marpit constructor can set a default level of heading divider.\n\n\n\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npaginate\nShow page number on the slide if you set true.\n\n\nheader\nSpecify the content of slide header.\n\n\nfooter\nSpecify the content of slide footer.\n\n\nclass\nSpecify HTML class of slide’s &lt;section&gt; element.\n\n\nbackgroundColor\nSetting background-color style of slide.\n\n\nbackgroundImage\nSetting background-image style of slide.\n\n\nbackgroundPosition\nSetting background-position style of slide.\n\n\nbackgroundRepeat\nSetting background-repeat style of slide.\n\n\nbackgroundSize\nSetting background-size style of slide.\n\n\ncolor\nSetting color style of slide.\n\n\n\n\n\nWe support pagination by the paginate local directive.\n&lt;!-- paginate: true --&gt;\n\nYou would be able to see a page number of slide in the lower right.\n\n\nThere are 2 things happening on each slide:\n\nthe page number is rendered and\nthe page number is being incremented.\n\nYou can control both of these with the paginate directive:\n\n\n\npaginate\nPage number\nIncrement\n\n\n\n\ntrue\nShow\nYes\n\n\nfalse\nHide\nYes\n\n\nhold\nShow\nNo\n\n\nskip\nHide\nNo\n\n\n\n\n\n\nA common use case is excluding the title slide from pagination. For this you simply have to define the paginate directive on the second page instead of the first.\n# Title slide\n\nThis page will not have pagination by lack of the `paginate` directive.\n\n---\n\n&lt;!-- paginate: true --&gt;\n\nPagination will render from this slide onwards (starting at 2).\nOr you can use the spot directive.\n---\npaginate: true\n_paginate: false # or use `_paginate: skip`\n---\n\n\n\nTo both exclude a page from pagination and hide the pagination at the same time use skip:\n&lt;!-- _paginate: skip --&gt;\n\n# Slide to exclude\n\nThis page will not update the page number and also not show the pagination\nYou can exclude a page from pagination but keep the pagination visible using hold:\n---\npaginate: true\n---\n\n# Slide 1\n\n![Image Example 1](https://raw.githubusercontent.com/marp-team/marp/main/website/assets/marp.png)\n\n&gt; Page 1 of 1\n\n---\n\n&lt;!-- _paginate: hold --&gt;\n\n# Slide 2\n\n![Image Example 2](https://raw.githubusercontent.com/marp-team/marp/main/website/assets/marp.png)\n\n&gt; Page 1 of 1\n\n\n\n\nWhen you have to be shown the same content across multiple slides like a title of the slide deck, you may use header or footer local directives.\n---\nheader: 'Header content'\nfooter: 'Footer content'\n---\n\n# Page 1\n\n---\n\n## Page 2\nIt will render to HTML like this:\n&lt;section&gt;\n  &lt;header&gt;Header content&lt;/header&gt;\n  &lt;h1&gt;Page 1&lt;/h1&gt;\n  &lt;footer&gt;Footer content&lt;/footer&gt;\n&lt;/section&gt;\n&lt;section&gt;\n  &lt;header&gt;Header content&lt;/header&gt;\n  &lt;h2&gt;Page 2&lt;/h2&gt;\n  &lt;footer&gt;Footer content&lt;/footer&gt;\n&lt;/section&gt;\nThe content will be wrapped by a corresponding element, and insert to a right place of each slide. These could see as the part of slide contents.\nIf you want to place these contents to the marginal of the slide as like as PowerPoint, you have to use supported theme.\n\n\nIn addition, you can format the content of header/footer through markdown syntax and insert inline images.\n---\nheader: '**bold** _italic_'\nfooter: '![image](https://example.com/image.jpg)'\n---\n\nNOTE: Wrap by (double-)quotes to avoid parsed as invalid YAML.\n?&gt; You cannot use ![bg]() syntax in header and footer directives due to the parsing order of Markdown.\n\n\n\n\n\n\nAt the some page, you might think want to change the layout, theme color, and so on. class local directive can change a class attribute of &lt;section&gt; element of slide page.\nLet’s say you’re using a theme includes a rule like this:\nsection.lead h1 {\n  text-align: center;\n}\nYou could use the centered leading header by setting class spot directive to lead.\n&lt;!-- _class: lead --&gt;\n\n# THE LEADING HEADER\n\n\n\nIf you want to use any color or the gradient as background, you can set style through backgroundColor or backgroundImage local directives.\n&lt;!-- backgroundImage: \"linear-gradient(to bottom, #67b8e3, #0288d1)\" --&gt;\n\nGradient background\n\n---\n\n&lt;!--\n_backgroundColor: black\n_color: white\n--&gt;\n\nBlack background + White text\nIn addition, we have supported customize for these declarations:\n\nbackgroundColor\nbackgroundImage\nbackgroundPosition (center by default)\nbackgroundRepeat (no-repeat by default)\nbackgroundSize (cover by default)\ncolor\n\n?&gt; It also can use extended image syntax if you want to set image or color as background to single page.\n\n\n\n\n\n\n\nDeveloper can extend recognizable directives. For example, Marp Core has extended size global directive to change slide size in Markdown. Marp CLI will add directives for setting meta properties of converted HTML.\nMarpit instance has customDirectives.global and customDirectives.local object to allow adding directives as you like.\n\n\nThe following example is defining dollar-prefixed alias of built-in theme global directive.\nmarpit.customDirectives.global.$theme = (value, marpit) =&gt; {\n  return { theme: value }\n}\nPlease define a function to handle passed value from Markdown. The first argument is the passed value(s), and the second is the current Marpit instance. It should return an object includes pairs of key-value for passing to same kind directives.\n\n\n\nCustom directives also can provide a way of assigning multiple same kind directives at once. Let’s define colorPreset local directive for assigning preset of slide colors.\nmarpit.customDirectives.local.colorPreset = (value, marpit) =&gt; {\n  switch (value) {\n    case 'sunset':\n      return { backgroundColor: '#e62e00', color: '#fffff2' }\n    case 'dark':\n      return { backgroundColor: '#303033', color: '#f8f8ff' }\n    default:\n      // Return an empty object if not have to assign new values\n      return {}\n  }\n}\nNow you can use the defined colorPreset local directive with same way of built-in local directives. The underscore prefix (_colorPreset) for applying preset to single slide also works well.\n&lt;!-- colorPreset: sunset --&gt;\n\n# Sunset color preset\n\n---\n\n&lt;!-- _colorPreset: dark --&gt;\n\n# Dark color preset\n\n---\n\n# Sunset color preset\n?&gt; The returned key-value will assign to marpitDirectives property in meta object of predetermined markdown-it token(s) by the kind of directive. It would be useful for using assigned value in markdown-it plugin. # Image syntax\nMarpit has extended Markdown image syntax ![](image.jpg) to be helpful creating beautiful slides.\n\n\n\nFeatures\nInline image\nSlide BG\nAdvanced BG\n\n\n\n\nResizing by keywords\nauto only\n:heavy_check_mark:\n:heavy_check_mark:\n\n\nResizing by percentage\n:x:\n:heavy_check_mark:\n:heavy_check_mark:\n\n\nResizing by length\n:heavy_check_mark:\n:heavy_check_mark:\n:heavy_check_mark:\n\n\nImage filters\n:heavy_check_mark:\n:x:\n:heavy_check_mark:\n\n\nMultiple backgrounds\n-\n:x:\n:heavy_check_mark:\n\n\nSplit backgrounds\n-\n:x:\n:heavy_check_mark:\n\n\n\nBasically the extended features can turn enable by including corresponded keywords to the image’s alternative text.\n\n\n\n\nYou can resize image by using width and height keyword options.\n![width:200px](image.jpg) &lt;!-- Setting width to 200px --&gt;\n![height:30cm](image.jpg) &lt;!-- Setting height to 300px --&gt;\n![width:200px height:30cm](image.jpg) &lt;!-- Setting both lengths --&gt;\nWe also support the shorthand options w and h. Normally it’s useful to use these.\n![w:32 h:32](image.jpg) &lt;!-- Setting size to 32x32 px --&gt;\nInline images only allow auto keyword and the length units defined in CSS.\n!&gt; Several units related to the size of the viewport (e.g. vw, vh, vmin, vmax) cannot use to ensure immutable render result.\n\n\n\nYou can apply CSS filters to image through markdown image syntax. Include &lt;filter-name&gt;(:&lt;param&gt;(,&lt;param&gt;...)) to the alternate text of image.\nFilters can use in the inline image and the advanced backgrounds.\n\n\n\nMarkdown\nw/ arguments\n\n\n\n\n![blur]()\n![blur:10px]()\n\n\n![brightness]()\n![brightness:1.5]()\n\n\n![contrast]()\n![contrast:200%]()\n\n\n![drop-shadow]()\n![drop-shadow:0,5px,10px,rgba(0,0,0,.4)]()\n\n\n![grayscale]()\n![grayscale:1]()\n\n\n![hue-rotate]()\n![hue-rotate:180deg]()\n\n\n![invert]()\n![invert:100%]()\n\n\n![opacity]()\n![opacity:.5]()\n\n\n![saturate]()\n![saturate:2.0]()\n\n\n![sepia]()\n![sepia:1.0]()\n\n\n\nMarpit will use the default arguments shown in above when you omit arguments.\nNaturally multiple filters can apply to a image.\n![brightness:.8 sepia:50%](https://example.com/image.jpg)\n\n\n\n\nWe provide a background image syntax to specify slide’s background through Markdown. It only have to include bg keyword to the alternate text.\n![bg](https://example.com/background.jpg)\nWhen you defined two or more background images in a slide, Marpit will show the last defined image only. If you want to show multiple images, try the advanced backgrounds by enabling inline SVG slide.\n\n\nYou can resize the background image by keywords. The keyword value basically follows background-size style.\n![bg contain](https://example.com/background.jpg)\n\n\n\n\n\n\n\n\nKeyword\nDescription\nExample\n\n\n\n\ncover\nScale image to fill the slide. (Default)\n![bg cover](image.jpg)\n\n\ncontain\nScale image to fit the slide.\n![bg contain](image.jpg)\n\n\nfit\nAlias to contain, compatible with Deckset.\n![bg fit](image.jpg)\n\n\nauto\nNot scale image, and use the original size.\n![bg auto](image.jpg)\n\n\nx%\nSpecify the scaling factor by percentage value.\n![bg 150%](image.jpg)\n\n\n\nYou also can continue to use width (w) and height (h) option keywords to specify size by length.\n\n\n\n\n!&gt; 📐 It will work only in experimental inline SVG slide.\nThe advanced backgrounds support multiple backgrounds, split backgrounds, and image filters for background.\n\n\n\n![bg](https://fakeimg.pl/800x600/0288d1/fff/?text=A)\n![bg](https://fakeimg.pl/800x600/02669d/fff/?text=B)\n![bg](https://fakeimg.pl/800x600/67b8e3/fff/?text=C)\n\n\n\n\nMultiple backgrounds\n\n\n\n\nThese images will arrange in a horizontal row.\n\n\nYou may change alignment direction from horizontal to vertical, by using vertical direction keyword.\n\n![bg vertical](https://fakeimg.pl/800x600/0288d1/fff/?text=A)\n![bg](https://fakeimg.pl/800x600/02669d/fff/?text=B)\n![bg](https://fakeimg.pl/800x600/67b8e3/fff/?text=C)\n\n\n\n\nMultiple backgrounds with vertical direction\n\n\n\n\n\n\n\n\nThe left or right keyword with bg keyword make a space for the background to the specified side. It has a half of slide size, and the space of a slide content will shrink too.\n\n![bg left](https://picsum.photos/720?image=29)\n\n# Split backgrounds\n\nThe space of a slide content will shrink to the right side.\n\n\n\n\nMultiple backgrounds will work well in the specified background side.\n\n![bg right](https://picsum.photos/720?image=3)\n![bg](https://picsum.photos/720?image=20)\n\n# Split + Multiple BGs\n\nThe space of a slide content will shrink to the left side.\n\n\n\n\nThis feature is similar to Deckset’s Split Slides.\n?&gt; Marpit uses a last defined keyword in a slide when left and right keyword is mixed in the same slide by using multiple backgrounds.\n\n\nMarpit can specify split size for background by percentage like left:33%.\n\n![bg left:33%](https://picsum.photos/720?image=27)\n\n# Split backgrounds with specified size"
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html#usage",
    "href": "knowledgebase/cheatsheets/marp.html#usage",
    "title": "Directives",
    "section": "",
    "text": "The written directives would parse as YAML.\nWhen the value includes YAML special chars, you should wrap with quotes to be recognized correctly. You may enable a loose parsing by looseYAML Marpit constructor option if you want.\n\n\n&lt;!--\ntheme: default\npaginate: true\n--&gt;\n?&gt; The HTML comment is also used for presenter notes. When it is parsed as a directive, it would not be collected in the comments result of Marpit.render().\n\n\n\nMarpit also supports YAML front-matter, that is a syntax often used for keeping metadata of Markdown. It must be the first thing of Markdown, and between the dash rulers.\n---\ntheme: default\npaginate: true\n---\nPlease not confuse to the ruler for paging slides. The actual slide contents would start after the ending ruler of front-matter."
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html#type-of-directives",
    "href": "knowledgebase/cheatsheets/marp.html#type-of-directives",
    "title": "Directives",
    "section": "",
    "text": "Global directives are the setting value of the whole slide deck such as theme. Marpit recognizes only the last value if you wrote a same global directives many times.\n!&gt; $ prefix for global directives has removed in v1.4.0. Developer may re-define dollar-prefixed custom directives as an alias to built-in directive if necessary.\n\n\n\nLocal directives are the setting value per slide pages. These would apply to defined page and following pages.\n&lt;!-- backgroundColor: aqua --&gt;\n\nThis page has aqua background.\n\n---\n\nThe second page also has same color.\n\n\nIf you want to apply local directives only to the current page, you have to add the prefix _ to the name of directives.\n&lt;!-- _backgroundColor: aqua --&gt;\n\nAdd underscore prefix `_` to the name of local directives.\n\n---\n\nThe second page would not apply setting of directives.\n\n\n\n\n\n\n\nDirectives Diagram"
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html#global-directives",
    "href": "knowledgebase/cheatsheets/marp.html#global-directives",
    "title": "Directives",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nheadingDivider\nSpecify heading divider option.\n\n\nlang\nSet the value of lang attribute for each slide\n\n\nstyle\nSpecify CSS for tweaking theme.\n\n\ntheme\nSpecify theme of the slide deck.\n\n\n\n\n\nChoose a theme with theme global directive.\n&lt;!-- theme: registered-theme-name --&gt;\nIt recognizes the name of theme added to themeSet of Marpit instance.\n\n\nNormally you may tweak theme by &lt;style&gt; element, but it might break a style for documentation when opening in another Markdown editor. Thus you can use style global directive instead of &lt;style&gt;.\n---\ntheme: base-theme\nstyle: |\n  section {\n    background-color: #ccc;\n  }\n---\n\n\n\n\nYou may instruct to divide slide pages automatically at before of headings by using headingDivider global directive. This feature is similar to Pandoc’s --slide-level option and Deckset 2’s “Slide Dividers” option.\nIt have to specify heading level from 1 to 6, or array of them. This feature is enabled at headings whose the level larger than or equal to the specified value if in a number, and it is enabled at only specified levels if in array.\nFor example, the below two Markdowns have the same output.\n\n\n# 1st page\n\nThe content of 1st page\n\n---\n\n## 2nd page\n\n### The content of 2nd page\n\nHello, world!\n\n---\n\n# 3rd page\n\n😃\n\n\n\n&lt;!-- headingDivider: 2 --&gt;\n\n# 1st page\n\nThe content of 1st page\n\n## 2nd page\n\n### The content of 2nd page\n\nHello, world!\n\n# 3rd page\n\n😃\nIt is useful when you want to create a slide deck from a plain Markdown. Even if you opened Markdown that is using headingDivider in general editor, it keeps a beautiful rendering with no unsightly rulers.\n?&gt; Marpit constructor can set a default level of heading divider."
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html#local-directives",
    "href": "knowledgebase/cheatsheets/marp.html#local-directives",
    "title": "Directives",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npaginate\nShow page number on the slide if you set true.\n\n\nheader\nSpecify the content of slide header.\n\n\nfooter\nSpecify the content of slide footer.\n\n\nclass\nSpecify HTML class of slide’s &lt;section&gt; element.\n\n\nbackgroundColor\nSetting background-color style of slide.\n\n\nbackgroundImage\nSetting background-image style of slide.\n\n\nbackgroundPosition\nSetting background-position style of slide.\n\n\nbackgroundRepeat\nSetting background-repeat style of slide.\n\n\nbackgroundSize\nSetting background-size style of slide.\n\n\ncolor\nSetting color style of slide.\n\n\n\n\n\nWe support pagination by the paginate local directive.\n&lt;!-- paginate: true --&gt;\n\nYou would be able to see a page number of slide in the lower right.\n\n\nThere are 2 things happening on each slide:\n\nthe page number is rendered and\nthe page number is being incremented.\n\nYou can control both of these with the paginate directive:\n\n\n\npaginate\nPage number\nIncrement\n\n\n\n\ntrue\nShow\nYes\n\n\nfalse\nHide\nYes\n\n\nhold\nShow\nNo\n\n\nskip\nHide\nNo\n\n\n\n\n\n\nA common use case is excluding the title slide from pagination. For this you simply have to define the paginate directive on the second page instead of the first.\n# Title slide\n\nThis page will not have pagination by lack of the `paginate` directive.\n\n---\n\n&lt;!-- paginate: true --&gt;\n\nPagination will render from this slide onwards (starting at 2).\nOr you can use the spot directive.\n---\npaginate: true\n_paginate: false # or use `_paginate: skip`\n---\n\n\n\nTo both exclude a page from pagination and hide the pagination at the same time use skip:\n&lt;!-- _paginate: skip --&gt;\n\n# Slide to exclude\n\nThis page will not update the page number and also not show the pagination\nYou can exclude a page from pagination but keep the pagination visible using hold:\n---\npaginate: true\n---\n\n# Slide 1\n\n![Image Example 1](https://raw.githubusercontent.com/marp-team/marp/main/website/assets/marp.png)\n\n&gt; Page 1 of 1\n\n---\n\n&lt;!-- _paginate: hold --&gt;\n\n# Slide 2\n\n![Image Example 2](https://raw.githubusercontent.com/marp-team/marp/main/website/assets/marp.png)\n\n&gt; Page 1 of 1\n\n\n\n\nWhen you have to be shown the same content across multiple slides like a title of the slide deck, you may use header or footer local directives.\n---\nheader: 'Header content'\nfooter: 'Footer content'\n---\n\n# Page 1\n\n---\n\n## Page 2\nIt will render to HTML like this:\n&lt;section&gt;\n  &lt;header&gt;Header content&lt;/header&gt;\n  &lt;h1&gt;Page 1&lt;/h1&gt;\n  &lt;footer&gt;Footer content&lt;/footer&gt;\n&lt;/section&gt;\n&lt;section&gt;\n  &lt;header&gt;Header content&lt;/header&gt;\n  &lt;h2&gt;Page 2&lt;/h2&gt;\n  &lt;footer&gt;Footer content&lt;/footer&gt;\n&lt;/section&gt;\nThe content will be wrapped by a corresponding element, and insert to a right place of each slide. These could see as the part of slide contents.\nIf you want to place these contents to the marginal of the slide as like as PowerPoint, you have to use supported theme.\n\n\nIn addition, you can format the content of header/footer through markdown syntax and insert inline images.\n---\nheader: '**bold** _italic_'\nfooter: '![image](https://example.com/image.jpg)'\n---\n\nNOTE: Wrap by (double-)quotes to avoid parsed as invalid YAML.\n?&gt; You cannot use ![bg]() syntax in header and footer directives due to the parsing order of Markdown.\n\n\n\n\n\n\nAt the some page, you might think want to change the layout, theme color, and so on. class local directive can change a class attribute of &lt;section&gt; element of slide page.\nLet’s say you’re using a theme includes a rule like this:\nsection.lead h1 {\n  text-align: center;\n}\nYou could use the centered leading header by setting class spot directive to lead.\n&lt;!-- _class: lead --&gt;\n\n# THE LEADING HEADER\n\n\n\nIf you want to use any color or the gradient as background, you can set style through backgroundColor or backgroundImage local directives.\n&lt;!-- backgroundImage: \"linear-gradient(to bottom, #67b8e3, #0288d1)\" --&gt;\n\nGradient background\n\n---\n\n&lt;!--\n_backgroundColor: black\n_color: white\n--&gt;\n\nBlack background + White text\nIn addition, we have supported customize for these declarations:\n\nbackgroundColor\nbackgroundImage\nbackgroundPosition (center by default)\nbackgroundRepeat (no-repeat by default)\nbackgroundSize (cover by default)\ncolor\n\n?&gt; It also can use extended image syntax if you want to set image or color as background to single page."
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html#advanced",
    "href": "knowledgebase/cheatsheets/marp.html#advanced",
    "title": "Directives",
    "section": "",
    "text": "Developer can extend recognizable directives. For example, Marp Core has extended size global directive to change slide size in Markdown. Marp CLI will add directives for setting meta properties of converted HTML.\nMarpit instance has customDirectives.global and customDirectives.local object to allow adding directives as you like.\n\n\nThe following example is defining dollar-prefixed alias of built-in theme global directive.\nmarpit.customDirectives.global.$theme = (value, marpit) =&gt; {\n  return { theme: value }\n}\nPlease define a function to handle passed value from Markdown. The first argument is the passed value(s), and the second is the current Marpit instance. It should return an object includes pairs of key-value for passing to same kind directives.\n\n\n\nCustom directives also can provide a way of assigning multiple same kind directives at once. Let’s define colorPreset local directive for assigning preset of slide colors.\nmarpit.customDirectives.local.colorPreset = (value, marpit) =&gt; {\n  switch (value) {\n    case 'sunset':\n      return { backgroundColor: '#e62e00', color: '#fffff2' }\n    case 'dark':\n      return { backgroundColor: '#303033', color: '#f8f8ff' }\n    default:\n      // Return an empty object if not have to assign new values\n      return {}\n  }\n}\nNow you can use the defined colorPreset local directive with same way of built-in local directives. The underscore prefix (_colorPreset) for applying preset to single slide also works well.\n&lt;!-- colorPreset: sunset --&gt;\n\n# Sunset color preset\n\n---\n\n&lt;!-- _colorPreset: dark --&gt;\n\n# Dark color preset\n\n---\n\n# Sunset color preset\n?&gt; The returned key-value will assign to marpitDirectives property in meta object of predetermined markdown-it token(s) by the kind of directive. It would be useful for using assigned value in markdown-it plugin. # Image syntax\nMarpit has extended Markdown image syntax ![](image.jpg) to be helpful creating beautiful slides.\n\n\n\nFeatures\nInline image\nSlide BG\nAdvanced BG\n\n\n\n\nResizing by keywords\nauto only\n:heavy_check_mark:\n:heavy_check_mark:\n\n\nResizing by percentage\n:x:\n:heavy_check_mark:\n:heavy_check_mark:\n\n\nResizing by length\n:heavy_check_mark:\n:heavy_check_mark:\n:heavy_check_mark:\n\n\nImage filters\n:heavy_check_mark:\n:x:\n:heavy_check_mark:\n\n\nMultiple backgrounds\n-\n:x:\n:heavy_check_mark:\n\n\nSplit backgrounds\n-\n:x:\n:heavy_check_mark:\n\n\n\nBasically the extended features can turn enable by including corresponded keywords to the image’s alternative text.\n\n\n\n\nYou can resize image by using width and height keyword options.\n![width:200px](image.jpg) &lt;!-- Setting width to 200px --&gt;\n![height:30cm](image.jpg) &lt;!-- Setting height to 300px --&gt;\n![width:200px height:30cm](image.jpg) &lt;!-- Setting both lengths --&gt;\nWe also support the shorthand options w and h. Normally it’s useful to use these.\n![w:32 h:32](image.jpg) &lt;!-- Setting size to 32x32 px --&gt;\nInline images only allow auto keyword and the length units defined in CSS.\n!&gt; Several units related to the size of the viewport (e.g. vw, vh, vmin, vmax) cannot use to ensure immutable render result.\n\n\n\nYou can apply CSS filters to image through markdown image syntax. Include &lt;filter-name&gt;(:&lt;param&gt;(,&lt;param&gt;...)) to the alternate text of image.\nFilters can use in the inline image and the advanced backgrounds.\n\n\n\nMarkdown\nw/ arguments\n\n\n\n\n![blur]()\n![blur:10px]()\n\n\n![brightness]()\n![brightness:1.5]()\n\n\n![contrast]()\n![contrast:200%]()\n\n\n![drop-shadow]()\n![drop-shadow:0,5px,10px,rgba(0,0,0,.4)]()\n\n\n![grayscale]()\n![grayscale:1]()\n\n\n![hue-rotate]()\n![hue-rotate:180deg]()\n\n\n![invert]()\n![invert:100%]()\n\n\n![opacity]()\n![opacity:.5]()\n\n\n![saturate]()\n![saturate:2.0]()\n\n\n![sepia]()\n![sepia:1.0]()\n\n\n\nMarpit will use the default arguments shown in above when you omit arguments.\nNaturally multiple filters can apply to a image.\n![brightness:.8 sepia:50%](https://example.com/image.jpg)"
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html#slide-backgrounds",
    "href": "knowledgebase/cheatsheets/marp.html#slide-backgrounds",
    "title": "Directives",
    "section": "",
    "text": "We provide a background image syntax to specify slide’s background through Markdown. It only have to include bg keyword to the alternate text.\n![bg](https://example.com/background.jpg)\nWhen you defined two or more background images in a slide, Marpit will show the last defined image only. If you want to show multiple images, try the advanced backgrounds by enabling inline SVG slide.\n\n\nYou can resize the background image by keywords. The keyword value basically follows background-size style.\n![bg contain](https://example.com/background.jpg)\n\n\n\n\n\n\n\n\nKeyword\nDescription\nExample\n\n\n\n\ncover\nScale image to fill the slide. (Default)\n![bg cover](image.jpg)\n\n\ncontain\nScale image to fit the slide.\n![bg contain](image.jpg)\n\n\nfit\nAlias to contain, compatible with Deckset.\n![bg fit](image.jpg)\n\n\nauto\nNot scale image, and use the original size.\n![bg auto](image.jpg)\n\n\nx%\nSpecify the scaling factor by percentage value.\n![bg 150%](image.jpg)\n\n\n\nYou also can continue to use width (w) and height (h) option keywords to specify size by length."
  },
  {
    "objectID": "knowledgebase/cheatsheets/marp.html#advanced-backgrounds",
    "href": "knowledgebase/cheatsheets/marp.html#advanced-backgrounds",
    "title": "Directives",
    "section": "",
    "text": "!&gt; 📐 It will work only in experimental inline SVG slide.\nThe advanced backgrounds support multiple backgrounds, split backgrounds, and image filters for background.\n\n\n\n![bg](https://fakeimg.pl/800x600/0288d1/fff/?text=A)\n![bg](https://fakeimg.pl/800x600/02669d/fff/?text=B)\n![bg](https://fakeimg.pl/800x600/67b8e3/fff/?text=C)\n\n\n\n\nMultiple backgrounds\n\n\n\n\nThese images will arrange in a horizontal row.\n\n\nYou may change alignment direction from horizontal to vertical, by using vertical direction keyword.\n\n![bg vertical](https://fakeimg.pl/800x600/0288d1/fff/?text=A)\n![bg](https://fakeimg.pl/800x600/02669d/fff/?text=B)\n![bg](https://fakeimg.pl/800x600/67b8e3/fff/?text=C)\n\n\n\n\nMultiple backgrounds with vertical direction\n\n\n\n\n\n\n\n\nThe left or right keyword with bg keyword make a space for the background to the specified side. It has a half of slide size, and the space of a slide content will shrink too.\n\n![bg left](https://picsum.photos/720?image=29)\n\n# Split backgrounds\n\nThe space of a slide content will shrink to the right side.\n\n\n\n\nMultiple backgrounds will work well in the specified background side.\n\n![bg right](https://picsum.photos/720?image=3)\n![bg](https://picsum.photos/720?image=20)\n\n# Split + Multiple BGs\n\nThe space of a slide content will shrink to the left side.\n\n\n\n\nThis feature is similar to Deckset’s Split Slides.\n?&gt; Marpit uses a last defined keyword in a slide when left and right keyword is mixed in the same slide by using multiple backgrounds.\n\n\nMarpit can specify split size for background by percentage like left:33%.\n\n![bg left:33%](https://picsum.photos/720?image=27)\n\n# Split backgrounds with specified size"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html",
    "href": "knowledgebase/python/python3_logging_api.html",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "Source code: Lib/logging/__init__.py\n\nThis module defines functions and classes which implement a flexible event logging system for applications and libraries.\nThe key benefit of having the logging API provided by a standard library module is that all Python modules can participate in logging, so your application log can include your own messages integrated with messages from third-party modules.\nHere’s a simple example of idiomatic usage:\n# myapp.py\nimport logging\nimport mylib\nlogger = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(filename='myapp.log', level=logging.INFO)\n    logger.info('Started')\n    mylib.do_something()\n    logger.info('Finished')\n\nif __name__ == '__main__':\n    main()\n\n# mylib.py\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef do_something():\n    logger.info('Doing something')\n\nIf you run myapp.py, you should see this in myapp.log:\nINFO:__main__:Started\nINFO:mylib:Doing something\nINFO:__main__:Finished\n\nThe key feature of this idiomatic usage is that the majority of code is simply creating a module level logger with getLogger(__name__), and using that logger to do any needed logging. This is concise, while allowing downstream code fine-grained control if needed. Logged messages to the module-level logger get forwarded to handlers of loggers in higher-level modules, all the way up to the highest-level logger known as the root logger; this approach is known as hierarchical logging.\nFor logging to be useful, it needs to be configured: setting the levels and destinations for each logger, potentially changing how specific modules log, often based on command-line arguments or application configuration. In most cases, like the one above, only the root logger needs to be so configured, since all the lower level loggers at module level eventually forward their messages to its handlers. basicConfig() provides a quick way to configure the root logger that handles many use cases.\nThe module provides a lot of functionality and flexibility. If you are unfamiliar with logging, the best way to get to grips with it is to view the tutorials (see the links above and on the right).\nThe basic classes defined by the module, together with their attributes and methods, are listed in the sections below.\n\nLoggers expose the interface that application code directly uses.\nHandlers send the log records (created by loggers) to the appropriate destination.\nFilters provide a finer grained facility for determining which log records to output.\nFormatters specify the layout of log records in the final output.\n\n\n\nLoggers have the following attributes and methods. Note that Loggers should NEVER be instantiated directly, but always through the module-level function logging.getLogger(name). Multiple calls to getLogger() with the same name will always return a reference to the same Logger object.\nThe name is potentially a period-separated hierarchical value, like foo.bar.baz (though it could also be just plain foo, for example). Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo. In addition, all loggers are descendants of the root logger. The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you organise your loggers on a per-module basis using the recommended construction logging.getLogger(__name__). That’s because in a module, __name__ is the module’s name in the Python package namespace.\nclass logging.Logger¶\nname¶\nThis is the logger’s name, and is the value that was passed to getLogger() to obtain the logger.\nNote\nThis attribute should be treated as read-only.\nlevel¶\nThe threshold of this logger, as set by the setLevel() method.\nNote\nDo not set this attribute directly - always use setLevel(), which has checks for the level passed to it.\nparent¶\nThe parent logger of this logger. It may change based on later instantiation of loggers which are higher up in the namespace hierarchy.\nNote\nThis value should be treated as read-only.\npropagate¶\nIf this attribute evaluates to true, events logged to this logger will be passed to the handlers of higher level (ancestor) loggers, in addition to any handlers attached to this logger. Messages are passed directly to the ancestor loggers’ handlers - neither the level nor filters of the ancestor loggers in question are considered.\nIf this evaluates to false, logging messages are not passed to the handlers of ancestor loggers.\nSpelling it out with an example: If the propagate attribute of the logger named A.B.C evaluates to true, any event logged to A.B.C via a method call such as logging.getLogger('A.B.C').error(...) will [subject to passing that logger’s level and filter settings] be passed in turn to any handlers attached to loggers named A.B, A and the root logger, after first being passed to any handlers attached to A.B.C. If any logger in the chain A.B.C, A.B, A has its propagate attribute set to false, then that is the last logger whose handlers are offered the event to handle, and propagation stops at that point.\nThe constructor sets this attribute to True.\nNote\nIf you attach a handler to a logger and one or more of its ancestors, it may emit the same record multiple times. In general, you should not need to attach a handler to more than one logger - if you just attach it to the appropriate logger which is highest in the logger hierarchy, then it will see all events logged by all descendant loggers, provided that their propagate setting is left set to True. A common scenario is to attach handlers only to the root logger, and to let propagation take care of the rest.\nhandlers¶\nThe list of handlers directly attached to this logger instance.\nNote\nThis attribute should be treated as read-only; it is normally changed via the addHandler() and removeHandler() methods, which use locks to ensure thread-safe operation.\ndisabled¶\nThis attribute disables handling of any events. It is set to False in the initializer, and only changed by logging configuration code.\nNote\nThis attribute should be treated as read-only.\nsetLevel(level)¶\nSets the threshold for this logger to level. Logging messages which are less severe than level will be ignored; logging messages which have severity level or higher will be emitted by whichever handler or handlers service this logger, unless a handler’s level has been set to a higher severity level than level.\nWhen a logger is created, the level is set to NOTSET (which causes all messages to be processed when the logger is the root logger, or delegation to the parent when the logger is a non-root logger). Note that the root logger is created with level WARNING.\nThe term ‘delegation to the parent’ means that if a logger has a level of NOTSET, its chain of ancestor loggers is traversed until either an ancestor with a level other than NOTSET is found, or the root is reached.\nIf an ancestor is found with a level other than NOTSET, then that ancestor’s level is treated as the effective level of the logger where the ancestor search began, and is used to determine how a logging event is handled.\nIf the root is reached, and it has a level of NOTSET, then all messages will be processed. Otherwise, the root’s level will be used as the effective level.\nSee Logging Levels for a list of levels.\nChanged in version 3.2: The level parameter now accepts a string representation of the level such as ‘INFO’ as an alternative to the integer constants such as INFO. Note, however, that levels are internally stored as integers, and methods such as e.g. getEffectiveLevel() and isEnabledFor() will return/expect to be passed integers.\nisEnabledFor(level)¶\nIndicates if a message of severity level would be processed by this logger. This method checks first the module-level level set by logging.disable(level) and then the logger’s effective level as determined by getEffectiveLevel().\ngetEffectiveLevel()¶\nIndicates the effective level for this logger. If a value other than NOTSET has been set using setLevel(), it is returned. Otherwise, the hierarchy is traversed towards the root until a value other than NOTSET is found, and that value is returned. The value returned is an integer, typically one of logging.DEBUG, logging.INFO etc.\ngetChild(suffix)¶\nReturns a logger which is a descendant to this logger, as determined by the suffix. Thus, logging.getLogger('abc').getChild('def.ghi') would return the same logger as would be returned by logging.getLogger('abc.def.ghi'). This is a convenience method, useful when the parent logger is named using e.g. __name__ rather than a literal string.\nAdded in version 3.2.\ngetChildren()¶\nReturns a set of loggers which are immediate children of this logger. So for example logging.getLogger().getChildren() might return a set containing loggers named foo and bar, but a logger named foo.bar wouldn’t be included in the set. Likewise, logging.getLogger('foo').getChildren() might return a set including a logger named foo.bar, but it wouldn’t include one named foo.bar.baz.\nAdded in version 3.12.\ndebug(msg, *args, **kwargs)¶\nLogs a message with level DEBUG on this logger. The msg is the message format string, and the args are the arguments which are merged into msg using the string formatting operator. (Note that this means that you can use keywords in the format string, together with a single dictionary argument.) No % formatting operation is performed on msg when no args are supplied.\nThere are four keyword arguments in kwargs which are inspected: exc_info, stack_info, stacklevel and extra.\nIf exc_info does not evaluate as false, it causes exception information to be added to the logging message. If an exception tuple (in the format returned by sys.exc_info()) or an exception instance is provided, it is used; otherwise, sys.exc_info() is called to get the exception information.\nThe second optional keyword argument is stack_info, which defaults to False. If true, stack information is added to the logging message, including the actual logging call. Note that this is not the same stack information as that displayed through specifying exc_info: The former is stack frames from the bottom of the stack up to the logging call in the current thread, whereas the latter is information about stack frames which have been unwound, following an exception, while searching for exception handlers.\nYou can specify stack_info independently of exc_info, e.g. to just show how you got to a certain point in your code, even when no exceptions were raised. The stack frames are printed following a header line which says:\nStack (most recent call last):\n\nThis mimics the Traceback (most recent call last): which is used when displaying exception frames.\nThe third optional keyword argument is stacklevel, which defaults to 1. If greater than 1, the corresponding number of stack frames are skipped when computing the line number and function name set in the LogRecord created for the logging event. This can be used in logging helpers so that the function name, filename and line number recorded are not the information for the helper function/method, but rather its caller. The name of this parameter mirrors the equivalent one in the warnings module.\nThe fourth keyword argument is extra which can be used to pass a dictionary which is used to populate the __dict__ of the LogRecord created for the logging event with user-defined attributes. These custom attributes can then be used as you like. For example, they could be incorporated into logged messages. For example:\nFORMAT = '%(asctime)s %(clientip)-15s %(user)-8s %(message)s'\nlogging.basicConfig(format=FORMAT)\nd = {'clientip': '192.168.0.1', 'user': 'fbloggs'}\nlogger = logging.getLogger('tcpserver')\nlogger.warning('Protocol problem: %s', 'connection reset', extra=d)\n\nwould print something like\n2006-02-08 22:20:02,165 192.168.0.1 fbloggs  Protocol problem: connection reset\n\nThe keys in the dictionary passed in extra should not clash with the keys used by the logging system. (See the section on LogRecord attributes for more information on which keys are used by the logging system.)\nIf you choose to use these attributes in logged messages, you need to exercise some care. In the above example, for instance, the Formatter has been set up with a format string which expects ‘clientip’ and ‘user’ in the attribute dictionary of the LogRecord. If these are missing, the message will not be logged because a string formatting exception will occur. So in this case, you always need to pass the extra dictionary with these keys.\nWhile this might be annoying, this feature is intended for use in specialized circumstances, such as multi-threaded servers where the same code executes in many contexts, and interesting conditions which arise are dependent on this context (such as remote client IP address and authenticated user name, in the above example). In such circumstances, it is likely that specialized Formatters would be used with particular Handlers.\nIf no handler is attached to this logger (or any of its ancestors, taking into account the relevant Logger.propagate attributes), the message will be sent to the handler set on lastResort.\nChanged in version 3.2: The stack_info parameter was added.\nChanged in version 3.5: The exc_info parameter can now accept exception instances.\nChanged in version 3.8: The stacklevel parameter was added.\ninfo(msg, *args, **kwargs)¶\nLogs a message with level INFO on this logger. The arguments are interpreted as for debug().\nwarning(msg, *args, **kwargs)¶\nLogs a message with level WARNING on this logger. The arguments are interpreted as for debug().\nNote\nThere is an obsolete method warn which is functionally identical to warning. As warn is deprecated, please do not use it - use warning instead.\nerror(msg, *args, **kwargs)¶\nLogs a message with level ERROR on this logger. The arguments are interpreted as for debug().\ncritical(msg, *args, **kwargs)¶\nLogs a message with level CRITICAL on this logger. The arguments are interpreted as for debug().\nlog(level, msg, *args, **kwargs)¶\nLogs a message with integer level level on this logger. The other arguments are interpreted as for debug().\nexception(msg, *args, **kwargs)¶\nLogs a message with level ERROR on this logger. The arguments are interpreted as for debug(). Exception info is added to the logging message. This method should only be called from an exception handler.\naddFilter(filter)¶\nAdds the specified filter filter to this logger.\nremoveFilter(filter)¶\nRemoves the specified filter filter from this logger.\nfilter(record)¶\nApply this logger’s filters to the record and return True if the record is to be processed. The filters are consulted in turn, until one of them returns a false value. If none of them return a false value, the record will be processed (passed to handlers). If one returns a false value, no further processing of the record occurs.\naddHandler(hdlr)¶\nAdds the specified handler hdlr to this logger.\nremoveHandler(hdlr)¶\nRemoves the specified handler hdlr from this logger.\nfindCaller(stack_info=False, stacklevel=1)¶\nFinds the caller’s source filename and line number. Returns the filename, line number, function name and stack information as a 4-element tuple. The stack information is returned as None unless stack_info is True.\nThe stacklevel parameter is passed from code calling the debug() and other APIs. If greater than 1, the excess is used to skip stack frames before determining the values to be returned. This will generally be useful when calling logging APIs from helper/wrapper code, so that the information in the event log refers not to the helper/wrapper code, but to the code that calls it.\nhandle(record)¶\nHandles a record by passing it to all handlers associated with this logger and its ancestors (until a false value of propagate is found). This method is used for unpickled records received from a socket, as well as those created locally. Logger-level filtering is applied using filter().\nmakeRecord(name, level, fn, lno, msg, args, exc_info, func=None, extra=None, sinfo=None)¶\nThis is a factory method which can be overridden in subclasses to create specialized LogRecord instances.\nhasHandlers()¶\nChecks to see if this logger has any handlers configured. This is done by looking for handlers in this logger and its parents in the logger hierarchy. Returns True if a handler was found, else False. The method stops searching up the hierarchy whenever a logger with the ‘propagate’ attribute set to false is found - that will be the last logger which is checked for the existence of handlers.\nAdded in version 3.2.\nChanged in version 3.7: Loggers can now be pickled and unpickled.\n\n\n\nThe numeric values of logging levels are given in the following table. These are primarily of interest if you want to define your own levels, and need them to have specific values relative to the predefined levels. If you define a level with the same numeric value, it overwrites the predefined value; the predefined name is lost.\n\nLevel\n|\nNumeric value\n|\nWhat it means / When to use it\n\n\n\n\n\n\n\n\n\nlogging.NOTSET¶\n|\n0\n|\nWhen set on a logger, indicates that ancestor loggers are to be consulted to determine the effective level. If that still resolves to NOTSET, then all events are logged. When set on a handler, all events are handled.\n| |\nlogging.DEBUG¶\n|\n10\n|\nDetailed information, typically only of interest to a developer trying to diagnose a problem.\n| |\nlogging.INFO¶\n|\n20\n|\nConfirmation that things are working as expected.\n| |\nlogging.WARNING¶\n|\n30\n|\nAn indication that something unexpected happened, or that a problem might occur in the near future (e.g. ‘disk space low’). The software is still working as expected.\n| |\nlogging.ERROR¶\n|\n40\n|\nDue to a more serious problem, the software has not been able to perform some function.\n| |\nlogging.CRITICAL¶\n|\n50\n|\nA serious error, indicating that the program itself may be unable to continue running.\n|\n\n\n\nHandlers have the following attributes and methods. Note that Handler is never instantiated directly; this class acts as a base for more useful subclasses. However, the __init__() method in subclasses needs to call Handler.__init__().\nclass logging.Handler¶\n__init__(level=NOTSET)¶\nInitializes the Handler instance by setting its level, setting the list of filters to the empty list and creating a lock (using createLock()) for serializing access to an I/O mechanism.\ncreateLock()¶\nInitializes a thread lock which can be used to serialize access to underlying I/O functionality which may not be threadsafe.\nacquire()¶\nAcquires the thread lock created with createLock().\nrelease()¶\nReleases the thread lock acquired with acquire().\nsetLevel(level)¶\nSets the threshold for this handler to level. Logging messages which are less severe than level will be ignored. When a handler is created, the level is set to NOTSET (which causes all messages to be processed).\nSee Logging Levels for a list of levels.\nChanged in version 3.2: The level parameter now accepts a string representation of the level such as ‘INFO’ as an alternative to the integer constants such as INFO.\nsetFormatter(fmt)¶\nSets the Formatter for this handler to fmt.\naddFilter(filter)¶\nAdds the specified filter filter to this handler.\nremoveFilter(filter)¶\nRemoves the specified filter filter from this handler.\nfilter(record)¶\nApply this handler’s filters to the record and return True if the record is to be processed. The filters are consulted in turn, until one of them returns a false value. If none of them return a false value, the record will be emitted. If one returns a false value, the handler will not emit the record.\nflush()¶\nEnsure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses.\nclose()¶\nTidy up any resources used by the handler. This version does no output but removes the handler from an internal list of handlers which is closed when shutdown() is called. Subclasses should ensure that this gets called from overridden close() methods.\nhandle(record)¶\nConditionally emits the specified logging record, depending on filters which may have been added to the handler. Wraps the actual emission of the record with acquisition/release of the I/O thread lock.\nhandleError(record)¶\nThis method should be called from handlers when an exception is encountered during an emit() call. If the module-level attribute raiseExceptions is False, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The specified record is the one which was being processed when the exception occurred. (The default value of raiseExceptions is True, as that is more useful during development).\nformat(record)¶\nDo formatting for a record - if a formatter is set, use it. Otherwise, use the default formatter for the module.\nemit(record)¶\nDo whatever it takes to actually log the specified logging record. This version is intended to be implemented by subclasses and so raises a NotImplementedError.\nWarning\nThis method is called after a handler-level lock is acquired, which is released after this method returns. When you override this method, note that you should be careful when calling anything that invokes other parts of the logging API which might do locking, because that might result in a deadlock. Specifically:\n\nLogging configuration APIs acquire the module-level lock, and then individual handler-level locks as those handlers are configured.\nMany logging APIs lock the module-level lock. If such an API is called from this method, it could cause a deadlock if a configuration call is made on another thread, because that thread will try to acquire the module-level lock before the handler-level lock, whereas this thread tries to acquire the module-level lock after the handler-level lock (because in this method, the handler-level lock has already been acquired).\n\nFor a list of handlers included as standard, see logging.handlers.\n\n\n\nclass logging.Formatter(fmt=None, datefmt=None, style=‘%’, validate=True, *, defaults=None)¶\nResponsible for converting a LogRecord to an output string to be interpreted by a human or external system.\nParameters:\n\nfmt (str) – A format string in the given style for the logged output as a whole. The possible mapping keys are drawn from the LogRecord object’s LogRecord attributes. If not specified, '%(message)s' is used, which is just the logged message.\ndatefmt (str) – A format string in the given style for the date/time portion of the logged output. If not specified, the default described in formatTime() is used.\nstyle (str) – Can be one of '%', '{' or '$' and determines how the format string will be merged with its data: using one of printf-style String Formatting (%), str.format() ({) or string.Template ($). This only applies to fmt and datefmt (e.g. '%(message)s' versus '{message}'), not to the actual log messages passed to the logging methods. However, there are other ways to use {- and $-formatting for log messages.\nvalidate (bool) – If True (the default), incorrect or mismatched fmt and style will raise a ValueError; for example, logging.Formatter('%(asctime)s - %(message)s', style='{').\ndefaults (dict[str, _Any__]_) – A dictionary with default values to use in custom fields. For example, logging.Formatter('%(ip)s %(message)s', defaults={\"ip\": None})\n\nChanged in version 3.2: Added the style parameter.\nChanged in version 3.8: Added the validate parameter.\nChanged in version 3.10: Added the defaults parameter.\nformat(record)¶\nThe record’s attribute dictionary is used as the operand to a string formatting operation. Returns the resulting string. Before formatting the dictionary, a couple of preparatory steps are carried out. The message attribute of the record is computed using msg % args. If the formatting string contains '(asctime)', formatTime() is called to format the event time. If there is exception information, it is formatted using formatException() and appended to the message. Note that the formatted exception information is cached in attribute exc_text. This is useful because the exception information can be pickled and sent across the wire, but you should be careful if you have more than one Formatter subclass which customizes the formatting of exception information. In this case, you will have to clear the cached value (by setting the exc_text attribute to None) after a formatter has done its formatting, so that the next formatter to handle the event doesn’t use the cached value, but recalculates it afresh.\nIf stack information is available, it’s appended after the exception information, using formatStack() to transform it if necessary.\nformatTime(record, datefmt=None)¶\nThis method should be called from format() by a formatter which wants to make use of a formatted time. This method can be overridden in formatters to provide for any specific requirement, but the basic behavior is as follows: if datefmt (a string) is specified, it is used with time.strftime() to format the creation time of the record. Otherwise, the format ‘%Y-%m-%d %H:%M:%S,uuu’ is used, where the uuu part is a millisecond value and the other letters are as per the time.strftime() documentation. An example time in this format is 2003-01-23 00:29:50,411. The resulting string is returned.\nThis function uses a user-configurable function to convert the creation time to a tuple. By default, time.localtime() is used; to change this for a particular formatter instance, set the converter attribute to a function with the same signature as time.localtime() or time.gmtime(). To change it for all formatters, for example if you want all logging times to be shown in GMT, set the converter attribute in the Formatter class.\nChanged in version 3.3: Previously, the default format was hard-coded as in this example: 2010-09-06 22:38:15,292 where the part before the comma is handled by a strptime format string ('%Y-%m-%d %H:%M:%S'), and the part after the comma is a millisecond value. Because strptime does not have a format placeholder for milliseconds, the millisecond value is appended using another format string, '%s,%03d' — and both of these format strings have been hardcoded into this method. With the change, these strings are defined as class-level attributes which can be overridden at the instance level when desired. The names of the attributes are default_time_format (for the strptime format string) and default_msec_format (for appending the millisecond value).\nChanged in version 3.9: The default_msec_format can be None.\nformatException(exc_info)¶\nFormats the specified exception information (a standard exception tuple as returned by sys.exc_info()) as a string. This default implementation just uses traceback.print_exception(). The resulting string is returned.\nformatStack(stack_info)¶\nFormats the specified stack information (a string as returned by traceback.print_stack(), but with the last newline removed) as a string. This default implementation just returns the input value.\nclass logging.BufferingFormatter(linefmt=None)¶\nA base formatter class suitable for subclassing when you want to format a number of records. You can pass a Formatter instance which you want to use to format each line (that corresponds to a single record). If not specified, the default formatter (which just outputs the event message) is used as the line formatter.\nReturn a header for a list of records. The base implementation just returns the empty string. You will need to override this method if you want specific behaviour, e.g. to show the count of records, a title or a separator line.\nReturn a footer for a list of records. The base implementation just returns the empty string. You will need to override this method if you want specific behaviour, e.g. to show the count of records or a separator line.\nformat(records)¶\nReturn formatted text for a list of records. The base implementation just returns the empty string if there are no records; otherwise, it returns the concatenation of the header, each record formatted with the line formatter, and the footer.\n\n\n\nFilters can be used by Handlers and Loggers for more sophisticated filtering than is provided by levels. The base filter class only allows events which are below a certain point in the logger hierarchy. For example, a filter initialized with ‘A.B’ will allow events logged by loggers ‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ etc. but not ‘A.BB’, ‘B.A.B’ etc. If initialized with the empty string, all events are passed.\nclass logging.Filter(name=’’)¶\nReturns an instance of the Filter class. If name is specified, it names a logger which, together with its children, will have its events allowed through the filter. If name is the empty string, allows every event.\nfilter(record)¶\nIs the specified record to be logged? Returns false for no, true for yes. Filters can either modify log records in-place or return a completely different record instance which will replace the original log record in any future processing of the event.\nNote that filters attached to handlers are consulted before an event is emitted by the handler, whereas filters attached to loggers are consulted whenever an event is logged (using debug(), info(), etc.), before sending an event to handlers. This means that events which have been generated by descendant loggers will not be filtered by a logger’s filter setting, unless the filter has also been applied to those descendant loggers.\nYou don’t actually need to subclass Filter: you can pass any instance which has a filter method with the same semantics.\nChanged in version 3.2: You don’t need to create specialized Filter classes, or use other classes with a filter method: you can use a function (or other callable) as a filter. The filtering logic will check to see if the filter object has a filter attribute: if it does, it’s assumed to be a Filter and its filter() method is called. Otherwise, it’s assumed to be a callable and called with the record as the single parameter. The returned value should conform to that returned by filter().\nChanged in version 3.12: You can now return a LogRecord instance from filters to replace the log record rather than modifying it in place. This allows filters attached to a Handler to modify the log record before it is emitted, without having side effects on other handlers.\nAlthough filters are used primarily to filter records based on more sophisticated criteria than levels, they get to see every record which is processed by the handler or logger they’re attached to: this can be useful if you want to do things like counting how many records were processed by a particular logger or handler, or adding, changing or removing attributes in the LogRecord being processed. Obviously changing the LogRecord needs to be done with some care, but it does allow the injection of contextual information into logs (see Using Filters to impart contextual information).\n\n\n\nLogRecord instances are created automatically by the Logger every time something is logged, and can be created manually via makeLogRecord() (for example, from a pickled event received over the wire).\nclass logging.LogRecord(name, level, pathname, lineno, msg, args, exc_info, func=None, sinfo=None)¶\nContains all the information pertinent to the event being logged.\nThe primary information is passed in msg and args, which are combined using msg % args to create the message attribute of the record.\nParameters:\n\nname (str) – The name of the logger used to log the event represented by this LogRecord. Note that the logger name in the LogRecord will always have this value, even though it may be emitted by a handler attached to a different (ancestor) logger.\nlevel (int) – The numeric level of the logging event (such as 10 for DEBUG, 20 for INFO, etc). Note that this is converted to two attributes of the LogRecord: levelno for the numeric value and levelname for the corresponding level name.\npathname (str) – The full string path of the source file where the logging call was made.\nlineno (int) – The line number in the source file where the logging call was made.\nmsg (Any) – The event description message, which can be a %-format string with placeholders for variable data, or an arbitrary object (see Using arbitrary objects as messages).\nargs (tuple | dict[str, Any]) – Variable data to merge into the msg argument to obtain the event description.\nexc_info (tuple[type[BaseException_]__,_ BaseException, types.TracebackType] | None) – An exception tuple with the current exception information, as returned by sys.exc_info(), or None if no exception information is available.\nfunc (str | None) – The name of the function or method from which the logging call was invoked.\nsinfo (str | None) – A text string representing stack information from the base of the stack in the current thread, up to the logging call.\n\ngetMessage()¶\nReturns the message for this LogRecord instance after merging any user-supplied arguments with the message. If the user-supplied message argument to the logging call is not a string, str() is called on it to convert it to a string. This allows use of user-defined classes as messages, whose __str__ method can return the actual format string to be used.\nChanged in version 3.2: The creation of a LogRecord has been made more configurable by providing a factory which is used to create the record. The factory can be set using getLogRecordFactory() and setLogRecordFactory() (see this for the factory’s signature).\nThis functionality can be used to inject your own values into a LogRecord at creation time. You can use the following pattern:\nold_factory = logging.getLogRecordFactory()\n\ndef record_factory(*args, **kwargs):\n    record = old_factory(*args, **kwargs)\n    record.custom_attribute = 0xdecafbad\n    return record\n\nlogging.setLogRecordFactory(record_factory)\n\nWith this pattern, multiple factories could be chained, and as long as they don’t overwrite each other’s attributes or unintentionally overwrite the standard attributes listed above, there should be no surprises.\n\n\n\nThe LogRecord has a number of attributes, most of which are derived from the parameters to the constructor. (Note that the names do not always correspond exactly between the LogRecord constructor parameters and the LogRecord attributes.) These attributes can be used to merge data from the record into the format string. The following table lists (in alphabetical order) the attribute names, their meanings and the corresponding placeholder in a %-style format string.\nIf you are using {}-formatting (str.format()), you can use {attrname} as the placeholder in the format string. If you are using \\(-formatting ([`string.Template`](https://docs.python.org/3/library/string.html#string.Template \"string.Template\")), use the form `\\){attrname}. In both cases, of course, replaceattrname` with the actual attribute name you want to use.\nIn the case of {}-formatting, you can specify formatting flags by placing them after the attribute name, separated from it with a colon. For example: a placeholder of {msecs:03.0f} would format a millisecond value of 4 as 004. Refer to the str.format() documentation for full details on the options available to you.\n\nAttribute name\n|\nFormat\n|\nDescription\n\n\n\n\n\n\n\n\n\nargs\n|\nYou shouldn’t need to format this yourself.\n|\nThe tuple of arguments merged into msg to produce message, or a dict whose values are used for the merge (when there is only one argument, and it is a dictionary).\n| |\nasctime\n|\n%(asctime)s\n|\nHuman-readable time when the LogRecord was created. By default this is of the form ‘2003-07-08 16:49:45,896’ (the numbers after the comma are millisecond portion of the time).\n| |\ncreated\n|\n%(created)f\n|\nTime when the LogRecord was created (as returned by time.time_ns() / 1e9).\n| |\nexc_info\n|\nYou shouldn’t need to format this yourself.\n|\nException tuple (à la sys.exc_info) or, if no exception has occurred, None.\n| |\nfilename\n|\n%(filename)s\n|\nFilename portion of pathname.\n| |\nfuncName\n|\n%(funcName)s\n|\nName of function containing the logging call.\n| |\nlevelname\n|\n%(levelname)s\n|\nText logging level for the message ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL').\n| |\nlevelno\n|\n%(levelno)s\n|\nNumeric logging level for the message (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n| |\nlineno\n|\n%(lineno)d\n|\nSource line number where the logging call was issued (if available).\n| |\nmessage\n|\n%(message)s\n|\nThe logged message, computed as msg % args. This is set when Formatter.format() is invoked.\n| |\nmodule\n|\n%(module)s\n|\nModule (name portion of filename).\n| |\nmsecs\n|\n%(msecs)d\n|\nMillisecond portion of the time when the LogRecord was created.\n| |\nmsg\n|\nYou shouldn’t need to format this yourself.\n|\nThe format string passed in the original logging call. Merged with args to produce message, or an arbitrary object (see Using arbitrary objects as messages).\n| |\nname\n|\n%(name)s\n|\nName of the logger used to log the call.\n| |\npathname\n|\n%(pathname)s\n|\nFull pathname of the source file where the logging call was issued (if available).\n| |\nprocess\n|\n%(process)d\n|\nProcess ID (if available).\n| |\nprocessName\n|\n%(processName)s\n|\nProcess name (if available).\n| |\nrelativeCreated\n|\n%(relativeCreated)d\n|\nTime in milliseconds when the LogRecord was created, relative to the time the logging module was loaded.\n| |\nstack_info\n|\nYou shouldn’t need to format this yourself.\n|\nStack frame information (where available) from the bottom of the stack in the current thread, up to and including the stack frame of the logging call which resulted in the creation of this record.\n| |\nthread\n|\n%(thread)d\n|\nThread ID (if available).\n| |\nthreadName\n|\n%(threadName)s\n|\nThread name (if available).\n| |\ntaskName\n|\n%(taskName)s\n|\nasyncio.Task name (if available).\n|\nChanged in version 3.1: processName was added.\nChanged in version 3.12: taskName was added.\n\n\n\nLoggerAdapter instances are used to conveniently pass contextual information into logging calls. For a usage example, see the section on adding contextual information to your logging output.\nclass logging.LoggerAdapter(logger, extra, merge_extra=False)¶\nReturns an instance of LoggerAdapter initialized with an underlying Logger instance, a dict-like object (extra), and a boolean (merge_extra) indicating whether or not the extra argument of individual log calls should be merged with the LoggerAdapter extra. The default behavior is to ignore the extra argument of individual log calls and only use the one of the LoggerAdapter instance\nprocess(msg, kwargs)¶\nModifies the message and/or keyword arguments passed to a logging call in order to insert contextual information. This implementation takes the object passed as extra to the constructor and adds it to kwargs using key ‘extra’. The return value is a (msg, kwargs) tuple which has the (possibly modified) versions of the arguments passed in.\nmanager¶\nDelegates to the underlying manager on logger.\n_log¶\nDelegates to the underlying _log() method on logger.\nIn addition to the above, LoggerAdapter supports the following methods of Logger: debug(), info(), warning(), error(), exception(), critical(), log(), isEnabledFor(), getEffectiveLevel(), setLevel() and hasHandlers(). These methods have the same signatures as their counterparts in Logger, so you can use the two types of instances interchangeably.\nChanged in version 3.2: The isEnabledFor(), getEffectiveLevel(), setLevel() and hasHandlers() methods were added to LoggerAdapter. These methods delegate to the underlying logger.\nChanged in version 3.6: Attribute manager and method _log() were added, which delegate to the underlying logger and allow adapters to be nested.\nChanged in version 3.13: The merge_extra argument was added.\n\n\n\nThe logging module is intended to be thread-safe without any special work needing to be done by its clients. It achieves this though using threading locks; there is one lock to serialize access to the module’s shared data, and each handler also creates a lock to serialize access to its underlying I/O.\nIf you are implementing asynchronous signal handlers using the signal module, you may not be able to use logging from within such handlers. This is because lock implementations in the threading module are not always re-entrant, and so cannot be invoked from such signal handlers.\n\n\n\nIn addition to the classes described above, there are a number of module-level functions.\nlogging.getLogger(name=None)¶\nReturn a logger with the specified name or, if name is None, return the root logger of the hierarchy. If specified, the name is typically a dot-separated hierarchical name like ‘a’, ‘a.b’ or ‘a.b.c.d’. Choice of these names is entirely up to the developer who is using logging, though it is recommended that __name__ be used unless you have a specific reason for not doing that, as mentioned in Logger Objects.\nAll calls to this function with a given name return the same logger instance. This means that logger instances never need to be passed between different parts of an application.\nlogging.getLoggerClass()¶\nReturn either the standard Logger class, or the last class passed to setLoggerClass(). This function may be called from within a new class definition, to ensure that installing a customized Logger class will not undo customizations already applied by other code. For example:\nclass MyLogger(logging.getLoggerClass()):\n    # ... override behaviour here\n\nlogging.getLogRecordFactory()¶\nReturn a callable which is used to create a LogRecord.\nAdded in version 3.2: This function has been provided, along with setLogRecordFactory(), to allow developers more control over how the LogRecord representing a logging event is constructed.\nSee setLogRecordFactory() for more information about the how the factory is called.\nlogging.debug(msg, *args, **kwargs)¶\nThis is a convenience function that calls Logger.debug(), on the root logger. The handling of the arguments is in every way identical to what is described in that method.\nThe only difference is that if the root logger has no handlers, then basicConfig() is called, prior to calling debug on the root logger.\nFor very short scripts or quick demonstrations of logging facilities, debug and the other module-level functions may be convenient. However, most programs will want to carefully and explicitly control the logging configuration, and should therefore prefer creating a module-level logger and calling Logger.debug() (or other level-specific methods) on it, as described at the beginnning of this documentation.\nlogging.info(msg, *args, **kwargs)¶\nLogs a message with level INFO on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.warning(msg, *args, **kwargs)¶\nLogs a message with level WARNING on the root logger. The arguments and behavior are otherwise the same as for debug().\nNote\nThere is an obsolete function warn which is functionally identical to warning. As warn is deprecated, please do not use it - use warning instead.\nlogging.error(msg, *args, **kwargs)¶\nLogs a message with level ERROR on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.critical(msg, *args, **kwargs)¶\nLogs a message with level CRITICAL on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.exception(msg, *args, **kwargs)¶\nLogs a message with level ERROR on the root logger. The arguments and behavior are otherwise the same as for debug(). Exception info is added to the logging message. This function should only be called from an exception handler.\nlogging.log(level, msg, *args, **kwargs)¶\nLogs a message with level level on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.disable(level=CRITICAL)¶\nProvides an overriding level level for all loggers which takes precedence over the logger’s own level. When the need arises to temporarily throttle logging output down across the whole application, this function can be useful. Its effect is to disable all logging calls of severity level and below, so that if you call it with a value of INFO, then all INFO and DEBUG events would be discarded, whereas those of severity WARNING and above would be processed according to the logger’s effective level. If logging.disable(logging.NOTSET) is called, it effectively removes this overriding level, so that logging output again depends on the effective levels of individual loggers.\nNote that if you have defined any custom logging level higher than CRITICAL (this is not recommended), you won’t be able to rely on the default value for the level parameter, but will have to explicitly supply a suitable value.\nChanged in version 3.7: The level parameter was defaulted to level CRITICAL. See bpo-28524 for more information about this change.\nlogging.addLevelName(level, levelName)¶\nAssociates level level with text levelName in an internal dictionary, which is used to map numeric levels to a textual representation, for example when a Formatter formats a message. This function can also be used to define your own levels. The only constraints are that all levels used must be registered using this function, levels should be positive integers and they should increase in increasing order of severity.\nNote\nIf you are thinking of defining your own levels, please see the section on Custom Levels.\nlogging.getLevelNamesMapping()¶\nReturns a mapping from level names to their corresponding logging levels. For example, the string “CRITICAL” maps to CRITICAL. The returned mapping is copied from an internal mapping on each call to this function.\nAdded in version 3.11.\nlogging.getLevelName(level)¶\nReturns the textual or numeric representation of logging level level.\nIf level is one of the predefined levels CRITICAL, ERROR, WARNING, INFO or DEBUG then you get the corresponding string. If you have associated levels with names using addLevelName() then the name you have associated with level is returned. If a numeric value corresponding to one of the defined levels is passed in, the corresponding string representation is returned.\nThe level parameter also accepts a string representation of the level such as ‘INFO’. In such cases, this functions returns the corresponding numeric value of the level.\nIf no matching numeric or string value is passed in, the string ‘Level %s’ % level is returned.\nNote\nLevels are internally integers (as they need to be compared in the logging logic). This function is used to convert between an integer level and the level name displayed in the formatted log output by means of the %(levelname)s format specifier (see LogRecord attributes), and vice versa.\nChanged in version 3.4: In Python versions earlier than 3.4, this function could also be passed a text level, and would return the corresponding numeric value of the level. This undocumented behaviour was considered a mistake, and was removed in Python 3.4, but reinstated in 3.4.2 due to retain backward compatibility.\nlogging.getHandlerByName(name)¶\nReturns a handler with the specified name, or None if there is no handler with that name.\nAdded in version 3.12.\nlogging.getHandlerNames()¶\nReturns an immutable set of all known handler names.\nAdded in version 3.12.\nlogging.makeLogRecord(attrdict)¶\nCreates and returns a new LogRecord instance whose attributes are defined by attrdict. This function is useful for taking a pickled LogRecord attribute dictionary, sent over a socket, and reconstituting it as a LogRecord instance at the receiving end.\nlogging.basicConfig(**kwargs)¶\nDoes basic configuration for the logging system by creating a StreamHandler with a default Formatter and adding it to the root logger. The functions debug(), info(), warning(), error() and critical() will call basicConfig() automatically if no handlers are defined for the root logger.\nThis function does nothing if the root logger already has handlers configured, unless the keyword argument force is set to True.\nNote\nThis function should be called from the main thread before other threads are started. In versions of Python prior to 2.7.1 and 3.2, if this function is called from multiple threads, it is possible (in rare circumstances) that a handler will be added to the root logger more than once, leading to unexpected results such as messages being duplicated in the log.\nThe following keyword arguments are supported.\n\nFormat\n|\nDescription\n\n\n\n\n\n\n\n\nfilename\n|\nSpecifies that a FileHandler be created, using the specified filename, rather than a StreamHandler.\n| |\nfilemode\n|\nIf filename is specified, open the file in this mode. Defaults to 'a'.\n| |\nformat\n|\nUse the specified format string for the handler. Defaults to attributes levelname, name and message separated by colons.\n| |\ndatefmt\n|\nUse the specified date/time format, as accepted by time.strftime().\n| |\nstyle\n|\nIf format is specified, use this style for the format string. One of '%', '{' or '$' for printf-style, str.format() or string.Template respectively. Defaults to '%'.\n| |\nlevel\n|\nSet the root logger level to the specified level.\n| |\nstream\n|\nUse the specified stream to initialize the StreamHandler. Note that this argument is incompatible with filename - if both are present, a ValueError is raised.\n| |\nhandlers\n|\nIf specified, this should be an iterable of already created handlers to add to the root logger. Any handlers which don’t already have a formatter set will be assigned the default formatter created in this function. Note that this argument is incompatible with filename or stream - if both are present, a ValueError is raised.\n| |\nforce\n|\nIf this keyword argument is specified as true, any existing handlers attached to the root logger are removed and closed, before carrying out the configuration as specified by the other arguments.\n| |\nencoding\n|\nIf this keyword argument is specified along with filename, its value is used when the FileHandler is created, and thus used when opening the output file.\n| |\nerrors\n|\nIf this keyword argument is specified along with filename, its value is used when the FileHandler is created, and thus used when opening the output file. If not specified, the value ‘backslashreplace’ is used. Note that if None is specified, it will be passed as such to open(), which means that it will be treated the same as passing ‘errors’.\n|\nChanged in version 3.2: The style argument was added.\nChanged in version 3.3: The handlers argument was added. Additional checks were added to catch situations where incompatible arguments are specified (e.g. handlers together with stream or filename, or stream together with filename).\nChanged in version 3.8: The force argument was added.\nChanged in version 3.9: The encoding and errors arguments were added.\nlogging.shutdown()¶\nInforms the logging system to perform an orderly shutdown by flushing and closing all handlers. This should be called at application exit and no further use of the logging system should be made after this call.\nWhen the logging module is imported, it registers this function as an exit handler (see atexit), so normally there’s no need to do that manually.\nlogging.setLoggerClass(klass)¶\nTells the logging system to use the class klass when instantiating a logger. The class should define __init__() such that only a name argument is required, and the __init__() should call Logger.__init__(). This function is typically called before any loggers are instantiated by applications which need to use custom logger behavior. After this call, as at any other time, do not instantiate loggers directly using the subclass: continue to use the logging.getLogger() API to get your loggers.\nlogging.setLogRecordFactory(factory)¶\nSet a callable which is used to create a LogRecord.\nParameters:\nfactory – The factory callable to be used to instantiate a log record.\nAdded in version 3.2: This function has been provided, along with getLogRecordFactory(), to allow developers more control over how the LogRecord representing a logging event is constructed.\nThe factory has the following signature:\nfactory(name, level, fn, lno, msg, args, exc_info, func=None, sinfo=None, **kwargs)\n\nname:\nThe logger name.\nlevel:\nThe logging level (numeric).\nfn:\nThe full pathname of the file where the logging call was made.\nlno:\nThe line number in the file where the logging call was made.\nmsg:\nThe logging message.\nargs:\nThe arguments for the logging message.\nexc_info:\nAn exception tuple, or None.\nfunc:\nThe name of the function or method which invoked the logging call.\nsinfo:\nA stack traceback such as is provided by traceback.print_stack(), showing the call hierarchy.\nkwargs:\nAdditional keyword arguments.\n\n\n\n\nlogging.lastResort¶\nA “handler of last resort” is available through this attribute. This is a StreamHandler writing to sys.stderr with a level of WARNING, and is used to handle logging events in the absence of any logging configuration. The end result is to just print the message to sys.stderr. This replaces the earlier error message saying that “no handlers could be found for logger XYZ”. If you need the earlier behaviour for some reason, lastResort can be set to None.\nAdded in version 3.2.\nlogging.raiseExceptions¶\nUsed to see if exceptions during handling should be propagated.\nDefault: True.\nIf raiseExceptions is False, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors.\n\n\n\nThe captureWarnings() function can be used to integrate logging with the warnings module.\nlogging.captureWarnings(capture)¶\nThis function is used to turn the capture of warnings by logging on and off.\nIf capture is True, warnings issued by the warnings module will be redirected to the logging system. Specifically, a warning will be formatted using warnings.formatwarning() and the resulting string logged to a logger named 'py.warnings' with a severity of WARNING.\nIf capture is False, the redirection of warnings to the logging system will stop, and warnings will be redirected to their original destinations (i.e. those in effect before captureWarnings(True) was called).\nSee also\nModule logging.config\nConfiguration API for the logging module.\nModule logging.handlers\nUseful handlers included with the logging module.\nPEP 282 - A Logging System\nThe proposal which described this feature for inclusion in the Python standard library.\nOriginal Python logging package\nThis is the original source for the logging package. The version of the package available from this site is suitable for use with Python 1.5.2, 2.1.x and 2.2.x, which do not include the logging package in the standard library."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#logger-objects",
    "href": "knowledgebase/python/python3_logging_api.html#logger-objects",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "Loggers have the following attributes and methods. Note that Loggers should NEVER be instantiated directly, but always through the module-level function logging.getLogger(name). Multiple calls to getLogger() with the same name will always return a reference to the same Logger object.\nThe name is potentially a period-separated hierarchical value, like foo.bar.baz (though it could also be just plain foo, for example). Loggers that are further down in the hierarchical list are children of loggers higher up in the list. For example, given a logger with a name of foo, loggers with names of foo.bar, foo.bar.baz, and foo.bam are all descendants of foo. In addition, all loggers are descendants of the root logger. The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you organise your loggers on a per-module basis using the recommended construction logging.getLogger(__name__). That’s because in a module, __name__ is the module’s name in the Python package namespace.\nclass logging.Logger¶\nname¶\nThis is the logger’s name, and is the value that was passed to getLogger() to obtain the logger.\nNote\nThis attribute should be treated as read-only.\nlevel¶\nThe threshold of this logger, as set by the setLevel() method.\nNote\nDo not set this attribute directly - always use setLevel(), which has checks for the level passed to it.\nparent¶\nThe parent logger of this logger. It may change based on later instantiation of loggers which are higher up in the namespace hierarchy.\nNote\nThis value should be treated as read-only.\npropagate¶\nIf this attribute evaluates to true, events logged to this logger will be passed to the handlers of higher level (ancestor) loggers, in addition to any handlers attached to this logger. Messages are passed directly to the ancestor loggers’ handlers - neither the level nor filters of the ancestor loggers in question are considered.\nIf this evaluates to false, logging messages are not passed to the handlers of ancestor loggers.\nSpelling it out with an example: If the propagate attribute of the logger named A.B.C evaluates to true, any event logged to A.B.C via a method call such as logging.getLogger('A.B.C').error(...) will [subject to passing that logger’s level and filter settings] be passed in turn to any handlers attached to loggers named A.B, A and the root logger, after first being passed to any handlers attached to A.B.C. If any logger in the chain A.B.C, A.B, A has its propagate attribute set to false, then that is the last logger whose handlers are offered the event to handle, and propagation stops at that point.\nThe constructor sets this attribute to True.\nNote\nIf you attach a handler to a logger and one or more of its ancestors, it may emit the same record multiple times. In general, you should not need to attach a handler to more than one logger - if you just attach it to the appropriate logger which is highest in the logger hierarchy, then it will see all events logged by all descendant loggers, provided that their propagate setting is left set to True. A common scenario is to attach handlers only to the root logger, and to let propagation take care of the rest.\nhandlers¶\nThe list of handlers directly attached to this logger instance.\nNote\nThis attribute should be treated as read-only; it is normally changed via the addHandler() and removeHandler() methods, which use locks to ensure thread-safe operation.\ndisabled¶\nThis attribute disables handling of any events. It is set to False in the initializer, and only changed by logging configuration code.\nNote\nThis attribute should be treated as read-only.\nsetLevel(level)¶\nSets the threshold for this logger to level. Logging messages which are less severe than level will be ignored; logging messages which have severity level or higher will be emitted by whichever handler or handlers service this logger, unless a handler’s level has been set to a higher severity level than level.\nWhen a logger is created, the level is set to NOTSET (which causes all messages to be processed when the logger is the root logger, or delegation to the parent when the logger is a non-root logger). Note that the root logger is created with level WARNING.\nThe term ‘delegation to the parent’ means that if a logger has a level of NOTSET, its chain of ancestor loggers is traversed until either an ancestor with a level other than NOTSET is found, or the root is reached.\nIf an ancestor is found with a level other than NOTSET, then that ancestor’s level is treated as the effective level of the logger where the ancestor search began, and is used to determine how a logging event is handled.\nIf the root is reached, and it has a level of NOTSET, then all messages will be processed. Otherwise, the root’s level will be used as the effective level.\nSee Logging Levels for a list of levels.\nChanged in version 3.2: The level parameter now accepts a string representation of the level such as ‘INFO’ as an alternative to the integer constants such as INFO. Note, however, that levels are internally stored as integers, and methods such as e.g. getEffectiveLevel() and isEnabledFor() will return/expect to be passed integers.\nisEnabledFor(level)¶\nIndicates if a message of severity level would be processed by this logger. This method checks first the module-level level set by logging.disable(level) and then the logger’s effective level as determined by getEffectiveLevel().\ngetEffectiveLevel()¶\nIndicates the effective level for this logger. If a value other than NOTSET has been set using setLevel(), it is returned. Otherwise, the hierarchy is traversed towards the root until a value other than NOTSET is found, and that value is returned. The value returned is an integer, typically one of logging.DEBUG, logging.INFO etc.\ngetChild(suffix)¶\nReturns a logger which is a descendant to this logger, as determined by the suffix. Thus, logging.getLogger('abc').getChild('def.ghi') would return the same logger as would be returned by logging.getLogger('abc.def.ghi'). This is a convenience method, useful when the parent logger is named using e.g. __name__ rather than a literal string.\nAdded in version 3.2.\ngetChildren()¶\nReturns a set of loggers which are immediate children of this logger. So for example logging.getLogger().getChildren() might return a set containing loggers named foo and bar, but a logger named foo.bar wouldn’t be included in the set. Likewise, logging.getLogger('foo').getChildren() might return a set including a logger named foo.bar, but it wouldn’t include one named foo.bar.baz.\nAdded in version 3.12.\ndebug(msg, *args, **kwargs)¶\nLogs a message with level DEBUG on this logger. The msg is the message format string, and the args are the arguments which are merged into msg using the string formatting operator. (Note that this means that you can use keywords in the format string, together with a single dictionary argument.) No % formatting operation is performed on msg when no args are supplied.\nThere are four keyword arguments in kwargs which are inspected: exc_info, stack_info, stacklevel and extra.\nIf exc_info does not evaluate as false, it causes exception information to be added to the logging message. If an exception tuple (in the format returned by sys.exc_info()) or an exception instance is provided, it is used; otherwise, sys.exc_info() is called to get the exception information.\nThe second optional keyword argument is stack_info, which defaults to False. If true, stack information is added to the logging message, including the actual logging call. Note that this is not the same stack information as that displayed through specifying exc_info: The former is stack frames from the bottom of the stack up to the logging call in the current thread, whereas the latter is information about stack frames which have been unwound, following an exception, while searching for exception handlers.\nYou can specify stack_info independently of exc_info, e.g. to just show how you got to a certain point in your code, even when no exceptions were raised. The stack frames are printed following a header line which says:\nStack (most recent call last):\n\nThis mimics the Traceback (most recent call last): which is used when displaying exception frames.\nThe third optional keyword argument is stacklevel, which defaults to 1. If greater than 1, the corresponding number of stack frames are skipped when computing the line number and function name set in the LogRecord created for the logging event. This can be used in logging helpers so that the function name, filename and line number recorded are not the information for the helper function/method, but rather its caller. The name of this parameter mirrors the equivalent one in the warnings module.\nThe fourth keyword argument is extra which can be used to pass a dictionary which is used to populate the __dict__ of the LogRecord created for the logging event with user-defined attributes. These custom attributes can then be used as you like. For example, they could be incorporated into logged messages. For example:\nFORMAT = '%(asctime)s %(clientip)-15s %(user)-8s %(message)s'\nlogging.basicConfig(format=FORMAT)\nd = {'clientip': '192.168.0.1', 'user': 'fbloggs'}\nlogger = logging.getLogger('tcpserver')\nlogger.warning('Protocol problem: %s', 'connection reset', extra=d)\n\nwould print something like\n2006-02-08 22:20:02,165 192.168.0.1 fbloggs  Protocol problem: connection reset\n\nThe keys in the dictionary passed in extra should not clash with the keys used by the logging system. (See the section on LogRecord attributes for more information on which keys are used by the logging system.)\nIf you choose to use these attributes in logged messages, you need to exercise some care. In the above example, for instance, the Formatter has been set up with a format string which expects ‘clientip’ and ‘user’ in the attribute dictionary of the LogRecord. If these are missing, the message will not be logged because a string formatting exception will occur. So in this case, you always need to pass the extra dictionary with these keys.\nWhile this might be annoying, this feature is intended for use in specialized circumstances, such as multi-threaded servers where the same code executes in many contexts, and interesting conditions which arise are dependent on this context (such as remote client IP address and authenticated user name, in the above example). In such circumstances, it is likely that specialized Formatters would be used with particular Handlers.\nIf no handler is attached to this logger (or any of its ancestors, taking into account the relevant Logger.propagate attributes), the message will be sent to the handler set on lastResort.\nChanged in version 3.2: The stack_info parameter was added.\nChanged in version 3.5: The exc_info parameter can now accept exception instances.\nChanged in version 3.8: The stacklevel parameter was added.\ninfo(msg, *args, **kwargs)¶\nLogs a message with level INFO on this logger. The arguments are interpreted as for debug().\nwarning(msg, *args, **kwargs)¶\nLogs a message with level WARNING on this logger. The arguments are interpreted as for debug().\nNote\nThere is an obsolete method warn which is functionally identical to warning. As warn is deprecated, please do not use it - use warning instead.\nerror(msg, *args, **kwargs)¶\nLogs a message with level ERROR on this logger. The arguments are interpreted as for debug().\ncritical(msg, *args, **kwargs)¶\nLogs a message with level CRITICAL on this logger. The arguments are interpreted as for debug().\nlog(level, msg, *args, **kwargs)¶\nLogs a message with integer level level on this logger. The other arguments are interpreted as for debug().\nexception(msg, *args, **kwargs)¶\nLogs a message with level ERROR on this logger. The arguments are interpreted as for debug(). Exception info is added to the logging message. This method should only be called from an exception handler.\naddFilter(filter)¶\nAdds the specified filter filter to this logger.\nremoveFilter(filter)¶\nRemoves the specified filter filter from this logger.\nfilter(record)¶\nApply this logger’s filters to the record and return True if the record is to be processed. The filters are consulted in turn, until one of them returns a false value. If none of them return a false value, the record will be processed (passed to handlers). If one returns a false value, no further processing of the record occurs.\naddHandler(hdlr)¶\nAdds the specified handler hdlr to this logger.\nremoveHandler(hdlr)¶\nRemoves the specified handler hdlr from this logger.\nfindCaller(stack_info=False, stacklevel=1)¶\nFinds the caller’s source filename and line number. Returns the filename, line number, function name and stack information as a 4-element tuple. The stack information is returned as None unless stack_info is True.\nThe stacklevel parameter is passed from code calling the debug() and other APIs. If greater than 1, the excess is used to skip stack frames before determining the values to be returned. This will generally be useful when calling logging APIs from helper/wrapper code, so that the information in the event log refers not to the helper/wrapper code, but to the code that calls it.\nhandle(record)¶\nHandles a record by passing it to all handlers associated with this logger and its ancestors (until a false value of propagate is found). This method is used for unpickled records received from a socket, as well as those created locally. Logger-level filtering is applied using filter().\nmakeRecord(name, level, fn, lno, msg, args, exc_info, func=None, extra=None, sinfo=None)¶\nThis is a factory method which can be overridden in subclasses to create specialized LogRecord instances.\nhasHandlers()¶\nChecks to see if this logger has any handlers configured. This is done by looking for handlers in this logger and its parents in the logger hierarchy. Returns True if a handler was found, else False. The method stops searching up the hierarchy whenever a logger with the ‘propagate’ attribute set to false is found - that will be the last logger which is checked for the existence of handlers.\nAdded in version 3.2.\nChanged in version 3.7: Loggers can now be pickled and unpickled."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#logging-levels",
    "href": "knowledgebase/python/python3_logging_api.html#logging-levels",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "The numeric values of logging levels are given in the following table. These are primarily of interest if you want to define your own levels, and need them to have specific values relative to the predefined levels. If you define a level with the same numeric value, it overwrites the predefined value; the predefined name is lost.\n\nLevel\n|\nNumeric value\n|\nWhat it means / When to use it\n\n\n\n\n\n\n\n\n\nlogging.NOTSET¶\n|\n0\n|\nWhen set on a logger, indicates that ancestor loggers are to be consulted to determine the effective level. If that still resolves to NOTSET, then all events are logged. When set on a handler, all events are handled.\n| |\nlogging.DEBUG¶\n|\n10\n|\nDetailed information, typically only of interest to a developer trying to diagnose a problem.\n| |\nlogging.INFO¶\n|\n20\n|\nConfirmation that things are working as expected.\n| |\nlogging.WARNING¶\n|\n30\n|\nAn indication that something unexpected happened, or that a problem might occur in the near future (e.g. ‘disk space low’). The software is still working as expected.\n| |\nlogging.ERROR¶\n|\n40\n|\nDue to a more serious problem, the software has not been able to perform some function.\n| |\nlogging.CRITICAL¶\n|\n50\n|\nA serious error, indicating that the program itself may be unable to continue running.\n|"
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#handler-objects",
    "href": "knowledgebase/python/python3_logging_api.html#handler-objects",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "Handlers have the following attributes and methods. Note that Handler is never instantiated directly; this class acts as a base for more useful subclasses. However, the __init__() method in subclasses needs to call Handler.__init__().\nclass logging.Handler¶\n__init__(level=NOTSET)¶\nInitializes the Handler instance by setting its level, setting the list of filters to the empty list and creating a lock (using createLock()) for serializing access to an I/O mechanism.\ncreateLock()¶\nInitializes a thread lock which can be used to serialize access to underlying I/O functionality which may not be threadsafe.\nacquire()¶\nAcquires the thread lock created with createLock().\nrelease()¶\nReleases the thread lock acquired with acquire().\nsetLevel(level)¶\nSets the threshold for this handler to level. Logging messages which are less severe than level will be ignored. When a handler is created, the level is set to NOTSET (which causes all messages to be processed).\nSee Logging Levels for a list of levels.\nChanged in version 3.2: The level parameter now accepts a string representation of the level such as ‘INFO’ as an alternative to the integer constants such as INFO.\nsetFormatter(fmt)¶\nSets the Formatter for this handler to fmt.\naddFilter(filter)¶\nAdds the specified filter filter to this handler.\nremoveFilter(filter)¶\nRemoves the specified filter filter from this handler.\nfilter(record)¶\nApply this handler’s filters to the record and return True if the record is to be processed. The filters are consulted in turn, until one of them returns a false value. If none of them return a false value, the record will be emitted. If one returns a false value, the handler will not emit the record.\nflush()¶\nEnsure all logging output has been flushed. This version does nothing and is intended to be implemented by subclasses.\nclose()¶\nTidy up any resources used by the handler. This version does no output but removes the handler from an internal list of handlers which is closed when shutdown() is called. Subclasses should ensure that this gets called from overridden close() methods.\nhandle(record)¶\nConditionally emits the specified logging record, depending on filters which may have been added to the handler. Wraps the actual emission of the record with acquisition/release of the I/O thread lock.\nhandleError(record)¶\nThis method should be called from handlers when an exception is encountered during an emit() call. If the module-level attribute raiseExceptions is False, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors. You could, however, replace this with a custom handler if you wish. The specified record is the one which was being processed when the exception occurred. (The default value of raiseExceptions is True, as that is more useful during development).\nformat(record)¶\nDo formatting for a record - if a formatter is set, use it. Otherwise, use the default formatter for the module.\nemit(record)¶\nDo whatever it takes to actually log the specified logging record. This version is intended to be implemented by subclasses and so raises a NotImplementedError.\nWarning\nThis method is called after a handler-level lock is acquired, which is released after this method returns. When you override this method, note that you should be careful when calling anything that invokes other parts of the logging API which might do locking, because that might result in a deadlock. Specifically:\n\nLogging configuration APIs acquire the module-level lock, and then individual handler-level locks as those handlers are configured.\nMany logging APIs lock the module-level lock. If such an API is called from this method, it could cause a deadlock if a configuration call is made on another thread, because that thread will try to acquire the module-level lock before the handler-level lock, whereas this thread tries to acquire the module-level lock after the handler-level lock (because in this method, the handler-level lock has already been acquired).\n\nFor a list of handlers included as standard, see logging.handlers."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#formatter-objects",
    "href": "knowledgebase/python/python3_logging_api.html#formatter-objects",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "class logging.Formatter(fmt=None, datefmt=None, style=‘%’, validate=True, *, defaults=None)¶\nResponsible for converting a LogRecord to an output string to be interpreted by a human or external system.\nParameters:\n\nfmt (str) – A format string in the given style for the logged output as a whole. The possible mapping keys are drawn from the LogRecord object’s LogRecord attributes. If not specified, '%(message)s' is used, which is just the logged message.\ndatefmt (str) – A format string in the given style for the date/time portion of the logged output. If not specified, the default described in formatTime() is used.\nstyle (str) – Can be one of '%', '{' or '$' and determines how the format string will be merged with its data: using one of printf-style String Formatting (%), str.format() ({) or string.Template ($). This only applies to fmt and datefmt (e.g. '%(message)s' versus '{message}'), not to the actual log messages passed to the logging methods. However, there are other ways to use {- and $-formatting for log messages.\nvalidate (bool) – If True (the default), incorrect or mismatched fmt and style will raise a ValueError; for example, logging.Formatter('%(asctime)s - %(message)s', style='{').\ndefaults (dict[str, _Any__]_) – A dictionary with default values to use in custom fields. For example, logging.Formatter('%(ip)s %(message)s', defaults={\"ip\": None})\n\nChanged in version 3.2: Added the style parameter.\nChanged in version 3.8: Added the validate parameter.\nChanged in version 3.10: Added the defaults parameter.\nformat(record)¶\nThe record’s attribute dictionary is used as the operand to a string formatting operation. Returns the resulting string. Before formatting the dictionary, a couple of preparatory steps are carried out. The message attribute of the record is computed using msg % args. If the formatting string contains '(asctime)', formatTime() is called to format the event time. If there is exception information, it is formatted using formatException() and appended to the message. Note that the formatted exception information is cached in attribute exc_text. This is useful because the exception information can be pickled and sent across the wire, but you should be careful if you have more than one Formatter subclass which customizes the formatting of exception information. In this case, you will have to clear the cached value (by setting the exc_text attribute to None) after a formatter has done its formatting, so that the next formatter to handle the event doesn’t use the cached value, but recalculates it afresh.\nIf stack information is available, it’s appended after the exception information, using formatStack() to transform it if necessary.\nformatTime(record, datefmt=None)¶\nThis method should be called from format() by a formatter which wants to make use of a formatted time. This method can be overridden in formatters to provide for any specific requirement, but the basic behavior is as follows: if datefmt (a string) is specified, it is used with time.strftime() to format the creation time of the record. Otherwise, the format ‘%Y-%m-%d %H:%M:%S,uuu’ is used, where the uuu part is a millisecond value and the other letters are as per the time.strftime() documentation. An example time in this format is 2003-01-23 00:29:50,411. The resulting string is returned.\nThis function uses a user-configurable function to convert the creation time to a tuple. By default, time.localtime() is used; to change this for a particular formatter instance, set the converter attribute to a function with the same signature as time.localtime() or time.gmtime(). To change it for all formatters, for example if you want all logging times to be shown in GMT, set the converter attribute in the Formatter class.\nChanged in version 3.3: Previously, the default format was hard-coded as in this example: 2010-09-06 22:38:15,292 where the part before the comma is handled by a strptime format string ('%Y-%m-%d %H:%M:%S'), and the part after the comma is a millisecond value. Because strptime does not have a format placeholder for milliseconds, the millisecond value is appended using another format string, '%s,%03d' — and both of these format strings have been hardcoded into this method. With the change, these strings are defined as class-level attributes which can be overridden at the instance level when desired. The names of the attributes are default_time_format (for the strptime format string) and default_msec_format (for appending the millisecond value).\nChanged in version 3.9: The default_msec_format can be None.\nformatException(exc_info)¶\nFormats the specified exception information (a standard exception tuple as returned by sys.exc_info()) as a string. This default implementation just uses traceback.print_exception(). The resulting string is returned.\nformatStack(stack_info)¶\nFormats the specified stack information (a string as returned by traceback.print_stack(), but with the last newline removed) as a string. This default implementation just returns the input value.\nclass logging.BufferingFormatter(linefmt=None)¶\nA base formatter class suitable for subclassing when you want to format a number of records. You can pass a Formatter instance which you want to use to format each line (that corresponds to a single record). If not specified, the default formatter (which just outputs the event message) is used as the line formatter.\nReturn a header for a list of records. The base implementation just returns the empty string. You will need to override this method if you want specific behaviour, e.g. to show the count of records, a title or a separator line.\nReturn a footer for a list of records. The base implementation just returns the empty string. You will need to override this method if you want specific behaviour, e.g. to show the count of records or a separator line.\nformat(records)¶\nReturn formatted text for a list of records. The base implementation just returns the empty string if there are no records; otherwise, it returns the concatenation of the header, each record formatted with the line formatter, and the footer."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#filter-objects",
    "href": "knowledgebase/python/python3_logging_api.html#filter-objects",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "Filters can be used by Handlers and Loggers for more sophisticated filtering than is provided by levels. The base filter class only allows events which are below a certain point in the logger hierarchy. For example, a filter initialized with ‘A.B’ will allow events logged by loggers ‘A.B’, ‘A.B.C’, ‘A.B.C.D’, ‘A.B.D’ etc. but not ‘A.BB’, ‘B.A.B’ etc. If initialized with the empty string, all events are passed.\nclass logging.Filter(name=’’)¶\nReturns an instance of the Filter class. If name is specified, it names a logger which, together with its children, will have its events allowed through the filter. If name is the empty string, allows every event.\nfilter(record)¶\nIs the specified record to be logged? Returns false for no, true for yes. Filters can either modify log records in-place or return a completely different record instance which will replace the original log record in any future processing of the event.\nNote that filters attached to handlers are consulted before an event is emitted by the handler, whereas filters attached to loggers are consulted whenever an event is logged (using debug(), info(), etc.), before sending an event to handlers. This means that events which have been generated by descendant loggers will not be filtered by a logger’s filter setting, unless the filter has also been applied to those descendant loggers.\nYou don’t actually need to subclass Filter: you can pass any instance which has a filter method with the same semantics.\nChanged in version 3.2: You don’t need to create specialized Filter classes, or use other classes with a filter method: you can use a function (or other callable) as a filter. The filtering logic will check to see if the filter object has a filter attribute: if it does, it’s assumed to be a Filter and its filter() method is called. Otherwise, it’s assumed to be a callable and called with the record as the single parameter. The returned value should conform to that returned by filter().\nChanged in version 3.12: You can now return a LogRecord instance from filters to replace the log record rather than modifying it in place. This allows filters attached to a Handler to modify the log record before it is emitted, without having side effects on other handlers.\nAlthough filters are used primarily to filter records based on more sophisticated criteria than levels, they get to see every record which is processed by the handler or logger they’re attached to: this can be useful if you want to do things like counting how many records were processed by a particular logger or handler, or adding, changing or removing attributes in the LogRecord being processed. Obviously changing the LogRecord needs to be done with some care, but it does allow the injection of contextual information into logs (see Using Filters to impart contextual information)."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#logrecord-objects",
    "href": "knowledgebase/python/python3_logging_api.html#logrecord-objects",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "LogRecord instances are created automatically by the Logger every time something is logged, and can be created manually via makeLogRecord() (for example, from a pickled event received over the wire).\nclass logging.LogRecord(name, level, pathname, lineno, msg, args, exc_info, func=None, sinfo=None)¶\nContains all the information pertinent to the event being logged.\nThe primary information is passed in msg and args, which are combined using msg % args to create the message attribute of the record.\nParameters:\n\nname (str) – The name of the logger used to log the event represented by this LogRecord. Note that the logger name in the LogRecord will always have this value, even though it may be emitted by a handler attached to a different (ancestor) logger.\nlevel (int) – The numeric level of the logging event (such as 10 for DEBUG, 20 for INFO, etc). Note that this is converted to two attributes of the LogRecord: levelno for the numeric value and levelname for the corresponding level name.\npathname (str) – The full string path of the source file where the logging call was made.\nlineno (int) – The line number in the source file where the logging call was made.\nmsg (Any) – The event description message, which can be a %-format string with placeholders for variable data, or an arbitrary object (see Using arbitrary objects as messages).\nargs (tuple | dict[str, Any]) – Variable data to merge into the msg argument to obtain the event description.\nexc_info (tuple[type[BaseException_]__,_ BaseException, types.TracebackType] | None) – An exception tuple with the current exception information, as returned by sys.exc_info(), or None if no exception information is available.\nfunc (str | None) – The name of the function or method from which the logging call was invoked.\nsinfo (str | None) – A text string representing stack information from the base of the stack in the current thread, up to the logging call.\n\ngetMessage()¶\nReturns the message for this LogRecord instance after merging any user-supplied arguments with the message. If the user-supplied message argument to the logging call is not a string, str() is called on it to convert it to a string. This allows use of user-defined classes as messages, whose __str__ method can return the actual format string to be used.\nChanged in version 3.2: The creation of a LogRecord has been made more configurable by providing a factory which is used to create the record. The factory can be set using getLogRecordFactory() and setLogRecordFactory() (see this for the factory’s signature).\nThis functionality can be used to inject your own values into a LogRecord at creation time. You can use the following pattern:\nold_factory = logging.getLogRecordFactory()\n\ndef record_factory(*args, **kwargs):\n    record = old_factory(*args, **kwargs)\n    record.custom_attribute = 0xdecafbad\n    return record\n\nlogging.setLogRecordFactory(record_factory)\n\nWith this pattern, multiple factories could be chained, and as long as they don’t overwrite each other’s attributes or unintentionally overwrite the standard attributes listed above, there should be no surprises."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#logrecord-attributes",
    "href": "knowledgebase/python/python3_logging_api.html#logrecord-attributes",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "The LogRecord has a number of attributes, most of which are derived from the parameters to the constructor. (Note that the names do not always correspond exactly between the LogRecord constructor parameters and the LogRecord attributes.) These attributes can be used to merge data from the record into the format string. The following table lists (in alphabetical order) the attribute names, their meanings and the corresponding placeholder in a %-style format string.\nIf you are using {}-formatting (str.format()), you can use {attrname} as the placeholder in the format string. If you are using \\(-formatting ([`string.Template`](https://docs.python.org/3/library/string.html#string.Template \"string.Template\")), use the form `\\){attrname}. In both cases, of course, replaceattrname` with the actual attribute name you want to use.\nIn the case of {}-formatting, you can specify formatting flags by placing them after the attribute name, separated from it with a colon. For example: a placeholder of {msecs:03.0f} would format a millisecond value of 4 as 004. Refer to the str.format() documentation for full details on the options available to you.\n\nAttribute name\n|\nFormat\n|\nDescription\n\n\n\n\n\n\n\n\n\nargs\n|\nYou shouldn’t need to format this yourself.\n|\nThe tuple of arguments merged into msg to produce message, or a dict whose values are used for the merge (when there is only one argument, and it is a dictionary).\n| |\nasctime\n|\n%(asctime)s\n|\nHuman-readable time when the LogRecord was created. By default this is of the form ‘2003-07-08 16:49:45,896’ (the numbers after the comma are millisecond portion of the time).\n| |\ncreated\n|\n%(created)f\n|\nTime when the LogRecord was created (as returned by time.time_ns() / 1e9).\n| |\nexc_info\n|\nYou shouldn’t need to format this yourself.\n|\nException tuple (à la sys.exc_info) or, if no exception has occurred, None.\n| |\nfilename\n|\n%(filename)s\n|\nFilename portion of pathname.\n| |\nfuncName\n|\n%(funcName)s\n|\nName of function containing the logging call.\n| |\nlevelname\n|\n%(levelname)s\n|\nText logging level for the message ('DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL').\n| |\nlevelno\n|\n%(levelno)s\n|\nNumeric logging level for the message (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n| |\nlineno\n|\n%(lineno)d\n|\nSource line number where the logging call was issued (if available).\n| |\nmessage\n|\n%(message)s\n|\nThe logged message, computed as msg % args. This is set when Formatter.format() is invoked.\n| |\nmodule\n|\n%(module)s\n|\nModule (name portion of filename).\n| |\nmsecs\n|\n%(msecs)d\n|\nMillisecond portion of the time when the LogRecord was created.\n| |\nmsg\n|\nYou shouldn’t need to format this yourself.\n|\nThe format string passed in the original logging call. Merged with args to produce message, or an arbitrary object (see Using arbitrary objects as messages).\n| |\nname\n|\n%(name)s\n|\nName of the logger used to log the call.\n| |\npathname\n|\n%(pathname)s\n|\nFull pathname of the source file where the logging call was issued (if available).\n| |\nprocess\n|\n%(process)d\n|\nProcess ID (if available).\n| |\nprocessName\n|\n%(processName)s\n|\nProcess name (if available).\n| |\nrelativeCreated\n|\n%(relativeCreated)d\n|\nTime in milliseconds when the LogRecord was created, relative to the time the logging module was loaded.\n| |\nstack_info\n|\nYou shouldn’t need to format this yourself.\n|\nStack frame information (where available) from the bottom of the stack in the current thread, up to and including the stack frame of the logging call which resulted in the creation of this record.\n| |\nthread\n|\n%(thread)d\n|\nThread ID (if available).\n| |\nthreadName\n|\n%(threadName)s\n|\nThread name (if available).\n| |\ntaskName\n|\n%(taskName)s\n|\nasyncio.Task name (if available).\n|\nChanged in version 3.1: processName was added.\nChanged in version 3.12: taskName was added."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#loggeradapter-objects",
    "href": "knowledgebase/python/python3_logging_api.html#loggeradapter-objects",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "LoggerAdapter instances are used to conveniently pass contextual information into logging calls. For a usage example, see the section on adding contextual information to your logging output.\nclass logging.LoggerAdapter(logger, extra, merge_extra=False)¶\nReturns an instance of LoggerAdapter initialized with an underlying Logger instance, a dict-like object (extra), and a boolean (merge_extra) indicating whether or not the extra argument of individual log calls should be merged with the LoggerAdapter extra. The default behavior is to ignore the extra argument of individual log calls and only use the one of the LoggerAdapter instance\nprocess(msg, kwargs)¶\nModifies the message and/or keyword arguments passed to a logging call in order to insert contextual information. This implementation takes the object passed as extra to the constructor and adds it to kwargs using key ‘extra’. The return value is a (msg, kwargs) tuple which has the (possibly modified) versions of the arguments passed in.\nmanager¶\nDelegates to the underlying manager on logger.\n_log¶\nDelegates to the underlying _log() method on logger.\nIn addition to the above, LoggerAdapter supports the following methods of Logger: debug(), info(), warning(), error(), exception(), critical(), log(), isEnabledFor(), getEffectiveLevel(), setLevel() and hasHandlers(). These methods have the same signatures as their counterparts in Logger, so you can use the two types of instances interchangeably.\nChanged in version 3.2: The isEnabledFor(), getEffectiveLevel(), setLevel() and hasHandlers() methods were added to LoggerAdapter. These methods delegate to the underlying logger.\nChanged in version 3.6: Attribute manager and method _log() were added, which delegate to the underlying logger and allow adapters to be nested.\nChanged in version 3.13: The merge_extra argument was added."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#thread-safety",
    "href": "knowledgebase/python/python3_logging_api.html#thread-safety",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "The logging module is intended to be thread-safe without any special work needing to be done by its clients. It achieves this though using threading locks; there is one lock to serialize access to the module’s shared data, and each handler also creates a lock to serialize access to its underlying I/O.\nIf you are implementing asynchronous signal handlers using the signal module, you may not be able to use logging from within such handlers. This is because lock implementations in the threading module are not always re-entrant, and so cannot be invoked from such signal handlers."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#module-level-functions",
    "href": "knowledgebase/python/python3_logging_api.html#module-level-functions",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "In addition to the classes described above, there are a number of module-level functions.\nlogging.getLogger(name=None)¶\nReturn a logger with the specified name or, if name is None, return the root logger of the hierarchy. If specified, the name is typically a dot-separated hierarchical name like ‘a’, ‘a.b’ or ‘a.b.c.d’. Choice of these names is entirely up to the developer who is using logging, though it is recommended that __name__ be used unless you have a specific reason for not doing that, as mentioned in Logger Objects.\nAll calls to this function with a given name return the same logger instance. This means that logger instances never need to be passed between different parts of an application.\nlogging.getLoggerClass()¶\nReturn either the standard Logger class, or the last class passed to setLoggerClass(). This function may be called from within a new class definition, to ensure that installing a customized Logger class will not undo customizations already applied by other code. For example:\nclass MyLogger(logging.getLoggerClass()):\n    # ... override behaviour here\n\nlogging.getLogRecordFactory()¶\nReturn a callable which is used to create a LogRecord.\nAdded in version 3.2: This function has been provided, along with setLogRecordFactory(), to allow developers more control over how the LogRecord representing a logging event is constructed.\nSee setLogRecordFactory() for more information about the how the factory is called.\nlogging.debug(msg, *args, **kwargs)¶\nThis is a convenience function that calls Logger.debug(), on the root logger. The handling of the arguments is in every way identical to what is described in that method.\nThe only difference is that if the root logger has no handlers, then basicConfig() is called, prior to calling debug on the root logger.\nFor very short scripts or quick demonstrations of logging facilities, debug and the other module-level functions may be convenient. However, most programs will want to carefully and explicitly control the logging configuration, and should therefore prefer creating a module-level logger and calling Logger.debug() (or other level-specific methods) on it, as described at the beginnning of this documentation.\nlogging.info(msg, *args, **kwargs)¶\nLogs a message with level INFO on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.warning(msg, *args, **kwargs)¶\nLogs a message with level WARNING on the root logger. The arguments and behavior are otherwise the same as for debug().\nNote\nThere is an obsolete function warn which is functionally identical to warning. As warn is deprecated, please do not use it - use warning instead.\nlogging.error(msg, *args, **kwargs)¶\nLogs a message with level ERROR on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.critical(msg, *args, **kwargs)¶\nLogs a message with level CRITICAL on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.exception(msg, *args, **kwargs)¶\nLogs a message with level ERROR on the root logger. The arguments and behavior are otherwise the same as for debug(). Exception info is added to the logging message. This function should only be called from an exception handler.\nlogging.log(level, msg, *args, **kwargs)¶\nLogs a message with level level on the root logger. The arguments and behavior are otherwise the same as for debug().\nlogging.disable(level=CRITICAL)¶\nProvides an overriding level level for all loggers which takes precedence over the logger’s own level. When the need arises to temporarily throttle logging output down across the whole application, this function can be useful. Its effect is to disable all logging calls of severity level and below, so that if you call it with a value of INFO, then all INFO and DEBUG events would be discarded, whereas those of severity WARNING and above would be processed according to the logger’s effective level. If logging.disable(logging.NOTSET) is called, it effectively removes this overriding level, so that logging output again depends on the effective levels of individual loggers.\nNote that if you have defined any custom logging level higher than CRITICAL (this is not recommended), you won’t be able to rely on the default value for the level parameter, but will have to explicitly supply a suitable value.\nChanged in version 3.7: The level parameter was defaulted to level CRITICAL. See bpo-28524 for more information about this change.\nlogging.addLevelName(level, levelName)¶\nAssociates level level with text levelName in an internal dictionary, which is used to map numeric levels to a textual representation, for example when a Formatter formats a message. This function can also be used to define your own levels. The only constraints are that all levels used must be registered using this function, levels should be positive integers and they should increase in increasing order of severity.\nNote\nIf you are thinking of defining your own levels, please see the section on Custom Levels.\nlogging.getLevelNamesMapping()¶\nReturns a mapping from level names to their corresponding logging levels. For example, the string “CRITICAL” maps to CRITICAL. The returned mapping is copied from an internal mapping on each call to this function.\nAdded in version 3.11.\nlogging.getLevelName(level)¶\nReturns the textual or numeric representation of logging level level.\nIf level is one of the predefined levels CRITICAL, ERROR, WARNING, INFO or DEBUG then you get the corresponding string. If you have associated levels with names using addLevelName() then the name you have associated with level is returned. If a numeric value corresponding to one of the defined levels is passed in, the corresponding string representation is returned.\nThe level parameter also accepts a string representation of the level such as ‘INFO’. In such cases, this functions returns the corresponding numeric value of the level.\nIf no matching numeric or string value is passed in, the string ‘Level %s’ % level is returned.\nNote\nLevels are internally integers (as they need to be compared in the logging logic). This function is used to convert between an integer level and the level name displayed in the formatted log output by means of the %(levelname)s format specifier (see LogRecord attributes), and vice versa.\nChanged in version 3.4: In Python versions earlier than 3.4, this function could also be passed a text level, and would return the corresponding numeric value of the level. This undocumented behaviour was considered a mistake, and was removed in Python 3.4, but reinstated in 3.4.2 due to retain backward compatibility.\nlogging.getHandlerByName(name)¶\nReturns a handler with the specified name, or None if there is no handler with that name.\nAdded in version 3.12.\nlogging.getHandlerNames()¶\nReturns an immutable set of all known handler names.\nAdded in version 3.12.\nlogging.makeLogRecord(attrdict)¶\nCreates and returns a new LogRecord instance whose attributes are defined by attrdict. This function is useful for taking a pickled LogRecord attribute dictionary, sent over a socket, and reconstituting it as a LogRecord instance at the receiving end.\nlogging.basicConfig(**kwargs)¶\nDoes basic configuration for the logging system by creating a StreamHandler with a default Formatter and adding it to the root logger. The functions debug(), info(), warning(), error() and critical() will call basicConfig() automatically if no handlers are defined for the root logger.\nThis function does nothing if the root logger already has handlers configured, unless the keyword argument force is set to True.\nNote\nThis function should be called from the main thread before other threads are started. In versions of Python prior to 2.7.1 and 3.2, if this function is called from multiple threads, it is possible (in rare circumstances) that a handler will be added to the root logger more than once, leading to unexpected results such as messages being duplicated in the log.\nThe following keyword arguments are supported.\n\nFormat\n|\nDescription\n\n\n\n\n\n\n\n\nfilename\n|\nSpecifies that a FileHandler be created, using the specified filename, rather than a StreamHandler.\n| |\nfilemode\n|\nIf filename is specified, open the file in this mode. Defaults to 'a'.\n| |\nformat\n|\nUse the specified format string for the handler. Defaults to attributes levelname, name and message separated by colons.\n| |\ndatefmt\n|\nUse the specified date/time format, as accepted by time.strftime().\n| |\nstyle\n|\nIf format is specified, use this style for the format string. One of '%', '{' or '$' for printf-style, str.format() or string.Template respectively. Defaults to '%'.\n| |\nlevel\n|\nSet the root logger level to the specified level.\n| |\nstream\n|\nUse the specified stream to initialize the StreamHandler. Note that this argument is incompatible with filename - if both are present, a ValueError is raised.\n| |\nhandlers\n|\nIf specified, this should be an iterable of already created handlers to add to the root logger. Any handlers which don’t already have a formatter set will be assigned the default formatter created in this function. Note that this argument is incompatible with filename or stream - if both are present, a ValueError is raised.\n| |\nforce\n|\nIf this keyword argument is specified as true, any existing handlers attached to the root logger are removed and closed, before carrying out the configuration as specified by the other arguments.\n| |\nencoding\n|\nIf this keyword argument is specified along with filename, its value is used when the FileHandler is created, and thus used when opening the output file.\n| |\nerrors\n|\nIf this keyword argument is specified along with filename, its value is used when the FileHandler is created, and thus used when opening the output file. If not specified, the value ‘backslashreplace’ is used. Note that if None is specified, it will be passed as such to open(), which means that it will be treated the same as passing ‘errors’.\n|\nChanged in version 3.2: The style argument was added.\nChanged in version 3.3: The handlers argument was added. Additional checks were added to catch situations where incompatible arguments are specified (e.g. handlers together with stream or filename, or stream together with filename).\nChanged in version 3.8: The force argument was added.\nChanged in version 3.9: The encoding and errors arguments were added.\nlogging.shutdown()¶\nInforms the logging system to perform an orderly shutdown by flushing and closing all handlers. This should be called at application exit and no further use of the logging system should be made after this call.\nWhen the logging module is imported, it registers this function as an exit handler (see atexit), so normally there’s no need to do that manually.\nlogging.setLoggerClass(klass)¶\nTells the logging system to use the class klass when instantiating a logger. The class should define __init__() such that only a name argument is required, and the __init__() should call Logger.__init__(). This function is typically called before any loggers are instantiated by applications which need to use custom logger behavior. After this call, as at any other time, do not instantiate loggers directly using the subclass: continue to use the logging.getLogger() API to get your loggers.\nlogging.setLogRecordFactory(factory)¶\nSet a callable which is used to create a LogRecord.\nParameters:\nfactory – The factory callable to be used to instantiate a log record.\nAdded in version 3.2: This function has been provided, along with getLogRecordFactory(), to allow developers more control over how the LogRecord representing a logging event is constructed.\nThe factory has the following signature:\nfactory(name, level, fn, lno, msg, args, exc_info, func=None, sinfo=None, **kwargs)\n\nname:\nThe logger name.\nlevel:\nThe logging level (numeric).\nfn:\nThe full pathname of the file where the logging call was made.\nlno:\nThe line number in the file where the logging call was made.\nmsg:\nThe logging message.\nargs:\nThe arguments for the logging message.\nexc_info:\nAn exception tuple, or None.\nfunc:\nThe name of the function or method which invoked the logging call.\nsinfo:\nA stack traceback such as is provided by traceback.print_stack(), showing the call hierarchy.\nkwargs:\nAdditional keyword arguments."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#module-level-attributes",
    "href": "knowledgebase/python/python3_logging_api.html#module-level-attributes",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "logging.lastResort¶\nA “handler of last resort” is available through this attribute. This is a StreamHandler writing to sys.stderr with a level of WARNING, and is used to handle logging events in the absence of any logging configuration. The end result is to just print the message to sys.stderr. This replaces the earlier error message saying that “no handlers could be found for logger XYZ”. If you need the earlier behaviour for some reason, lastResort can be set to None.\nAdded in version 3.2.\nlogging.raiseExceptions¶\nUsed to see if exceptions during handling should be propagated.\nDefault: True.\nIf raiseExceptions is False, exceptions get silently ignored. This is what is mostly wanted for a logging system - most users will not care about errors in the logging system, they are more interested in application errors."
  },
  {
    "objectID": "knowledgebase/python/python3_logging_api.html#integration-with-the-warnings-module",
    "href": "knowledgebase/python/python3_logging_api.html#integration-with-the-warnings-module",
    "title": "logging — Logging facility for Python — Python 3.13.1 documentation",
    "section": "",
    "text": "The captureWarnings() function can be used to integrate logging with the warnings module.\nlogging.captureWarnings(capture)¶\nThis function is used to turn the capture of warnings by logging on and off.\nIf capture is True, warnings issued by the warnings module will be redirected to the logging system. Specifically, a warning will be formatted using warnings.formatwarning() and the resulting string logged to a logger named 'py.warnings' with a severity of WARNING.\nIf capture is False, the redirection of warnings to the logging system will stop, and warnings will be redirected to their original destinations (i.e. those in effect before captureWarnings(True) was called).\nSee also\nModule logging.config\nConfiguration API for the logging module.\nModule logging.handlers\nUseful handlers included with the logging module.\nPEP 282 - A Logging System\nThe proposal which described this feature for inclusion in the Python standard library.\nOriginal Python logging package\nThis is the original source for the logging package. The version of the package available from this site is suitable for use with Python 1.5.2, 2.1.x and 2.2.x, which do not include the logging package in the standard library."
  },
  {
    "objectID": "kb/llm/llms.html",
    "href": "kb/llm/llms.html",
    "title": "LLM",
    "section": "",
    "text": "https://github.com/ollama/ollama\n\n\n\n\n\nhttps://github.com/BerriAI/litellm “Call LLM APIs using the OpenAI Format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]”"
  },
  {
    "objectID": "kb/llm/llms.html#inference-serving",
    "href": "kb/llm/llms.html#inference-serving",
    "title": "LLM",
    "section": "",
    "text": "https://github.com/ollama/ollama"
  },
  {
    "objectID": "kb/llm/llms.html#application-development",
    "href": "kb/llm/llms.html#application-development",
    "title": "LLM",
    "section": "",
    "text": "https://github.com/BerriAI/litellm “Call LLM APIs using the OpenAI Format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]”"
  },
  {
    "objectID": "post/llm-evaluation-challenges/index.html",
    "href": "post/llm-evaluation-challenges/index.html",
    "title": "The Challenges of Evaluating Large Language Models",
    "section": "",
    "text": "tags: - LLM - NLP - ML Evaluation - AI Research\ncategories: - Research - Machine Learning —"
  },
  {
    "objectID": "post/llm-evaluation-challenges/index.html#the-moving-target-of-llm-evaluation",
    "href": "post/llm-evaluation-challenges/index.html#the-moving-target-of-llm-evaluation",
    "title": "The Challenges of Evaluating Large Language Models",
    "section": "The Moving Target of LLM Evaluation",
    "text": "The Moving Target of LLM Evaluation\nAs large language models (LLMs) continue to advance at a rapid pace, the methods we use to evaluate them are struggling to keep up. Traditional NLP benchmarks like GLUE and SuperGLUE, which once represented the gold standard for measuring language understanding, are now being solved with near-human or superhuman performance by state-of-the-art models. This success has created a paradoxical situation: our best models are acing our best tests, yet we know they still have significant limitations.\nThis post explores the challenges of LLM evaluation and highlights promising approaches for more comprehensive assessment frameworks.\n\nThe Limitations of Current Benchmarks\nCurrent benchmarks face several key limitations:\n\nStatic Nature: Most benchmarks are static collections of problems that quickly become outdated as models improve and are trained on similar data.\nNarrow Scope: Many benchmarks focus on specific capabilities (like question answering or text classification) rather than holistic evaluation of model capabilities.\nLack of Adversarial Testing: Few benchmarks systematically probe for model weaknesses or test robustness against adversarial inputs.\nMetric Mismatch: Simple accuracy or F1 scores often fail to capture the nuanced quality of model outputs, especially for generation tasks.\nDomain Specificity: General benchmarks may not adequately test performance in specialized domains like finance, healthcare, or legal reasoning.\n\n\n\nMoving Beyond Accuracy: Evaluation Dimensions\nModern LLM evaluation requires moving beyond simple accuracy metrics to assess multiple dimensions:\n\n1. Factuality and Knowledge\nHow factually accurate is the model’s output? Can it recall correct information about the world? Can it avoid “hallucinating” false information? This dimension is particularly challenging because it requires:\n\nDistinguishing between facts, opinions, and speculations\nEvaluating whether information is current and up-to-date\nChecking consistency across multiple statements\n\n\n\n2. Reasoning and Problem-Solving\nCan the model apply logical inference to solve multi-step problems? This includes:\n\nMathematical reasoning\nSymbolic manipulation\nCausal reasoning\nPlanning and sequential decision making\n\n\n\n3. Safety and Alignment\nDoes the model adhere to human values and avoid harmful outputs? This dimension covers:\n\nRefusal of harmful requests\nAvoidance of biased or discriminatory outputs\nAdherence to ethical guidelines\nResistance to jailbreaking attempts\n\n\n\n4. Robustness\nHow consistently does the model perform across different:\n\nInput formulations\nContext lengths\nEdge cases\nAdversarial prompts\n\n\n\n5. Domain-Specific Performance\nHow well does the model perform in specialized domains like:\n\nFinancial analysis and compliance\nMedical knowledge and reasoning\nLegal interpretation\nTechnical fields like computer science or engineering\n\n\n\n\nPromising Approaches for Better Evaluation\nSeveral emerging approaches show promise for more comprehensive LLM evaluation:\n\nDynamic Benchmarking\nRather than static test sets, dynamic benchmarking continuously generates new test cases, potentially using other AI systems. Examples include:\n\nDynabench: An interactive platform where humans and models work together to create challenging examples\nAdversarial testing: Using one model to generate examples that might confuse another\nAutomatic benchmark generation: Creating test cases programmatically based on templates or schemas\n\n\n\nEvaluation-as-a-Service\nMoving from downloadable test sets to API-based evaluation services that can:\n\nKeep test data private to prevent memorization or leakage\nUpdate continuously with new examples\nProvide standardized evaluation protocols\n\n\n\nMultidimensional Scoring\nReplacing single metrics with multidimensional evaluation frameworks that separately assess different capabilities:\n\nHELM: The Holistic Evaluation of Language Models project evaluates models across multiple scenarios and metrics\nLM Harness: A framework for evaluating language models on diverse tasks\nDomain-specific evaluation suites: Like our FLAME framework for financial language model evaluation\n\n\n\nHuman-in-the-Loop Evaluation\nIncorporating human feedback more systematically:\n\nStructured evaluation protocols for human judges\nComparative judgments rather than absolute ratings\nDiverse evaluator pools to capture different perspectives and expertise\n\n\n\n\nThe Future of LLM Evaluation\nAs LLMs continue to evolve, our evaluation methods must evolve with them. The most promising direction appears to be comprehensive frameworks that:\n\nEvaluate multiple dimensions of performance\nContinuously update with new challenges\nIncorporate both automated metrics and human judgment\nTest for both capabilities and limitations\nInclude domain-specific evaluation modules\n\nOur research group’s work on the FLAME framework represents one approach to this problem, focusing specifically on financial domain evaluation. By creating specialized assessment tools for different domains, we can build a more complete picture of LLM capabilities and limitations."
  },
  {
    "objectID": "post/llm-evaluation-challenges/index.html#conclusion",
    "href": "post/llm-evaluation-challenges/index.html#conclusion",
    "title": "The Challenges of Evaluating Large Language Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe rapid advancement of LLMs has outpaced our ability to evaluate them effectively. Simple benchmarks and accuracy metrics are no longer sufficient to distinguish between models or identify their true capabilities and limitations. Moving forward, we need evaluation frameworks that are as sophisticated and multifaceted as the models themselves.\nBy developing more nuanced, comprehensive, and dynamic evaluation methods, we can better understand these powerful systems, guide their development in beneficial directions, and ensure they are deployed responsibly in real-world applications.\n\nWhat evaluation methods do you think are most important for assessing large language models? Share your thoughts in the comments below."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "A collection of my academic publications, conference papers, and technical reports. These works represent contributions to the fields of machine learning, artificial intelligence, and their applications."
  },
  {
    "objectID": "publications.html#academic-publications",
    "href": "publications.html#academic-publications",
    "title": "Publications",
    "section": "Academic Publications",
    "text": "Academic Publications"
  },
  {
    "objectID": "authors/glenn/index.html",
    "href": "authors/glenn/index.html",
    "title": "Glenn Matlin",
    "section": "",
    "text": "Glenn Matlin is a PhD student in Machine Learning at the Georgia Institute of Technology advised by Professor Sudheer Chava. His research focuses on developing innovative machine learning approaches for financial applications, with particular emphasis on large language models and their evaluation in domain-specific contexts.\nHis current work includes the FLAME framework for evaluating LLMs in financial contexts and exploring methods for making deep learning models more explainable and trustworthy for high-stakes financial decisions. Before joining Georgia Tech, Glenn spent several years as a data scientist applying machine learning solutions in healthcare, financial technology, and e-commerce.\nGlenn combines strong technical skills in Python, statistics, and deep learning with domain expertise in finance and healthcare. He is passionate about bridging the gap between academic research and practical industry applications of AI."
  },
  {
    "objectID": "publication/example/index.html",
    "href": "publication/example/index.html",
    "title": "UnfoldML: Cost-Aware and Uncertainty-Based Dynamic 2D Prediction for Multi-Stage Classification",
    "section": "",
    "text": "Venue: Neural Information Processing Systems (NeurIPS) 2022\nPDF: Download Paper"
  },
  {
    "objectID": "publication/example/index.html#publication-details",
    "href": "publication/example/index.html#publication-details",
    "title": "UnfoldML: Cost-Aware and Uncertainty-Based Dynamic 2D Prediction for Multi-Stage Classification",
    "section": "",
    "text": "Venue: Neural Information Processing Systems (NeurIPS) 2022\nPDF: Download Paper"
  },
  {
    "objectID": "publication/example/index.html#abstract",
    "href": "publication/example/index.html#abstract",
    "title": "UnfoldML: Cost-Aware and Uncertainty-Based Dynamic 2D Prediction for Multi-Stage Classification",
    "section": "Abstract",
    "text": "Abstract\nMachine Learning (ML) research has focused on maximizing the accuracy of predictive tasks. ML models, however, are increasingly more complex, resource intensive, and costlier to deploy in resource-constrained environments. These issues are exacerbated for prediction tasks with sequential classification on progressively transitioned stages with “happens-before” relation between them.\nWe argue that it is possible to “unfold” a monolithic single multi-class classifier, typically trained for all stages using all data, into a series of single-stage classifiers. Each single-stage classifier can be cascaded gradually from cheaper to more expensive binary classifiers that are trained using only the necessary data modalities or features required for that stage.\nUnfoldML is a cost-aware and uncertainty-based dynamic 2D prediction pipeline for multi-stage classification that enables:\n\nNavigation of the accuracy/cost tradeoff space\nReducing the spatio-temporal cost of inference by orders of magnitude\nEarly prediction on proceeding stages"
  },
  {
    "objectID": "publication/example/index.html#unfoldml-achieves-orders-of-magnitude-better-cost-in-clinical-settings-while-detecting-multi-stage-disease-development-in-real-time.-it-achieves-within-0.1-accuracy-from-the-highest-performing-multi-class-baseline-while-saving-close-to-20x-on-spatio-temporal-cost-of-inference-and-earlier-3.5hrs-disease-onset-prediction.-we-also-show-that-unfoldml-generalizes-to-image-classification-where-it-can-predict-different-level-of-labels-from-coarse-to-fine-given-different-level-of-abstractions-of-a-image-saving-close-to-5x-cost-with-as-little-as-0.4-accuracy-reduction.",
    "href": "publication/example/index.html#unfoldml-achieves-orders-of-magnitude-better-cost-in-clinical-settings-while-detecting-multi-stage-disease-development-in-real-time.-it-achieves-within-0.1-accuracy-from-the-highest-performing-multi-class-baseline-while-saving-close-to-20x-on-spatio-temporal-cost-of-inference-and-earlier-3.5hrs-disease-onset-prediction.-we-also-show-that-unfoldml-generalizes-to-image-classification-where-it-can-predict-different-level-of-labels-from-coarse-to-fine-given-different-level-of-abstractions-of-a-image-saving-close-to-5x-cost-with-as-little-as-0.4-accuracy-reduction.",
    "title": "UnfoldML: Cost-Aware and Uncertainty-Based Dynamic 2D Prediction for Multi-Stage Classification",
    "section": "UnfoldML achieves orders of magnitude better cost in clinical settings, while detecting multi-stage disease development in real time. It achieves within 0.1% accuracy from the highest-performing multi-class baseline, while saving close to 20X on spatio-temporal cost of inference and earlier (3.5hrs) disease onset prediction. We also show that UnfoldML generalizes to image classification, where it can predict different level of labels (from coarse to fine) given different level of abstractions of a image, saving close to 5X cost with as little as 0.4% accuracy reduction.",
    "text": "UnfoldML achieves orders of magnitude better cost in clinical settings, while detecting multi-stage disease development in real time. It achieves within 0.1% accuracy from the highest-performing multi-class baseline, while saving close to 20X on spatio-temporal cost of inference and earlier (3.5hrs) disease onset prediction. We also show that UnfoldML generalizes to image classification, where it can predict different level of labels (from coarse to fine) given different level of abstractions of a image, saving close to 5X cost with as little as 0.4% accuracy reduction."
  },
  {
    "objectID": "publication/example/index.html#abstract-1",
    "href": "publication/example/index.html#abstract-1",
    "title": "UnfoldML: Cost-Aware and Uncertainty-Based Dynamic 2D Prediction for Multi-Stage Classification",
    "section": "Abstract",
    "text": "Abstract\nMachine Learning (ML) research has focused on maximizing the accuracy of predictive tasks. ML models, however, are increasingly more complex, resource intensive, and costlier to deploy in resource-constrained environments. These issues are exacerbated for prediction tasks with sequential classification on progressively transitioned stages with “happens-before” relation between them.\nWe propose UnfoldML, a cost-aware and uncertainty-based dynamic 2D prediction pipeline for multi-stage classification that:\n\nNavigates the accuracy/cost tradeoff space\nReduces the spatio-temporal cost of inference by orders of magnitude\nEnables early prediction on proceeding stages\n\nIn clinical settings, UnfoldML achieves within 0.1% accuracy of high-performing multi-class baselines while saving close to 20X on inference costs and enabling earlier (3.5hrs) disease onset prediction. We demonstrate that UnfoldML also generalizes to image classification tasks."
  },
  {
    "objectID": "docs/GLENN.html",
    "href": "docs/GLENN.html",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "Glenn Matlin is a Ph.D. candidate in Computer Science (Machine Learning) at the Georgia Institute of Technology. His work bridges large language models (LLMs), agent‑based AI systems, and high‑knowledge domains—especially finance. Prior to academia he held senior data‑science roles in healthcare and fintech. His research has appeared in top venues such as NeurIPS and ACL.\n\n\n\nPh.D. student, Georgia Tech (2022 – expected 2027)\nCreator of RAPID AI – Reasoning, Analysis & Planning for Interactive Decision‑Making\nFunded by DARPA (Advanced Research Concepts: Collaborative Knowledge Curation)\nResearch focus: collaborative & autonomous AI for finance, government, and economics\nPublications in NeurIPS 2022 and ACL; 1st‑Prize, 2023 GT School of CS Ph.D. Research & Poster Competition\n\n\n\n\n\nRAPID AI develops human-centered methods for Reasoning, Analysis, and Planning for Interactive Decision-making with Artificial Intelligence. Goals: * Enhance quantitative & causal reasoning of LLMs * Enable interactive human‑AI decision‑making * Complex tool use and knowledge retrieval (RAG, KG) * Use games & economic simulations as controlled testbeds that transfer to real‑world finance tasks\n\n\n\n\n\n\n\n\n\n\n\n\nDegree\nInstitution\nField\nDate\n\n\n\n\nPh.D. (in progress)\nGeorgia Institute of Technology\nComputer Science – Machine Learning\n2022 – 2027*\n\n\nB.A.\nUniversity of Central Florida\nEconomics\n 2011\n\n\n*Advised by Prof. Sudheer Chava (Scheller College of Business).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYears\nRole\nOrganization\nImpact Highlights\n\n\n\n\n2020 – 2021\nSenior Data Scientist (ML)\nKomodo Health\nBuilt rare‑disease detection models; 4× improvement in recall\n\n\n2019 – 2020\nSenior Data Scientist (AI)\nChange Healthcare\nNLP pipeline over 100 M medical documents; saved $400 k/yr\n\n\n2017 – 2018\nData Scientist\nLendUp\nCredit‑risk & marketing models; generated $1.5 M new revenue\n\n\n2015 – 2016\nSenior Data Analyst\nRichRelevance\nImproved e‑commerce recommendation quality\n\n\n2022 – present\nGraduate Research Assistant\nGeorgia Tech\nFinance‑focused LLM research; collaboration with Financial Services Innovation Lab & Entertainment Intelligence Lab\n\n\n\n\n\n\n\n“UnfoldML: Cost‑Aware and Uncertainty‑Based Dynamic 2‑D Prediction for Multi‑Stage Classification.” NeurIPS 2022. Demonstrates efficient multi‑stage disease‑prediction pipeline.\n“Financial Language Model Evaluation (FLaME)🔥 Association of Computational Linguistics, 2025\n\n\n\n\n\nLanguages/Frameworks/Services: Python, C/C++, PyTorch, TensorFlow, SQL, Git, Docker, AWS, Azure, Kubernetes, GCP, HuggingFace, Neo4j\nML & NLP: Transformers, RL, retrieval‑augmented generation, knowledge graphs, prompt & instruction tuning\n\n\n\n\n\n1st Prize – 2023 GT School of CS Ph.D. Academic Research Competition\n\n\n\n\n\nWebsite: https://www.glennmatlin.doctor\nLinkedIn: linkedin.com/in/glennmatlin\nEmail: glenn [at] gatech [dot] edu\nGitHub: glennmatlin\nHuggingFace: glennmatlin\n\n\n\n\n\n\n\nHugo Section\nContent Source\n\n\n\n\nabout.md\nOverview + RAPID summary\n\n\nresearch.md\nSections 4 & 5\n\n\nexperience.md\nSection 4 table\n\n\npublications/\nBibTeX for each paper\n\n\nprojects/\nDARPA knowledge‑curation, FinGT, Ferrari, etc.\n\n\ncontact.md\nSection 8\n\n\n\n\n\n\nThe following thematic stack is ordered to reflect Glenn’s highest‑priority passions.\n\n\n\nPlanning – long‑horizon, tool‑using agents\nDecision‑Making & Gaming – strategic and economic games as evaluation grounds\nTool Use – integrating external APIs, calculators, and domain tools\n\n\n\n\n\nNLP Tasks – question answering, summarization, reasoning benchmarks\nWorld Models & Knowledge/Ontology – explicit & latent representations\n\n\n\n\n\nFERRArI – instruction & reinforcement fine‑tuning for financial reasoning\nVerification & Synthetic Data – safety, truthfulness, and synthetic corpora\n\n\n\n\n\nFinGT / FinPile – domain‑specific pre‑training corpora\nRouting Synthetic Data – scalable data generation & filtering pipelines\n\n\n\n\n\nKnowledge Graphs & RAG – retrieval‑augmented generation pipelines\nFairness / Interpretability – auditability and trustworthy AI\n\n\n\n\n\nHuman‑AI Interaction – co‑creative decision support\nStrategic Behavior – modeling incentives & multi‑agent dynamics\n\n\n\n\n\nGame Theory – equilibrium analysis, bargaining, mechanism design\nDecision Theory – preferences, uncertainty, and risk in AI planning\n\n\nLast updated: 23 May 2025."
  },
  {
    "objectID": "docs/GLENN.html#overview",
    "href": "docs/GLENN.html#overview",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "Glenn Matlin is a Ph.D. candidate in Computer Science (Machine Learning) at the Georgia Institute of Technology. His work bridges large language models (LLMs), agent‑based AI systems, and high‑knowledge domains—especially finance. Prior to academia he held senior data‑science roles in healthcare and fintech. His research has appeared in top venues such as NeurIPS and ACL.\n\n\n\nPh.D. student, Georgia Tech (2022 – expected 2027)\nCreator of RAPID AI – Reasoning, Analysis & Planning for Interactive Decision‑Making\nFunded by DARPA (Advanced Research Concepts: Collaborative Knowledge Curation)\nResearch focus: collaborative & autonomous AI for finance, government, and economics\nPublications in NeurIPS 2022 and ACL; 1st‑Prize, 2023 GT School of CS Ph.D. Research & Poster Competition"
  },
  {
    "objectID": "docs/GLENN.html#rapid-ai",
    "href": "docs/GLENN.html#rapid-ai",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "RAPID AI develops human-centered methods for Reasoning, Analysis, and Planning for Interactive Decision-making with Artificial Intelligence. Goals: * Enhance quantitative & causal reasoning of LLMs * Enable interactive human‑AI decision‑making * Complex tool use and knowledge retrieval (RAG, KG) * Use games & economic simulations as controlled testbeds that transfer to real‑world finance tasks"
  },
  {
    "objectID": "docs/GLENN.html#educational-background",
    "href": "docs/GLENN.html#educational-background",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "Degree\nInstitution\nField\nDate\n\n\n\n\nPh.D. (in progress)\nGeorgia Institute of Technology\nComputer Science – Machine Learning\n2022 – 2027*\n\n\nB.A.\nUniversity of Central Florida\nEconomics\n 2011\n\n\n*Advised by Prof. Sudheer Chava (Scheller College of Business)."
  },
  {
    "objectID": "docs/GLENN.html#professional-experience",
    "href": "docs/GLENN.html#professional-experience",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "Years\nRole\nOrganization\nImpact Highlights\n\n\n\n\n2020 – 2021\nSenior Data Scientist (ML)\nKomodo Health\nBuilt rare‑disease detection models; 4× improvement in recall\n\n\n2019 – 2020\nSenior Data Scientist (AI)\nChange Healthcare\nNLP pipeline over 100 M medical documents; saved $400 k/yr\n\n\n2017 – 2018\nData Scientist\nLendUp\nCredit‑risk & marketing models; generated $1.5 M new revenue\n\n\n2015 – 2016\nSenior Data Analyst\nRichRelevance\nImproved e‑commerce recommendation quality\n\n\n2022 – present\nGraduate Research Assistant\nGeorgia Tech\nFinance‑focused LLM research; collaboration with Financial Services Innovation Lab & Entertainment Intelligence Lab"
  },
  {
    "objectID": "docs/GLENN.html#research-publications",
    "href": "docs/GLENN.html#research-publications",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "“UnfoldML: Cost‑Aware and Uncertainty‑Based Dynamic 2‑D Prediction for Multi‑Stage Classification.” NeurIPS 2022. Demonstrates efficient multi‑stage disease‑prediction pipeline.\n“Financial Language Model Evaluation (FLaME)🔥 Association of Computational Linguistics, 2025"
  },
  {
    "objectID": "docs/GLENN.html#technical-skills",
    "href": "docs/GLENN.html#technical-skills",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "Languages/Frameworks/Services: Python, C/C++, PyTorch, TensorFlow, SQL, Git, Docker, AWS, Azure, Kubernetes, GCP, HuggingFace, Neo4j\nML & NLP: Transformers, RL, retrieval‑augmented generation, knowledge graphs, prompt & instruction tuning"
  },
  {
    "objectID": "docs/GLENN.html#awards-honors",
    "href": "docs/GLENN.html#awards-honors",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "1st Prize – 2023 GT School of CS Ph.D. Academic Research Competition"
  },
  {
    "objectID": "docs/GLENN.html#contact-web-presence",
    "href": "docs/GLENN.html#contact-web-presence",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "Website: https://www.glennmatlin.doctor\nLinkedIn: linkedin.com/in/glennmatlin\nEmail: glenn [at] gatech [dot] edu\nGitHub: glennmatlin\nHuggingFace: glennmatlin"
  },
  {
    "objectID": "docs/GLENN.html#suggested-wowchemy-section-map",
    "href": "docs/GLENN.html#suggested-wowchemy-section-map",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "Hugo Section\nContent Source\n\n\n\n\nabout.md\nOverview + RAPID summary\n\n\nresearch.md\nSections 4 & 5\n\n\nexperience.md\nSection 4 table\n\n\npublications/\nBibTeX for each paper\n\n\nprojects/\nDARPA knowledge‑curation, FinGT, Ferrari, etc.\n\n\ncontact.md\nSection 8"
  },
  {
    "objectID": "docs/GLENN.html#current-research-interests",
    "href": "docs/GLENN.html#current-research-interests",
    "title": "Glenn Matlin – Academic & Professional Profile",
    "section": "",
    "text": "The following thematic stack is ordered to reflect Glenn’s highest‑priority passions.\n\n\n\nPlanning – long‑horizon, tool‑using agents\nDecision‑Making & Gaming – strategic and economic games as evaluation grounds\nTool Use – integrating external APIs, calculators, and domain tools\n\n\n\n\n\nNLP Tasks – question answering, summarization, reasoning benchmarks\nWorld Models & Knowledge/Ontology – explicit & latent representations\n\n\n\n\n\nFERRArI – instruction & reinforcement fine‑tuning for financial reasoning\nVerification & Synthetic Data – safety, truthfulness, and synthetic corpora\n\n\n\n\n\nFinGT / FinPile – domain‑specific pre‑training corpora\nRouting Synthetic Data – scalable data generation & filtering pipelines\n\n\n\n\n\nKnowledge Graphs & RAG – retrieval‑augmented generation pipelines\nFairness / Interpretability – auditability and trustworthy AI\n\n\n\n\n\nHuman‑AI Interaction – co‑creative decision support\nStrategic Behavior – modeling incentives & multi‑agent dynamics\n\n\n\n\n\nGame Theory – equilibrium analysis, bargaining, mechanism design\nDecision Theory – preferences, uncertainty, and risk in AI planning\n\n\nLast updated: 23 May 2025."
  }
]