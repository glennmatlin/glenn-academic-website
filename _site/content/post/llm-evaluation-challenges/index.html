<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-03-13">

<title>The Challenges of Evaluating Large Language Models – Glenn Matlin</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-17d0bf37f7016b094e565600d9551b64.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="fullcontent quarto-light">

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The Challenges of Evaluating Large Language Models</h1>
<p class="subtitle lead">Moving beyond traditional benchmarks to assess LLM capabilities</p>
  <div class="quarto-categories">
    <div class="quarto-category">Research</div>
    <div class="quarto-category">Machine Learning</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>glenn </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 13, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="the-moving-target-of-llm-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="the-moving-target-of-llm-evaluation">The Moving Target of LLM Evaluation</h2>
<p>As large language models (LLMs) continue to advance at a rapid pace, the methods we use to evaluate them are struggling to keep up. Traditional NLP benchmarks like GLUE and SuperGLUE, which once represented the gold standard for measuring language understanding, are now being solved with near-human or superhuman performance by state-of-the-art models. This success has created a paradoxical situation: our best models are acing our best tests, yet we know they still have significant limitations.</p>
<p>This post explores the challenges of LLM evaluation and highlights promising approaches for more comprehensive assessment frameworks.</p>
<section id="the-limitations-of-current-benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="the-limitations-of-current-benchmarks">The Limitations of Current Benchmarks</h3>
<p>Current benchmarks face several key limitations:</p>
<ol type="1">
<li><p><strong>Static Nature</strong>: Most benchmarks are static collections of problems that quickly become outdated as models improve and are trained on similar data.</p></li>
<li><p><strong>Narrow Scope</strong>: Many benchmarks focus on specific capabilities (like question answering or text classification) rather than holistic evaluation of model capabilities.</p></li>
<li><p><strong>Lack of Adversarial Testing</strong>: Few benchmarks systematically probe for model weaknesses or test robustness against adversarial inputs.</p></li>
<li><p><strong>Metric Mismatch</strong>: Simple accuracy or F1 scores often fail to capture the nuanced quality of model outputs, especially for generation tasks.</p></li>
<li><p><strong>Domain Specificity</strong>: General benchmarks may not adequately test performance in specialized domains like finance, healthcare, or legal reasoning.</p></li>
</ol>
</section>
<section id="moving-beyond-accuracy-evaluation-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="moving-beyond-accuracy-evaluation-dimensions">Moving Beyond Accuracy: Evaluation Dimensions</h3>
<p>Modern LLM evaluation requires moving beyond simple accuracy metrics to assess multiple dimensions:</p>
<section id="factuality-and-knowledge" class="level4">
<h4 class="anchored" data-anchor-id="factuality-and-knowledge">1. Factuality and Knowledge</h4>
<p>How factually accurate is the model’s output? Can it recall correct information about the world? Can it avoid “hallucinating” false information? This dimension is particularly challenging because it requires:</p>
<ul>
<li>Distinguishing between facts, opinions, and speculations</li>
<li>Evaluating whether information is current and up-to-date</li>
<li>Checking consistency across multiple statements</li>
</ul>
</section>
<section id="reasoning-and-problem-solving" class="level4">
<h4 class="anchored" data-anchor-id="reasoning-and-problem-solving">2. Reasoning and Problem-Solving</h4>
<p>Can the model apply logical inference to solve multi-step problems? This includes:</p>
<ul>
<li>Mathematical reasoning</li>
<li>Symbolic manipulation</li>
<li>Causal reasoning</li>
<li>Planning and sequential decision making</li>
</ul>
</section>
<section id="safety-and-alignment" class="level4">
<h4 class="anchored" data-anchor-id="safety-and-alignment">3. Safety and Alignment</h4>
<p>Does the model adhere to human values and avoid harmful outputs? This dimension covers:</p>
<ul>
<li>Refusal of harmful requests</li>
<li>Avoidance of biased or discriminatory outputs</li>
<li>Adherence to ethical guidelines</li>
<li>Resistance to jailbreaking attempts</li>
</ul>
</section>
<section id="robustness" class="level4">
<h4 class="anchored" data-anchor-id="robustness">4. Robustness</h4>
<p>How consistently does the model perform across different:</p>
<ul>
<li>Input formulations</li>
<li>Context lengths</li>
<li>Edge cases</li>
<li>Adversarial prompts</li>
</ul>
</section>
<section id="domain-specific-performance" class="level4">
<h4 class="anchored" data-anchor-id="domain-specific-performance">5. Domain-Specific Performance</h4>
<p>How well does the model perform in specialized domains like:</p>
<ul>
<li>Financial analysis and compliance</li>
<li>Medical knowledge and reasoning</li>
<li>Legal interpretation</li>
<li>Technical fields like computer science or engineering</li>
</ul>
</section>
</section>
<section id="promising-approaches-for-better-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="promising-approaches-for-better-evaluation">Promising Approaches for Better Evaluation</h3>
<p>Several emerging approaches show promise for more comprehensive LLM evaluation:</p>
<section id="dynamic-benchmarking" class="level4">
<h4 class="anchored" data-anchor-id="dynamic-benchmarking">Dynamic Benchmarking</h4>
<p>Rather than static test sets, dynamic benchmarking continuously generates new test cases, potentially using other AI systems. Examples include:</p>
<ul>
<li><strong>Dynabench</strong>: An interactive platform where humans and models work together to create challenging examples</li>
<li><strong>Adversarial testing</strong>: Using one model to generate examples that might confuse another</li>
<li><strong>Automatic benchmark generation</strong>: Creating test cases programmatically based on templates or schemas</li>
</ul>
</section>
<section id="evaluation-as-a-service" class="level4">
<h4 class="anchored" data-anchor-id="evaluation-as-a-service">Evaluation-as-a-Service</h4>
<p>Moving from downloadable test sets to API-based evaluation services that can:</p>
<ul>
<li>Keep test data private to prevent memorization or leakage</li>
<li>Update continuously with new examples</li>
<li>Provide standardized evaluation protocols</li>
</ul>
</section>
<section id="multidimensional-scoring" class="level4">
<h4 class="anchored" data-anchor-id="multidimensional-scoring">Multidimensional Scoring</h4>
<p>Replacing single metrics with multidimensional evaluation frameworks that separately assess different capabilities:</p>
<ul>
<li><strong>HELM</strong>: The Holistic Evaluation of Language Models project evaluates models across multiple scenarios and metrics</li>
<li><strong>LM Harness</strong>: A framework for evaluating language models on diverse tasks</li>
<li><strong>Domain-specific evaluation suites</strong>: Like our FLAME framework for financial language model evaluation</li>
</ul>
</section>
<section id="human-in-the-loop-evaluation" class="level4">
<h4 class="anchored" data-anchor-id="human-in-the-loop-evaluation">Human-in-the-Loop Evaluation</h4>
<p>Incorporating human feedback more systematically:</p>
<ul>
<li>Structured evaluation protocols for human judges</li>
<li>Comparative judgments rather than absolute ratings</li>
<li>Diverse evaluator pools to capture different perspectives and expertise</li>
</ul>
</section>
</section>
<section id="the-future-of-llm-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="the-future-of-llm-evaluation">The Future of LLM Evaluation</h3>
<p>As LLMs continue to evolve, our evaluation methods must evolve with them. The most promising direction appears to be comprehensive frameworks that:</p>
<ol type="1">
<li>Evaluate multiple dimensions of performance</li>
<li>Continuously update with new challenges</li>
<li>Incorporate both automated metrics and human judgment</li>
<li>Test for both capabilities and limitations</li>
<li>Include domain-specific evaluation modules</li>
</ol>
<p>Our research group’s work on the FLAME framework represents one approach to this problem, focusing specifically on financial domain evaluation. By creating specialized assessment tools for different domains, we can build a more complete picture of LLM capabilities and limitations.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The rapid advancement of LLMs has outpaced our ability to evaluate them effectively. Simple benchmarks and accuracy metrics are no longer sufficient to distinguish between models or identify their true capabilities and limitations. Moving forward, we need evaluation frameworks that are as sophisticated and multifaceted as the models themselves.</p>
<p>By developing more nuanced, comprehensive, and dynamic evaluation methods, we can better understand these powerful systems, guide their development in beneficial directions, and ensure they are deployed responsibly in real-world applications.</p>
<hr>
<p><em>What evaluation methods do you think are most important for assessing large language models? Share your thoughts in the comments below.</em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/glennmatlin\.github\.io\/glenn-academic-website\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 Glenn Matlin. All rights reserved.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Published with <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>




</body></html>